[ { "title": "Redis | Key Technologies - System Design Interview", "url": "/posts/Redis/", "categories": "System Design Interview, Key Technologies", "tags": "System Design, interview, redis", "date": "2025-08-04 18:57:00 +0900", "snippet": "Redis 소개 Redis is the world’s fastest in-memory database.- from redis.io Redis는 C로 작성된, 오픈소스 in-memory key-value 저장소 입니다.(데이터를 Disk가 아닌 RAM에 저장하여 사용합니다.) 빠른 속도와 다양한 자료구조(Data structure)를 지원하는, ...", "content": "Redis 소개 Redis is the world’s fastest in-memory database.- from redis.io Redis는 C로 작성된, 오픈소스 in-memory key-value 저장소 입니다.(데이터를 Disk가 아닌 RAM에 저장하여 사용합니다.) 빠른 속도와 다양한 자료구조(Data structure)를 지원하는, NoSQL DB입니다. String, List, Hash, Set, Sorted Set, 비트맵, HyperLogLog 등 다양한 자료구조(Data Structure)를 지원합니다. NoSQL은 기존에 많이 쓰이던, 관계형(SQL) DB가 아닌것을 의미합니다.관계형 DB와 다르게, 더 유연한 데이터 모델과 확장성을 제공합니다. 만약 데이터 저장 과정에서, Durability(영속성)이 더 중요하다면, Redis는 적합하지 않습니다.Redis가 AOF(Append-Only File)을 통해, Data의 Persistent를 지원하고 있지만, 이는 RDB의 것만큼 보장(guarantee)해주지 못해기 때문입니다.단, AWS의 Memory DB와 같은 대안도 가능합니다.Redis는 Key-Value Store입니다.Redis는 Key-Value 저장소 입니다.Key는 반드시 ‘String’이어야 하며, Value는 Redis가 지원하는 데이터 구조(binary data and strings, sets, lists, hashes, sorted sets 등)는 모두 가능합니다.또한, Redis안에서의 모든 object들은 Key를 갖고 있어야 합니다.Redis의 Value TypeRedis의 Value Type 및 구현 Algorithm | from blog.bytebytego.comRedis의 Single Thread와 성능Redis는 단일 스레드(Single Thread) 구조임에도 불구하고, 수십만 RPS(requests per second) 를 처리할 수 있을 정도로 매우 빠릅니다.여기서는, 이것이 가능한 이유를 살펴보고자 합니다. Redis 6부터는 I/O(read/write) 에 일부 멀티스레드가 도입되었습니다.클라이언트 요청을 읽어오거나 응답을 보내는 작업은 멀티스레드 가능하지만, 명령(Command)실행 자체는 여전히 Single Thread로 작동합니다.RAM 기반 처리모든 데이터는 디스크가 아닌, RAM에 저장되어 있기 때문에, I/O 병목이 거의 없습니다.즉, 디스크 → 메모리로 데이터를 불러오는 비용이 들지않고, 즉시 데이터를 이용한 연산이 가능합니다.Lock 경합(Race Condition) 없음Lock Race Condition을 표현하는 이미지Redis는 Single Thread를 사용해서, 여러 Thread간의 동시성 문제를 위해 Lock을 사용할 필요가 없습니다.(애초에 Single Thread이기 때문에)CPU의 Context Switching을 비유적으로 표현하는 이미지이는 ‘Context Switching(CPU Level에서의 작업 전환)’ 비용과 ‘Lock Condition Race’(Lock을 얻기 위한 경합)을 완전히 제거해줍니다.이를 통해, 훨씬 예측 가능하고 빠른 처리 시간을 제공합니다.Epoll 기반 비동기 이벤트 처리Redis는 리눅스 epoll을 사용해 수천 개의 클라이언트 연결을 하나의 이벤트 루프에서 효율적으로 처리합니다.Linux Epoll의 역할을 보여주는 이미지 | devarea.com ‘epoll’은 수많은 파일 디스크립터(FD)(예: 클라이언트 소켓)의 입출력 가능 상태(I/O readiness) 를 효율적으로 감시합니다. 이벤트가 발생한 것만 알려주는 고성능 커널 기능입니다.이벤트가 없는 Connection은 무시하고, 이벤트가 발생한 소켓만 처리하여, 매우 빠른 성능을 보여줍니다. ‘Epoll’은 시스템 콜로 호출되는 커널 기능입니다. 이 또한 Redis의 Single Thread에서 작동합니다‘Command’가 단순하고 작다.Redis에서 사용하는 명령어는, 대부분 매우 가볍습니다.(GET, SET, INCR 등은 O(1) 또는 O(log N) 수준의 처리 시간.)Redis 내부 데이터 구조가 최적화되어 있어 탐색, 삽입, 정렬 등에 강력한 성능을 보여줍니다.C언어 기반Redis는 C언어로 개발되어 있어, 운영체제와 매우 가까운, 저수준에서 동작합니다.즉, 별도의 VM없이 실행되어 메모리와 CPU사용을 최적화 하였습니다.Redis의 Infra 구성 방법들Redis의 Infra Architecture의 변화 | bytebytego.comRedis는 기본적으로 Single Node로 동작할 수 있지만, HA(High Availability)를 위해 Replica나 Cluster형태로도 구성 가능합니다.Redis ClusterRedis Cluster와 Key분배Gossip ProtocolRedis Cluster와 gossip protocol ‘gossip protocol’은 Redis Cluster 환경에서 노드 간 상태 정보를 전파(synchronize) 하는 데 사용되는 간단하고 효율적인 통신 프로토콜입니다. 클러스터의 각 노드들이 서로의 상태 정보를 주기적으로 교환하면서, 장애 감지, 노드 변경 전파, 뷰 일관성 유지 등을 가능하게 해주는 경량 분산 통신 방식입니다. Q: 왜 ‘gossip’ protocol이라고 부르나요?A: Node간 데이터를 주고받는 방식이, 마치 사람들이 소문을 주고받는 것처럼, 일부 정보만 여러 노드 간에 점진적으로 퍼지기 때문입니다.ex) “노드 X가 다운된 것 같아” → Y가 듣고 Z에게 전파Hash SlotRedis Cluster로 구성한다면, ‘Hash Slot’이라는 개념을 사용하게 됩니다. ‘Hash Slot’은 Cluster에서 데이터를 분산저장하는 Sharding의 최소 단위입니다. 데이터의 Key값과 Node를 맵핑(mapping)해줍니다. 키를 어느 노드에 저장할지를 결정하는 데 사용되는 고정된 범위(0 ~ 16383)의 해시 공간입니다. 만약 구성 Node의 변경이 있다면, Hash Slot단위로 Node에 재분배됩니다.(키 단위가 아닌 “슬롯 단위”로 이동되므로 성능과 일관성 측면에서 유리합니다.) Redis Client는 이 ‘Hash Slot’을 Caching하여, Key값에 따라 해당 Key를 갖고 있는 Node에 바로 접속합니다.Redis가 Consistency를 보장하는 방법Single Thread를 기반으로 명령어를 원자적으로 처리여러 Client 요청을 처리하는 단일 Redis InstanceRedis 서버 프로세스는 단일 스레드로 동작하기 때문에, 들어오는 모든 명령이 순서대로(serialized) 처리됩니다.덕분에 하나의 명령(command) 은 실행 중 중단되지 않으며, 원자성(Atomicity) 을 자연스럽게 보장합니다. 단, Redis 6부터, ‘클라이언트 소켓 읽기/쓰기’, ‘명령을 실행하기 전/후의 버퍼 작업’에 대해서 멀티쓰레드 I/O가 도입되었습니다. Epoll?epoll은 Linux 커널에서 제공하는 고성능 I/O 이벤트 통지 메커니즘입니다.수많은 파일 디스크립터(FD)(예: 클라이언트 소켓)의 입출력 가능 상태(I/O readiness) 를 효율적으로 감시하고, 이벤트가 발생한 것만 알려주는 고성능 커널 기능입니다.Replication에서는 Eventual Consistency사용 기본적으로 Redis는 Primary에서 처리된 쓰기 작업을 Replica에게 비동기적으로 전파합니다. 이론적으로, 마스터에 쓰기가 완료된 직후 장애가 나면 일부 업데이트가 슬레이브에 전파되지 않을 수 있어, 최종 일관성(Eventual Consistency) 을 따릅니다.Cluster에서의 ConsistencyCluster로 구성된 Redis Redis Cluster는 데이터를 16,384개의 해시 슬롯(hash slot) 으로 분산 저장합니다.(Sharding)(Cluster가 하나의 instance처럼 작동) 샤드 간 트랜잭션을 지원하지 않으며, 하나의 키에 대한 연산은 항상 그 키를 소유한 노드(master)에서 처리됩니다. 클러스터 구성원 간 복제는 마스터–슬레이브 모델을 따르므로, 기본적으로 비동기 복제에 따른 최종 일관성을 제공합니다.Redis의 PersistenceRedis에서, ‘Persistence를 위한 기능들’의 작동 방식 | from redis.io Redis can persist your data either by periodically dumping the dataset to disk or by appending each command to a disk-based log.- from redis.io Redis에서는 Data를 보존하기 위해, 정기적으로 Disk에 데이터를 쓰거나(periodically dumping the dataset to disk) 각각의 Command를 Disk기반의 Log에 기록하는(appending each command to a disk-based log)방법을 사용합니다.정기적으로 Disk에 데이터를 쓰기(rdb파일로 쓰기)지정된 시점(snapshot)의 메모리 데이터를 통째로 덤프해서 .rdb 파일로 저장하는 방식을 말합니다.각각의 Command를 Disk기반의 Log에 기록하기(AOF)Redis에서 실행된 모든 쓰기 명령을 순차적으로 로그 파일에 기록하는 방식입니다.AOF 파일은 계속 커지므로, 주기적으로 압축 및 재작성이 필요합니다.Redis는 자동으로 ‘BGREWRITEAOF(AOF최적화 명려어)’를 수행해 오래된 명령을 요약합니다.rdb와 AOF조합 방법 RDB만 사용: 단순하고 빠른 복구가 필요한 경우에 사용합니다. AOF만 사용: 안정성이 중요한 경우에 사용합니다. RDB + AOF 함께 사용대부분의 실무 환경에서 추천됩니다.이 경우, 복구 시 AOF가 더 최신이면 AOF 우선으로 사용합니다.(AOF를 더 신뢰)Redis와 MemcachedRedis와 Memcached는 모두 인메모리 기반의 캐시 시스템이지만, 목적과 기능 측면에서 다음과 같은 차이점이 있습니다.데이터 구조 Redis는 리스트, 해시, 셋, 정렬셋 등 다양한 구조를 제공 → 큐, 랭킹, 통계에 유용합니다. Memcached는 단순한 문자열 key-value 구조 → 복잡한 로직은 애플리케이션에서 직접 구현해야 합니다.Persistence (데이터 보존) Redis는 RDB(rdb파일로 저장)/AOF(Append-Only File) 방식으로 디스크에 데이터를 저장 가능하며, 이를 통해 장애 후 복구가 가능합니다. Memcached는 서버 재시작 시 모든 데이터가 사라집니다.분산성과 고가용성 Redis는 Sentinel, Cluster 등을 통해 자동 failover 와 수평 확장이 가능합니다. Memcached는 클라이언트 단에서 key hashing을 통해 수동 샤딩(consistent hashing)을 구현해야 합니다.Pub/Sub 기능 Redis는 채널 기반 메시지 전달 기능(Pub/Sub)을 제공하여 실시간 이벤트 알림 등에 사용 가능합니다. Memcached는 이런 기능 없이, 단순 Cache용도로 사용합니다.요약 ‘Memcached’는 가볍고 빠른, 단순 캐시용으로 사용하고, ‘Redis’는 다기능 인메모리 데이터 플랫폼으로 사용합니다.Redis 사용 방법들Redis as a CacheRedis를 Cache로 사용할때의 FlowRedis를 Cache로 사용하는것은 가장 흔하게 사용되는 사례중 하나입니다. Redis를 Cache로 사용할때는, 각 key에 대해 TTL(Time-to-live)를 설정합니다.(이는 Redis가 데이터를 관리하는 방법을 가이드 해주는 역할을 합니다.) Cache용도로 Redis를 사용하다 보면, ‘Hot key’문제에 부딪히게 되는데, 이는 뒤에서 다루려고 합니다.Redis as a Distributed Lock‘Distributed Lock’으로 Redis를 사용하는것 또한 가장 흔한 사례중 하나입니다.만약 System에서, ‘Ticket 구매’와 같이 ‘Strong Consistency’가 필요한 경우, Redis를 사용하여 Lock을 구현할 수 있습니다. 만약, 사용하고 있는 DB레벨에서 이미 Consistency를 제공하고 있다면, 해당 기능을 쓰는게 좋습니다. Redis를 통해 Lock을 구현하면, 불필요하게 복잡도를 높이게 됩니다.Redis를 이용하여 ‘Distributed Lock(분산 Lock)’을 구현하는 알고리즘 및 프로토콜로 ‘Redlock’이 있습니다.Redis for LeaderBoardsRedis의 ‘sorted set’데이터 타입은 정렬된 데이터를 제공해주며, $log(N)$시간의 Query응답을 제공해줍니다.이는 LeaderBoard App에 적합한 스펙입니다.만약 LeaderBoard와 같이, Write throughput은 높고, Read Latency는 낮아야 하는 경우를 SQL DB로 대응하려고 하면, 꽤나 힘들겁니다.Redis for Rate LimitingRate Limiting은 특정 클라이언트가 지나치게 많은 요청을 보내지 못하도록 제한하는 기법입니다. Redis는 고속의 in-memory 연산 + TTL 기능 + 원자적 연산을 제공하기 때문에, ‘Rate Limiting’용으로 사용할 수 있습니다. 카운터 기반, 토큰 버킷, 슬라이딩 윈도우 등 다양한 알고리즘으로 지원할 수 있습니다.How to build a Rate Limiter using RedisRedis for Proximity Search(근접 검색, 예: 위치 기반 검색 또는 유사 단어 검색)Redis는 기본적으로 전통적인 RDBMS의 공간 인덱스(GIS) 나 벡터 검색 엔진은 아니지만, 몇 가지 기능을 조합하여 근접 검색을 구현할 수 있습니다.Getting Started With Geo Location Search in Redis‘Proximity Search에는 2가지 맥락이 있는데, ‘지리적 근접 검색’과 ‘유사 문자열 검색’이 그것입니다.Redis GEO 기능으로 위치 기반 Proximity SearchRedis는 GEOADD, GEORADIUS, GEODIST, GEOPOS 등의 명령어로 지리 정보(위도, 경도)를 저장하고 검색할 수 있습니다.내부적으로는 Geohash + Sorted Set으로 구현되어 있습니다.문자열 유사성 기반 Proximity SearchRedis 자체적으로 지원하지는 않고, RediSearch 모듈을 통해 구현할 수 있습니다.‘RediSearch’는 텍스트 인덱싱, 검색, 유사 단어 매칭, 벡터 검색 (ANN) 까지도 지원합니다.Redis for Event SourcingRedis 5.0부터 추가된 데이터 구조인 ‘Stream’을 통해, ‘Event Sourcing’패턴을 구현할 수 있습니다.Redis streams and consumer groups | hellointerview.com ‘Event Sourcing’은 상태를 저장하는 대신, 모든 변경 이벤트의 로그(event stream)를 기록해서, 나중에 그 이벤트들을 재생(replay)하여 현재 상태를 만들어내는 아키텍처 패턴입니다. ‘Redis Stream’은 ‘Append-only Log’처럼 작동하는 자료구조입니다. 시간 순서대로, Log형식으로 데이터를 저장합니다. 내부적으로는 Kafka의 topic-like 구조와 매우 유사하며, ID 순 정렬, 범위 조회, consumer group 처리 등이 가능합니다.때문에 ‘Stream’을 통해 ‘Event Sourcing’을 구현할 수 있으며, 메모리 기반이라 매우 빠른 성능을 제공해줍니다.(Redis 특성에서 오는 데이터 유실 가능성이 있음.)Redis for Pub/SubRedis Pub/Sub | from geeksforgeeks.orgRedis는 자체적으로 ‘publish/subscribe(Pub/Sub)’ Messaging pattern을 지원하며, 메시지 브로커처럼 채널 기반으로 메시지를 전달하는 기능입니다.이는 주로 Chat System이나 실시간 Notification 혹은 ‘Message생산자와 소비자를 decouple하는 시나리오’에서 사용됩니다. Redis는, Pub/Sub에 대해서도 sharding을 지원합니다. (Redis 구버전에서는 불가)장점 메모리 기반이라 초 저지연 메세징 기능을 제공합니다. 간단한 설정으로 바로 사용할 수 있습니다. Pub/Sub Client는 Redis Cluster를 구성하는 각 Node마다 하나의 Connection을 사용합니다.Pub/Sub Channel마다 하나씩 쓰는게 아니라서, 커넥션 사용을 최소화할 수 있습니다.단점 메세지 영속성(Persistence)이 없습니다.Subscriber가 연결되어 있지 않으면, 메시지는 버려집니다. 큐(queue) 가 아닌, 모든 구독자에게 동시에 전달하는 ‘Broadcast’구조입니다. Redis의 단점과 개선방법Hot Key IssuesRedis Cluster에서 0-100범위에 대해 Hot Key Issue가 발생 | from hellointerview.com ‘Hot Key Issues’는 특정 키(key)에 과도하게 많은 요청이 집중되어 시스템의 성능 저하 또는 병목이 발생하는 문제를 말합니다. Redis의 특정 key에 트래픽이 몰리면서 해당 key와 연관된 노드나 자원이 비정상적으로 과부하 되는 문제입니다.Redis는 기본적으로 단일 스레드 기반이기 때문에, 하나의 key에 너무 많은 명령이 몰리면 그 key를 포함한 처리 루프 전체가 지연될 수 있습니다.만약 Cluster환경이라면, 해당 key가 할당된 해시 슬롯(slots)이 포함된 특정 노드에만 부하가 집중될 수 있으며, 이는 노드간 불균형(CPU, 메모리, 네트워크 I/O에 대한)을 초래합니다.해결 방법 Client level에서 In-memory Cache를 추가하여, Redis에 너무 많은 요청이 발생하지 않도록 합니다. Redis에서 사용하는 Key에 Random number를 추가하여, 여러 Node에 걸쳐서 데이터가 분산되도록 합니다. (Key Sharding)예: “rank:global” → “rank:global:shard1”, “rank:global:shard2”Big Key Issues Big Key Issue란, Redis의 단일 key가 너무 많은 요소(예: 리스트, 해시, 셋 등)를 가지고 있어서, 해당 key에 대한 연산이 느려지거나 전체 Redis 인스턴스에 영향을 주는 문제입니다. key 자체가 크다는 의미가 아니라, 하나의 key에 저장된 데이터가 지나치게 많은 경우를 말합니다. ‘Big Key’에 대한 정의 및 평가 기준은 실제 사용 및 애플리케이션의 특정 요구 사항에 따라 달라질 수 있습니다.예를 들어, 높은 동시성 및 낮은 지연 시간 시나리오에서는 10KB의 키만 빅 키로 간주될 수 있습니다. 그러나 낮은 동시성 및 고용량 환경에서는 빅 키의 임계값이 약 100KB일 수 있습니다해결 방법 Key를 나눕니다.(Sharding / Chunking)큰 리스트를 “chat:room:123:page:1”, “chat:room:123:page:2” 처럼 분할합니다. Key에 대한 전체 연산은 피합니다. 문자열 data인 경우, 압축을 고려합니다.References Redis | hellointerview.com Hello Interview | System Design in a Hurry Redis About | redis.io About - Redis Redis Distributed Caching | redis.io Distributed Caching Redis Cluster Architecture | redis.io Redis Cluster Architecture | Redis Enterprise Three Ways to Maintain Cache Consistency | redis.io Three Ways to Maintain Cache Consistency | Redis Redis Race Condition | redis.io Redis Race Condition High-Concurrency Practices of Redis: Snap-Up System | alibabacloud.com High-Concurrency Practices of Redis: Snap-Up System Understanding the Failover Mechanism of Redis Cluster | alibabacloud.com Understanding the Failover Mechanism of Redis Cluster Snapshotting | redis.io Redis persistence Append-only file | redis.io Redis persistence Redis Architecture의 진화 | bytebytego.com ByteByteGo | How Redis Architecture Evolved What makes Redis lightning fast ? | engineeringatscale.substack.com What makes Redis lightning fast ? Linux – IO Multiplexing – Select vs Poll vs Epoll | devarea.com Linux – IO Multiplexing – Select vs Poll vs Epoll BGREWRITEAOF | redis.io BGREWRITEAOF Distributed Locks with Redis | redis.io Distributed Locks with Redis How to build a Rate Limiter using Redis | redis.io How to build a Rate Limiter using Redis Getting Started With Geo Location Search in Redis | redis.io Getting Started With Geo Location Search in Redis How to Use Redis as an Event Store for Communication Between Microservices | redis.io How to Use Redis as an Event Store for Communication Between Microservices | Redis" }, { "title": "Consistent Hashing | Core Concepts - System Design Interview", "url": "/posts/Consistent-Hashing/", "categories": "System Design Interview, Core Concepts", "tags": "System Design, interview, Computer Science, Consistent Hashing, hash", "date": "2025-07-30 02:19:00 +0900", "snippet": " ‘Consistent Hashing’은 분산 시스템(Distributed System)의 Cluster에서, 데이터를 분산 저장할때 사용하는 기초적인 알고리즘 입니다.예시로 보는 Consistent Hashing의 필요성‘Ticketing System을 구성한다고 해봅시다.Simple System과 Sharding이 적용된 SystemClient...", "content": " ‘Consistent Hashing’은 분산 시스템(Distributed System)의 Cluster에서, 데이터를 분산 저장할때 사용하는 기초적인 알고리즘 입니다.예시로 보는 Consistent Hashing의 필요성‘Ticketing System을 구성한다고 해봅시다.Simple System과 Sharding이 적용된 SystemClient-Servcer-DataBase가 하나씩 구성되어 있는 ‘Simple System’으로 운영이 가능할때는 괜찮지만, 곧 다루는 이벤트(행사의 이벤트)가 늘면서 Data를 여러 Node에 분산시켜야 하게 됩니다.데이터를 분산시키는 방법 : Simple Modulo Hashing단순한 Modulo를 이용한 알고리즘가장 단순하게 접근하면, Event의 ID를 Hash처리하여, number값으로 만듭니다. 해당 number값을 Modulo(%)연산을 통해 데이터베이스를 할당합니다. 할당된 DB에 데이터를 저장합니다.이때의 문제점DB Cluster에 Node가 추가된다면..이때, DB Cluster에 Node가 추가된다면, Modulo의 값이 바뀌게 되면서, 전체 Data가 재분배(redistributed)되어야 하는 상황이 발생합니다. 이런 Data 재분배(redistributed)상황은 DB의 많은 리소스를 차지해서, 시스템에 장애를 만들어낼 수 있습니다. 즉, DB운영 작업에 리소스가 많이 투입되어, 외부 요청을 처리하지 못하게 됩니다.DB Cluster에 Node가 제거된다면..반대로, DB Cluster에서 Node가 제거되는 상황에서도, Data 재분배(redistributed)가 발생하게 됩니다.Consistent Hashing의 필요성‘Consistent Hashing’은 이런, 분산환경(Distributed System)에서 DB Instance구성이 변경되는 상황에 대한 솔루션을 제공해줍니다. Q: 여기서 ‘Consistent’의 의미는 무엇인가요?A: 노드가 추가되거나 제거되더라도(분산 환경 변화에도) 키–노드 매핑이 일관되게(consistent) 유지된다는 특성에서 유래하고 있습니다.이전 방식인 Modulo방식은 Node 구성이 변경됨에 따라, 전체 데이터가 re-hashing되어야 하지만, ‘Consistent Hashing’은 일관된 Hash값을 제공하여, re-hashing을 최소화합니다.핵심 아이디어는, Data와 Database를 ‘hash ring’이라 불리는 ‘Circular space(순환 공간)’에 정리해 두는 것입니다.Consistent Hash RingConsistent Hashing의 작동 과정 DB Node가 4개 있다고 가정했을때, 이를 통해 ‘Hash Ring’을 구성합니다.그리고 이 Hash Ring의 고정값의 범위를 0 ~ 100으로 구성합니다.(예시로 구성, 100 이상도 가능합니다.)DB Node들은 이 Hash Ring에 일정하게 분포되도록 합니다.(데이터를 균일하게 분포시키기 위해) Hash Ring | hellointerview.com 여기서 삽입(insert)하고자 하는 데이터의 hash값이 만들어지면, 시계 방향으로 제일 가까운 DB Node에 저장합니다.이 과정은, 아래와 같습니다. 데이터의 Key값을 Hash처리 합니다. Hash값을 기반으로, Hash Ring위의 위치를 계산합니다.이때, Hash처리한 값을 Hash Ring위의 좌표로 변환하기 위해, Hash Ring 값의 범위로 Modulo하는 과정을 거칩니다. \\(h_{\\rm ring} = \\text{hash}(\\,\\text{key}\\,) \\bmod M \\text{(M은 Ring의 크기)}\\) 이는, $H = \\text{hash}(key)$의 값의 범위가, 보통 Hash Ring의 범위보다 크기 때문에 필요합니다. $h_{\\rm ring}$(Hash Ring위의 좌표)에서 값을 증가시키며, 가장 가까운 DB Node를 찾습니다.(Ceil연산) DB Node를 찾게 되면, 해당 Node에 데이터를 저장합니다. DB Node가 추가되는 경우DB Node가 추가되는 경우 | hellointerview.com이 경우, 전체 데이터가 아닌, 일부 데이터에 대해 데이터 재배치가 이루어집니다. 만약, DB4와 DB1사이에 새로운 DB5를 위치시킨다면, DB1에 가야 했던 일부 데이터가 DB5로 재배치됩니다. 다른 데이터들은 그대로 위치합니다. 이 예시의 경우, DB1에 있던 데이터의 약 30%정도의 데이터만 재배치 됩니다.(Hash Ring의 값에 따른 추정)DB Node가 제거되는 경우DB Node가 제거되는 경우 | hellointerview.com이 경우, 제거된 Node에 있던 데이터만 재배치 됩니다. 만약, DB2가 시스템 장애로 Cluster에서 제거됐다면, DB2에 있던 데이터 전부가, DB3에 재배치됩니다. 다른 데이터들은 그래도 위치합니다.여기까지, ‘단순 Modulo방식’에 비해 많이 개선된 부분이 있지만, 여전히 ‘DB Node가 제거되는 경우 Node의 전체 데이터가 재배치되는 문제’가 있습니다.이는 Hash Ring위에서 DB Node간 데이터양의 불균형을 만들어냅니다.이를 해결하기 위해, ‘Virtual Node’로 Hash Ring을 구성하는 방법을 사용합니다.Virtual Nodes로 Hash Ring 구성하기Hash Ring을 Virtual Node로 구성합니다. | hellointerview.com 이 ‘Virtual Nodes’방법은, Hash Ring위에 단 하나의 지점에만 Node를 배치하는것이 아닌, 여러 지점에 가상(Virtual)으로 배치하는 방법입니다. 만약, DB2가 Fail상태가 된다면, Hash값에 따라, 데이터가 재배치 됩니다. 이때, Hash값에 따라, 남아 있는 다른 Node(DB1, DB3, DB4)에 분산되어 재배치됩니다.즉, Node가 Fail상태가 되면, 해당 Node의 데이터가 여러 Node로 분산되어 재배치됩니다.Virtual Nodes를 사용하는 이유 데이터 분포 균일화실제 노드가 링 위에 딱 하나의 점으로만 존재하면, 해시 함수 특성에 따라 특정 영역에 데이터가 몰릴 수 있습니다.각 실제 노드를 여러 개의 작은 “가상 노드”로 분할해 링 위에 고르게 흩어 놓으면, 키가 더 고르게 분산됩니다. 노드 용량·성능 차이 반영머신마다 처리 성능이 다를 때, 노드마다 할당할 가상 노드 수를 달리 줌으로써 “무거운” 머신에 더 많은 샤드를 몰아줄 수 있습니다.예) CPU·메모리가 2배인 노드는 가상 노드를 2배 배치 노드 추가·제거 시 부드러운 재배치실제 노드 하나를 추가/삭제할 때마다 전체 키 공간 중 가상 노드 하나 분량만 이동하면 되므로,“영향받는 키 비율”이 1/N → 1/(N·v) 수준으로 더욱 작아집니다.(여기서 v는 각 실제 노드당 가상 노드 개수) 운영 유연성가상 노드 단위로 릴리스·점검이 가능해, 실제 노드를 직접 건드리지 않고도 롤링 업데이트나 장애 격리가 수월해집니다. Consistent Hashing 사용 사례‘Consistent Hashing’은 데이터를 분산하는 ‘방법’에 해당하기 때문에, DB뿐만 아니라, Cache, Message Broker등의 사례가 있습니다.Apache Cassandra ‘Apache Cassandra’는 분산 키-값 저장소로, 내부적으로 Consistent Hashing 기반의 토큰 링(token ring) 구조를 사용해 데이터를 분산·저장합니다 Node(virtual node) 개념을 도입해, 클러스터 내 각 물리 Node에 여러 개(기본 256개)의 토큰을 랜덤 배치하며, 데이터 분포를 훨씬 고르게 만듭니다. Apache Cassandra는 Amazon의 ‘Dynamo’ 분산 저장 시스템을 사용하고 있습니다.(이런 Case를 Dynamo-style 시스템이라고 합니다.)Apache Cassandra의 Token Ring | cassandra.apache.orgAmazon DynamoDB ‘Amazon DynamoDB’은 완전관리형 NoSQL 키–값·문서(Document) 데이터베이스입니다. 데이터의 ‘Partition’ 과정에서 ‘Consistent Hashing’을 사용하고 있습니다.Content Delivery Networks(CDNs) ‘CDNs’은 전 세계에 분산된 엣지(Edge) 서버들에 콘텐츠를 캐시(Cache)하고, 사용자의 요청을 가장 적절한 서버로 라우팅해 응답 지연(latency)을 줄이는 시스템입니다. CDN에서 Server(Node) Pool을 관리할때, ‘Consistent Hashing’을 사용합니다. 요청 URL에 따라 이 Traffic을 처리할 Node가 정해지는 방식입니다. 단순히 URL로만 Node가 정해지면, 특정 Server(Node)가 ‘Hot’상태에 도달하게 됩니다.때문에, 실제 CDN의 설계는 Consistent Hashing 단독이 아니라, 여러 기법을 조합해 핫스팟을 완화하게 됩니다.(인기 콘텐츠일 수록 여러 Edge서버에 복제본을 두는 등..의 방식 사용)References Consistent Hashing | hellointerview.com Hello Interview | System Design in a Hurry Consistent hash ring | researchgate.net FIGURE 4: Consistent hash ring and forwarding process. Consistent Hashing Explained | systemdesign.one Consistent Hashing Explained Design Consistent Hashing | bytebytego.com System Design · Coding · Behavioral · Machine Learning Interviews Dynamo | cassandra.apache.org Apache Cassandra relies on a number of techniques from Amazon’s Dynamo distributed storage key-value system. Apache Cassandra에서의 분산 저장 시스템은 Amazon의 Dynamo의 테크닉에 영향을 받았다는 얘기.(Dynamo-style 시스템) Dynamo | Apache Cassandra Documentation Dynamo: Amazon’s Highly Available Key-value Store(논문) | www.cs.cornell.edu www.cs.cornell.edu How is hashing speeding up your CDN | cdn77.com How is hashing speeding up your CDN | CDN77.com Distributing Content to Open Connect | netflixtechblog.com We use Consistent Hashing to distribute content across multiple servers as follows. Distributing Content to Open Connect" }, { "title": "CAP Theorem(정리) | Core Concepts - System Design Interview", "url": "/posts/CAP-Theorem/", "categories": "System Design Interview, Core Concepts", "tags": "System Design, interview, Computer Science, CAP Theorem, SAGA Pattern", "date": "2025-07-25 00:04:00 +0900", "snippet": "CAP Theorem 소개CAP는 각각 Consistency, Availability, Partition Tolerance를 의미합니다. 이 ‘CAP Theorem’은 ‘Distributed System(분산처리 시스템)’의 3가지 핵심 속성에서, 이중 딱 2개만 취할 수 있다는 theorem(정리, 일정한 조건하에 참이라는 것이 증명됨)입니다. ...", "content": "CAP Theorem 소개CAP는 각각 Consistency, Availability, Partition Tolerance를 의미합니다. 이 ‘CAP Theorem’은 ‘Distributed System(분산처리 시스템)’의 3가지 핵심 속성에서, 이중 딱 2개만 취할 수 있다는 theorem(정리, 일정한 조건하에 참이라는 것이 증명됨)입니다. 즉, 3가지 속성사이의 Trade-off 관계를 설명하는 이론입니다.CAP theorem Euler diagram | en.wikipedia.org각각의 속성은 다음과 같습니다. Consistency(일관성)모든 노드가 ‘같은 시점에 동일한 데이터를 갖고 있다는것’을 보장하는것을 말합니다.분산환경에서, 클라이언트가 어떤 노드에 요청하든 항상 동일한 응답을 받을 수 있습니다. 여기서의 Consistency는 DB의 ACID에 있는 Consistency와는 다른 맥락을 갖고 있습니다.ACID의 Consistency의 경우, 트랜잭션 전후, 데이터에 대한 ‘무결성 제약조건’을 말합니다.CAP의 Consistency: “우리 가게 모든 지점에서 같은 가격표를 붙이자”ACID의 Consistency: “가격표는 항상 숫자이고, 재고보다 많은 수량은 팔 수 없다” Availability(가용성)모든 요청에 대해 ‘항상 응답이 오며, 실패하지 않는것’을 말합니다.응답이 늦어지거나 오류 없이 처리되는걸 의미합니다. Partition Tolerance(분할 허용)네트워크가 분할되어 일부 노드 간 통신이 불가능해져도, 시스템은 계속 동작해야 하는것을 말합니다.CAP조합과 적용 예시 조합 설명 대표 시스템 예시 CP (일관성 + 분할 허용) 네트워크 분할 시, 일부 요청은 차단되더라도 일관성을 유지합니다. HBase, MongoDB (옵션에 따라), Redis Sentinel AP (가용성 + 분할 허용) 네트워크 분할 시에도 응답을 주지만, 일관성은 잠시 깨질 수 있습니다. Cassandra, Couchbase, DynamoDB CA (일관성 + 가용성) 네트워크가 항상 정상적이라는 가정에서 가능합니다. (현실적으로는 어려움) 단일 노드 시스템 (e.g. RDBMS에서 분산 미적용 시) 현대 분산환경에서, Partition Tolerance는 필수.현대 분산 시스템은 네트워크 지연, 패킷 손실, 장애 등으로 인해 Partition(분할)이 언제든 발생할 수 있으므로, P를 포기할 수 없습니다.따라서, CAP 이론의 실질적인 선택은 C와 A 중 어떤 것을 포기할 것인가에 대한 문제입니다.Network Partition(네트워크 분할)이 발생했을때, ‘Consistency와 Availability중 어떤걸 선택하는냐’의 문제라고 정리할 수 있습니다.CAP Theorem 시나리오 예시시나리오상의 Idle 시스템 상황우리가, USA와 유럽 Region에 각각 서버를 운영하고 있다고 가정합니다.만약 유저가 자신의 프로필 정보(Public으로 노출되는)를 변경한다면, 다음과 같은 Flow가 만들어질 수 있습니다. UserA는 USA에 서버에 있는 프로필 정보를 update합니다. USA에서 변경된 Profile이 유럽으로 복제(전파)됩니다. UserB가 유럽서버에 대해, UserA의 정보를 조회합니다.시나리오상 Idle상황문제 발생Region간 Network가 끊긴 상황이런 Flow에서, USA와 유럽서버간 Network연결이 끊긴다면, 우리는 2가지 옵션중 선택해야 합니다. Option A (Consistency 우선 옵션)Error을 Return합니다. 우리는 최신정보가 반환되지 못하면 에러로 판단합니다.(Consistency를 선택하는 경우) Option B (Availability 우선 옵션)최신데이터가 아니어도, 데이터를 반환합니다.(Availability를 선택하는 경우) 시나리오 결과 정리이렇게, C(Consistency)와 A(Availability)중에 반드시 하나를 선택해야 하는 상황이 만들어집니다.‘시스템의 방향’과, ‘제공하고자 하는 서비스의 형태’에 따라 적합한 전략을 취해야 합니다.CAP Theorem과 시스템 디자인 인터뷰‘CAP Theorem’를 다루는건, 시스템 디자인 인터뷰에서 첫번째로 해야하는것 중 하나입니다.시스템 디자인 인터뷰는, 두가지 핵심적인 요구사항을 정리하면서 시작합니다. functional requirements(Feature)를 정리합니다.(반드시 달성해야 하는것들) non-functional requirements를 정의합니다.(system의 quality와 관련된것들)이때, ‘non-functional requirements’를 고려할때, CAP theorem이 ‘시작점’ 역할을 할 수 있습니다. 이때, 스스로에게 다음의 질문을 하면 좋습니다.“이 시스템에서, Consistency와 Availability중에 어떤걸 우선시 해야 할까?”Consistency를 우선시 한다면..다음을 포함시켜, 시스템을 디자인하는게 좋습니다. Distributed Transactions(분산 트랜잭션)여러개의 Data Source간에 강하게 Sync되도록 하려면, ‘two-phase commit protocol‘을 사용해야 합니다. Two-phase commit protocol(2PC, 2단계 커밋 프로토콜) :분산 시스템에서, 여러 노드가 하나의 트랜잭션에 참여할 때, “모두 성공하거나, 모두 실패”해야 합니다. 이를 위해 2PC컨셉을 적용하여, 트랜잭션을 두 단계로 나누어 처리하는것을 말합니다. 이는 시스템의 복잡도를 높이지만, 모든 Node들에 걸친 Consistency를 보장합니다.이 경우, 유저들이 높은 Latency를 경험할 수 있습니다.(여러 Node에 대한 consistency를 위해 시간이 오래걸려서) Single-Node Solutions하나의 DB Instance를 사용하여, 장애가 전파되는 문제를 해결할 수 있습니다.이 경우, Scalability(확장성)을 제한하게 되지만, ‘Single source of Truth’가 되어 Consistency를 쉽게 확보할 수 있습니다. Technology Choices는 다음과 같이.. PostgreSQL, MySQL과 같은 전통적인 RDBMS Goole Spanner Strong consistency mode를 기반으로한 DynamoDB Availability를 우선시 한다면…다음을 포함시켜, 시스템을 디자인하는게 좋습니다. Multiple Replicas여러 복제본을 만들어서, Replication환경을 만듭니다.(몇몇 Replica가 최신화 되지 않을 수 있는 환경을 허용)이는 Read에 대한 퍼포먼스와 가용성(Availability)에 대한 큰 개선을 제공해줍니다. Change Data Capture(CDC)Primary DB의 데이터가 바뀐다면, 이를 비동기적으로 Replica나 Cache혹은 다른 시스템에게 전달합니다.이를 통해 업데이트가 시스템에 반영되는 동안 기본 시스템을 계속 사용할 수 있습니다. Technology Choices는 다음과 같이… Cassandra 여러 AZ에 걸친 Cluster형 DynamoDB Redis Cluster 현대의 DB들은 ‘Consistency’와 ‘Availability’에 대한 옵션 모두 제공한다고 합니다.(Configuration을 통해, 둘중 하나를 선택할 수 있다는 뜻)Consistency LevelCAP에서 Consistency는 ‘Strong Consistency’만 의미하는것은 아닙니다.Consistency에는 여러 Spectrum이 있으며, 이를 이해하는것은 시스템 디자인을 할때에 큰 도움을 줍니다.Strong Consistency (강한 일관성)데이터 쓰기(write) 직후에, 모든 읽기(read)에 그 값이 반영되어야 합니다.항상 최신값을 읽을 수 있지만, Consistency Model중에 가장 비용(컴퓨팅 리소스 관점)이 비쌉니다.주로 ‘은행 계좌의 잔액’에 사용됩니다.예시 DB Spanner (Google): TrueTime을 기반으로 strong consistency 보장합니다. etcd, ZooKeeper: 리더를 통해 순차적으로 처리합니다. MongoDB (readConcern: “linearizable”): 선택적으로 제공Causal Consistency (인과 일관성)관련된 Event들이 모든 유저들에게 동일하게, 동일한 순서로 반영되는것을 말합니다.서로 의존성이 있는 Action들의 논리적 순서를 보장합니다.예를 들면, Post에 Comment를 단다면, Post가 먼저 존재해야합니다. ‘Causal Consistency’는 이 순서를 보장해 줍니다.예시 DB: Cassandra (with client-side tracking) Azure Cosmos DB (선택 가능)Sequential Consistency (순차 일관성) Sequential consistency is a consistency model used in the domain of concurrent computing (e.g. in distributed shared memory, distributed transactions, etc.).- from en.wikipedia.org모든 연산이 일관된 순서로 적용됩니다. 위의 ‘Causal Consistency’와 유사해 보이지만,‘Causal Consistency’가 Action간의 의존성이 있는경우에만 그 순서를 보장한다면,‘Sequential Consistency’은 의존성과 상관없이 모든 Action들의 순서에 대한 일관성을 보장합니다.대표적으로 ‘concurrent computing’에 쓰입니다.각 사용자가 보는 ‘연산 순서’는 동일하지만, 보는 시점에 따라 최신 값이 아닐 수도 있습니다.예시 DB: 일부 메지 큐 시스템, 일부 분산 캐시‘Causal Consistency’와 ‘Sequential Consistency’ 비교 항목 Sequential Consistency Causal Consistency 순서 기준 모든 연산을 하나의 글로벌 순서로 정렬합니다. 인과관계(causal relationship)만 보장합니다. 전체 순서 필요 여부 모든 연산 순서를 동일하게 유지합니다. 인과관계가 있는 연산만 순서 보장합니다. 병렬 연산 간 순서 순서를 강제로 정합니다. 자유롭게 재배열 가능 (인과관계 없으면) 성능 더 느릴 수 있습니다. 더 빠르고 병렬성이 높습니다. Read-your-own-writes(RYW) Consistency‘내가 쓴(write) 데이터’는 내가 읽을 때 항상(‘즉시’ 포함) 보이는 일관성을 말합니다. ‘다른 유저들은 Older version을 read할 수 있음’을 허용합니다.주로 소셜미디어에서 사용합니다.예시 DB: MongoDB (with session): 같은 세션 내에서 RYW 보장합니다. Firebase: 클라이언트 기반 동기화에서 자주 사용합니다.Eventual Consistency (최종 일관성)시간이 지나면 언젠가 모든 노드가 일관된 상태에 도달하는 형태의 일관성입니다. ‘일시적으로 일관성이 깨지는것’을 허용합니다.가장 허용적인(느슨한) Consistency입니다.DNS사용 되며, 대부분의 Distributed Database에서의 Default 설정입니다.예시 DB: DynamoDB, Cassandra, Riak S3, DNS: 변경 직후에 전파가 늦어질 수 있음Eventual Consistency Pattern들 | from bytebytego.comEvent-based Eventual Consistency한 서비스(Service A)가 Event를 실행하면, 다른 서비스가 그 이벤트를 받아서 자기 자신의 시스템에 반영합니다.Background Sync Eventual Consistency여기서는 별도의 Background Job(Cron같은)이 Database Node간에 데이터를 일치시켜, Consistency를 만들어 냅니다.스케쥴되어서 실행되기 때문에, 훨씬 느린 Consistency를 제공합니다.Saga-based Eventual Consistency ‘Saga-based Eventual Consistency’는 분산 트랜잭션을 처리하는 현대적인 방식으로, 특히 마이크로서비스 아키텍처에서 많이 사용됩니다. 2PC의 한계(복잡성, 블로킹)를 극복하고, 일관성보다 가용성과 확장성을 우선할 때 사용합니다. 트랜잭션을 여러 개의 작은 지역 트랜잭션(local transaction)으로 쪼개고, 각 단계가 실패하면 보상 작업(compensation, 흔히 rollback과정이라고 불리는)을 수행하여 이전 상태로 되돌리는 방식입니다. Saga Pattern은 느슨한 결합을 추구합니다.이를 통해 서비스 간 의존성을 최소화하고, 서로 독립적으로 개발·배포·운영될 수 있도록 만드는 구조가 만들어집니다. ‘Event-based Eventual Consistency’유사하지만, ‘Event-based Eventual Consistency’는 단위가 Event인 반면,‘Saga-based Eventual Consistency’는 ‘Transaction’단위로 작동합니다. 항목 Saga-based Eventual Consistency Event-based Eventual Consistency 핵심 개념 트랜잭션 단위로 보상/실패 흐름을 정의 이벤트를 기반으로 비동기적 상태 동기화 구조 보상 트랜잭션 정의 필수 보상 없음 (대부분 리드모델 갱신) 사용 목적 분산 비즈니스 트랜잭션 처리 시스템 간 데이터 복제/동기화 대표 시나리오 결제 실패 시 환불, 롤백 등 주문이 생성되면 배송 시스템이 동기화 CQRS(Command Query Responsibility Segregation)-based Eventual Consistency읽기(Read, Query)와 쓰기(Write, Command) 작업에 대한 책임(Responsibility)을 각각의 DB로 분리하는 ‘Eventual Consistency’방식입니다.보통 아래와 같은 상황에서 사용됩니다. 읽기와 쓰기 성능 요구가 비대칭일 때. 조회 요구사항이 복잡할 때. 확장성과 응답속도가 중요한 시스템일때. 일관성보다 가용성과 유연성이 중요한 시스템일때.References CAP Theorem | hellointerview.com Hello Interview | System Design in a Hurry Spanner, TrueTime &amp; The CAP Theorem | static.googleusercontent.com static.googleusercontent.com Towards robust distributed systems (abstract) | dl.acm.org Towards robust distributed systems (abstract) | Proceedings of the nineteenth annual ACM symposium on Principles of distributed computing Top Eventual Consistency Patterns You Must Know | bytebytego.com ByteByteGo | Top Eventual Consistency Patterns You Must Know CAP, PACELC, ACID, BASE - Essential Concepts for an Architect’s Toolkit | blog.bytebytego.com CAP, PACELC, ACID, BASE - Essential Concepts for an Architect’s Toolkit Two-phase commit protocol | en.wikipedia.org Two-phase commit protocol Engineering Trade-offs: Eventual Consistency in Practice | blog.bytebytego.com Engineering Trade-offs: Eventual Consistency in Practice" }, { "title": "What is Kafka? | CloudNative", "url": "/posts/What-is-Kafka/", "categories": "DevOps, CloudNative", "tags": "aws, kubernetes, cncf, k8s, kafka", "date": "2025-07-23 14:09:00 +0900", "snippet": "Kafka 소개 Kafka는 대용량의 실시간 데이터 스트리밍을 처리하는 분산(Distributed) 메시징 시스템입니다. 원래 LinkedIn에서 개발되었고, 이후 Apache Software Foundation에서 오픈소스로 관리되고 있습니다.탄생 배경(LinkedIn에서) 로그와 사용자 이벤트 데이터 폭증‘LinkedIn’에서는 We...", "content": "Kafka 소개 Kafka는 대용량의 실시간 데이터 스트리밍을 처리하는 분산(Distributed) 메시징 시스템입니다. 원래 LinkedIn에서 개발되었고, 이후 Apache Software Foundation에서 오픈소스로 관리되고 있습니다.탄생 배경(LinkedIn에서) 로그와 사용자 이벤트 데이터 폭증‘LinkedIn’에서는 Web에서 사용자들의 행동을 추적(tracking)하고 있었는데, 사용자가 급격하게 늘어남에 따라, 데이터가 폭발적으로 증가하였습니다.수천만 명의 사용자가 활동하면서 광고 클릭, 검색, 프로필 조회, 추천 요청 등의 이벤트가 초당 수십만 건씩 발생하였으며, 기존의 로그 수집 시스템(파일 로그 → 수집기 → Hadoop ETL)은 이를 감당하지 못했습니다. 실시간 분석 불가능모든 데이터는 Hadoop에 쌓인 후 배치 처리해야 했습니다.(즉, 실시간 분석이 불가능.)실시간 분석은 거의 불가능하고, 결과를 보려면 수 시간 또는 하루 이상 지연되었습니다. AMQP(Advanced Message Queuing Protocol)의 한계LinkedIn에서 처음에는 AMQP서비스들을 고려했다고 합니다. 하지만, AMQP로 분산처리 환경을 구현하는데에 여러 어려움이 있었습니다. 이를 극복하기 위해, 기존에 있던 Messaging System을 고려했지만, 적합한게 없어, Kafka를 만들게 되었습니다.Kafka의 초기 설계 철학Kafka는 단순한 메시지 큐가 아니라, 범용 로그 시스템(log-centric system)으로 설계되었습니다.핵심 철학은 다음과 같습니다.모든 데이터를 이벤트 로그로 저장합니다.이벤트 중심(Event-driven) 아키텍처를 채택하였습니다. 모든 시스템 간 통신과 상태 변화를 이벤트 로그로 기록·전파하여, 비동기·분산·확장 가능한 구조를 구현했다는 의미입니다.모든 시스템 간 통신도 로그 기반 이벤트 스트림으로 표현합니다.고성능 &amp; 고내구성디스크에 바로 쓰되, 디스크 I/O를 최적화하여 메모리 버퍼처럼 빠르게 동작합니다.메시지를 디스크에 저장하되 복제(replication)로 데이터 유실 방지합니다.단순하고 확장 가능한 APIProducer, Consumer 모두 단순한 API로 통신합니다.파티셔닝(partitioning)을 통해 수평 확장(Horizontal Scaling)이 쉽습니다.리플레이 가능한 스트림 처리데이터는 소비 후에도 삭제되지 않으며, 필요할 때 다시 읽을 수 있습니다.이 덕분에 재처리, 재분석, 에러 복구가 쉬워집니다.다양한 시스템 연결을 위한 허브Kafka를 중심으로 Hadoop, HDFS, Cassandra, Storm 등과 연계하곤 합니다.즉, Kafka는 데이터의 허브가 됩니다.Kafka ArchitectureKafka는 여러 Broker를 통해 Cluster를 생성하여, 분산처리를 위한 Architecture를 갖추고 있습니다.Kafka Cluster Architecture | from ibm-cloud-architecture.github.ioKafka Architecture with Offset | from d3s.mff.cuni.czComponentsBrokersKafka는 여러 Broker로 이루어진 Cluster로 운영됩니다. 즉, Broker는 Kafka 서버 인스턴스 하나하나를 의미합니다. 이 Broker들은 데이터의 Replication을 관리하고, Topic/partition을 관리하고, Broker안에 있는 Partition의 Offset을 관리합니다.Broker에는 Topic(실제로 Record Stream이 저장되는 곳)의 Partition이 저장됩니다. 만약, 여러 데이터센터를 걸쳐서 Kafka를 구성한다면, 센터간 15ms이하의 network latency를 요구합니다.이는 Kafka Broker와 zookeeper사이에 많은 통신이 있기때문입니다. Production level에선, 최소한 5개의 노드(Broker)를 구성하는게 좋다고 합니다.Zookeeper(최신버전에서는 KRaft Controller 사용) ‘Zookeeper’는 Component나 Kafka의 상태를 유지하는데 사용됩니다. HA(High Availability)를 위해, Cluster형태로 실행됩니다. 클러스터의 메타데이터(파티션 배치, ISR, 브로커 생·사 등) 관리합니다. Kafka Version에 따라, zookeeper에서 offset을 관리합니다.최신 버젼에서는 Kafka내부에서 ‘consumer offset’이라는 이름으로 관리합니다. Zookeeper는 Kafka 3.5부터 deprecated처리 되며, 4.0부터는 완전히 제거됩니다.KRaft Controller(4.0이상부터 default) KRaft Controller는 Zookeeper의 역할을 대체하면서도, Kafka 외부(Zookeeper)의 도움 없이 Kafka 스스로 클러스터가 운영될 수 있도록 합니다. Kafka의 아키텍처를 단순화하고, 운영 부담을 감소시켜 줍니다.(ZooKeeper에 대한 관리가 불필요)ZooKeeper mode vs. KRaft modeKafka 3.3부터 KRaft가 새 클러스터에 “프로덕션 레디”로 승인되고, 3.5에서 Zookeeper 모드가 deprecated, 4.0에서 제거될 예정입니다.(현재시점 4.0.0 출시) ‘Raft’는 뗏목을 의미하며, Cluster에서 Leader를 선출하는 합의 알고리즘중 하나입니다.Topics과 Partitions, Replication ‘Topic’은 메시지 논리 채널입니다. Record를 Publish하고, Consume하는 엔드포인트를 제공합니다. 하나의 Topic을 N개의 Partition으로 나누어, 각 Partition은 독립적인 append-only(추가만 되는) 로그로 관리합니다.하나의 Topic은 여러개의 Partition과 Replicas로 구성되어 있습니다. Partition은 단일 서버가 모든 이벤트를 처리할 수 없을 때, Broker Clustering을 사용하여 이벤트 처리를 병렬화하는 데에 사용됩니다. Topic을 병렬 처리 단위로 분할하는 개념이며, Broker에 분산 배치되어 쓰기·읽기 부하 분산 구현합니다. Consumer나 ‘Traffic pattern’에 따라 파티션의 갯수를 조절할 수 있으며, 각 Broker는 2000개의 파티션을 가질 수 있습니다. append-only Log파일로 구현되어 있으며(Disk에 저장), 오래된 Record는 정해진 시간이나, 파일 limit에 도달하면 지워집니다.Kafka의 Topic은 여러 Partition으로 이루어져 있습니다. Replication은 Partition이 여러 Broker(서버)에 걸쳐 복제된 ‘복제본’을 말합니다. 각 Partition은 Leader와 하나 이상의 Follower Replica를 갖고 있어, 장애 시에도 데이터 가용성·내구성 유지합니다. Leader가 모든 Read/Write 요청을 다루고, Follower는 Leader의 컨텐츠를 복제합니다.Topic의 Partition과 ReplicationProducer A producer is a thread safe kafka client API that publishes records to the cluster.- from ibm-cloud-architecture.github.io ‘Producer’는 Kafka에 메세지를 등록(publish)하는 Client입니다. 초기 연결(initial bootstrap connection)후에, 토픽(파티션)과 연결할 Leader Broker에 대한 메타데이터를 얻습니다. 메세지를 파티션에 할당할때, Key가 지정되지 않은경우, Round-robin을 사용합니다. Key가 정해져 있다면, Key의 Hash값으로 partition이 정해집니다. 혹은 이 과정을 Custom할 수 있습니다.Consumer와 Consumer Group Consumer는 특정 Topic의 Partition에서 Record를 Pull방식으로 가져와서 처리하는 컴포넌트입니다. 일반적으로 poll() 루프를 돌며 일정량씩 읽고(fetch), 처리 후 offset을 커밋(commit)합니다. Consumer Group은 동일한 group.id 를 가진, Consumer들의 집합을 ‘Consumer Group’이라고 합니다. 하나의 Topic 파티션을 그룹 내에서 단 1개의 Consumer만 소비하도록 해서 작업을 분산 합니다(Queue semantics). 각 partition마다 최소 1개의 Consumer가 있어야 하며, Consumer가 Group에 하나만 있다면, 모든 partition의 데이터를 다룹니다.Consumer와 Consumer Group의 관계Message 구조Kafka Message Anatomy | geeksforgeeks.org Message의 Key는 nullable입니다.key가 null이라면, Round-robin을 통해 random하게 Partition에 할당됩니다. Compression Type을 통해, Message의 합축을 표현할 수 있습니다. Header는 key/value 형식의 Metadata를 저장합니다. Partition은 전송 대상을 의미합니다.(어떤 partition에 저장될 지) Offset은 Broker가 메세지를 partition에 저장할때 부여되며, Consumer가 읽을 때 확인합니다. Timestamp는 이벤트의 생성시간 혹은 Log에 추가된 시간을 의미합니다.AvroAvro는 메시지를 직렬화(serialize)할 때 쓰는 포맷/스키마 언어입니다.(Protobuf와 유사)// user_event.avsc{ \"type\": \"record\", \"name\": \"UserEvent\", \"namespace\": \"com.example\", \"fields\": [ { \"name\": \"id\", \"type\": \"string\" }, { \"name\": \"email\", \"type\": \"string\" }, { \"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null } ]}데이터의 Schema를 JSON형식으로 입력하여 사용합니다. 이를 기반으로 데이터를 Binary로 변환하여 전송합니다.Schema Registry(Confluent, Apicurio)를 통해, Schema를 등록해서 재사용 하곤 합니다.Schema Registry의 사용 Flow | docs.confluent.ioKafka의 기능과 구현Producer의 Exactly-Once Delivery(= Exactly-Once Semantics, EOS) ‘Exactly-Once Delivery’는 “재시도·네트워크 오류가 있어도 Kafka 로그에 동일 레코드가 단 한 번만 쓰이고, 다운스트림(다른 토픽/컨슈머)에도 중복 없이 한 번만 보이게 하는 것”을 목표로 합니다. 분산 시스템에서 정확히 한 번만 Delivery하는 것이, 가장 해결하기 어려운 문제 중 하나입니다.이를 위해 2가지 기능이 결합되어 있습니다.멱등성(Idempotent)을 제공하는 ProducerProducer에서 메세지의 멱등성(Idempotent)을 위한 기능을 제공합니다.Producer가 ProducerId(PID) 와 시퀀스 번호를 붙여 전송합니다. → 브로커가 중복 쓰기 감지 후 무시하도록 처리합니다.같은 파티션에 대한 중복 기록 방지(“딱 한 번 쓰기”)는 보장할 수 있습니다트랜잭셔널(Transactional) Producer여러 파티션·토픽에 걸친 쓰기와 Consumer 오프셋 커밋까지 하나의 트랜잭션으로 묶어 정확히 한 번 처리를 보장합니다.Broker의 Offset Management“브로커가 오프셋을 관리한다”는 말은 보통 두 가지 Layer를 포함합니다. 메시지 자체의 오프셋(Log Offset)을 브로커가 붙입니다. “Consumer가 어디까지 읽었는지”를 브로커가 저장해 줍니다(Committed Offset)Log Offset메시지 자체의 오프셋(Log Offset)을 브로커가 붙입니다.각 파티션은 append-only 로그 파일이고, 브로커가 새 레코드가 들어올 때마다 0,1,2…처럼 증가하는 번호(Offset)를 붙여 저장합니다.그래서 파티션 안에서만 순서가 보장되고, Consumer는 “나는 offset=123부터 읽을래”처럼 요청할 수 있습니다.Distributed Log와 Offset | oso.shConsumer group과 Committed OffsetConsumer Group과 Offset의 관계“Consumer가 어디까지 읽었는지”를 브로커가 저장해 줍니다. (Committed Offset)Consumer Group은 처리 완료한 위치(offset)를 커밋(commit)합니다.이 커밋 정보는 브로커 내부의 __consumer_offsets라는 내부(compacted) 토픽에 저장됩니다. 키: (group, topic, partition) 값: committed offset + 메타데이터이렇게 해두면 Consumer가 재시작하거나 다른 인스턴스로 넘어가도 “마지막으로 읽은 지점”을 브로커에서 다시 가져올 수 있습니다. 메세지 순서 보장같은 Partition 내에서는 메시지 순서(order)가 보장되지만, Partition 간 순서는 보장되지 않습니다.Consumer Rebalancing ‘Consumer Rebalancing’은 같은 Consumer Group 안에서 어떤 Consumer가 어떤 파티션을 읽을지 다시 배분하는 과정을 말합니다. 새 Consumer가 추가/제거되거나, 파티션 수가 변할 때, “공평하게 나눠 읽도록” 그룹 전체가 잠깐 멈추고 재조정(Rebalancing)합니다.Stop-the-world rebalancing | redpanda.comRebalancing 발생 환경 그룹 구성에 변화가 있을때.새 Consumer 추가되거나, 기존 Consumer가 leave 또는 죽음(heartbeat 끊김) 상태일때. 토픽 파티션 수가 변경될때. Group Coordinator가 교체되거나 장애가 발생했을때.Rebalancing 과정(Eager 방식) JoinGroup 요청모든 Consumer가 Coordinator(그룹을 관리하는 브로커)에게 “나 여기 있어요”라고 알립니다. 리더 선출 &amp; 파티션 할당 계산Coordinator가 그룹 내 리더 Consumer를 하나 선출합니다.리더가 partition.assignment.strategy(Range, RoundRobin, Sticky 등)에 따라 ‘파티션 분배’ 세부 내용을 계산합니다. SyncGroup리더가 계산 결과를 Coordinator에 전달합니다.Coordinator가 모든 Consumer에게 “너는 이 파티션들 담당” 통보합니다. 재시작각 Consumer는 자신에게 배정된 파티션을 다시 poll()하기 시작합니다. Eager방식에서는, 모든 Consumer가 한 번에 파티션을 반납했다가 다시 받기 때문에, ‘Stop-the-world’ 구간이 크다는 단점이 있습니다.Side effects of Kafka rebalancingKafka Rebalancing과정으로 인해, 예기지 않은 side effect가 발생할 수 있습니다. Latency 증가(Increased latency)대량 이벤트를 처리하고 있다면, 잦은 리밸런스로 처리가 지연될 수 있습니다. 처리량 감소(Reduced Throughput)poll()이 멈춰 Consumer Lag 증가할 수 있습니다. 컴퓨팅 리소스 사용량 증가(Increased resource usage) 중복 처리 가능성재시작 전에 커밋 못한 레코드는 재처리될 수 있음 (At-least-once)Cooperative(Incremental) Rebalancing ‘Cooperative(Incremental) Rebalancing’은 Kafka Consumer Group에서 필요한 파티션만 “부분적으로” 재할당해, 기존의 “모든 파티션을 일단 반납(Eager)” 방식이 만들던 Stop-the-world(중단 시간) 문제를 줄이려는 메커니즘입니다. Kafka 2.4+ 클라이언트부터 도입되었습니다.Consumer Group과 Partition 구성 Best Practice 파티션 수 ≥ Consumer 수로 구성해야 합니다Consumer가 파티션보다 많으면 일부는 놀게되는 상황이 만들어집니다. 수동 커밋 + 에러 처리 전략 수립합니다.처리가 끝난 후 커밋(At-least-once, 최소한 1회)하도록 설계하고,재처리를 허용하도록 설계하는게 좋습니다(Idempotency, 멱등성 확보). 리밸런스 상황을 최소화 합니다.긴 처리 작업은 별도 쓰레드/큐로 넘기고 poll 주기를 짧게 유지합니다.최신 클라이언트를 통해, Cooperative rebalancing 사용하여, Balancing자체를 효율화 합니다. Lag를 모니터링합니다Latency가 커지면 스케일 아웃 또는 처리 로직 최적화가 필요합니다. 여러 Consumer Group으로 팬아웃(fanout, 입력 확장)합니다.서로 다른 용도(실시간 분석, ETL, 알림 등)는 각기 다른 Consumer Group(다른 group.id)로 독립 소비하도록 구성합니다. Producer partition strategiesProducer가 Partition에 메세지를 할당하는 Strategy(전략)에 대해서 알아봅니다.키 기반 해싱 (Keyed Partitioning)(default strategy)Hash로 작동하는 Default Strategy.키가 null이 아닐 때의 기본 전략입니다.같은 키는 항상 같은 파티션에 할당됩니다. → 키 단위 순서 보장해서, 로컬 캐시/상태 활용에 유리합니다.Round-Robin Partition StrategyRound-Robin으로 Random하게 분배.이 전략은 메시지 내용에 관계없이 메시지를 파티션에 순환적으로 할당합니다.모든 파티션에 메시지가 균등하게 분배되도록 보장하지만, 관련 메시지가 서로 다른 파티션에 배치될 수 있으므로 순서대로 처리된다는 보장은 없습니다.Sticky Partitioning (키가 null일 때 기본)키가 null이면 “한 파티션을 당분간 고정(sticky)”해서 배치를 크게 묶어 성능을 향상시켰습니다.배치를 flush 하거나 파티션 추가/에러 발생 시 다른 파티션을 새로 선택합니다.Uniform Sticky Partition StrategyUniform Sticky Partition StrategySticky의 장점(큰 배치 유지)을 살리면서 토픽 전체 파티션에 더 균일하게 분배하는 전략입니다.특히 많은 토픽/파티션을 동시에 다룰 때 분포 불균형을 줄여줍니다.Consumer assignment strategies같은 Consumer Group 안에서 파티션을 어떤 규칙으로 나눠 가질지 결정하는 알고리즘입니다.Rebalancing 때, partition.assignment.strategy에 지정된 클래스가 실행돼 Partition→Consumer 매핑을 계산합니다.Range assignor (default)Range assignor | from developer.confluent.io토픽별로 파티션을 정렬 후 컨슈머 수로 나눠 “연속 구간(range)” 배정합니다.즉, 단순하게 파티션을 순서대로 컨슈머에 분배하되, 파티션과 컨슈머가 짝수로 나누어떨어지지 않으면, 앞부분의 컨슈머들에 파티션을 좀 더 가져갑니디.(Round Robin과는 여기서 차이가 있음)Round-robin assignorRound-robin Assignor | from developer.confluent.io모든 토픽의 파티션을 하나의 리스트로 붙여 라운드로빈을 돌립니다.(토픽 구분 없이 모든 파티션을 균등 분배.)사용되는 Consumer 수를 극대화하는 것을 목표로 할때 사용합니다.하지만, Rebalancing시에, 전체 Partition에 대해 재배치가 이루어저야 하는 문제(Eager Rebalance)가 있습니다. 즉, 일부분의 Partition만 조정하지 못하는 문제가 있습니다.Sticky assignor‘Round-robin’과 유사하지만, Rebalancing 할 때에 일부의 Partition만 재배치됩니다.Cooperative Sticky assignor(권장)Sticky기반에, ‘필요 파티션만 천천히 이동’하는 assignor입니다. Eager(전통적)인 assignore는 컨슈머 목록에 변화가 생기면, 모든 Consumer를 반납하고 다시 할당 받습니다.이 과정에서 ‘Stop-the-world’가 발생하여, Lag 급증, 중복 처리 위험이 발생하게 됩니다.Avoid Pause with CooperativeStickyAssignor Step 1Avoid Pause with CooperativeStickyAssignor Step 2두 단계를 거쳐, “필요한 파티션만” 이동시켜 Stop-the-world를 피합니다.Kafka Message의 Forward/Backward Compatibility 필요성System이 성장하면서, Kafka의 Message도 계속 성장합니다.Kafka에서는 Avro를 통해, Schema의 진화(혹은 성장, Schema Evolution)를 가능하게 합니다.여기서는 Avro를 이용하여, Message의 ‘Forward/Backward Compatibility’가 왜 필요한지 알아봅니다.Kafka는 장기 보존 &amp; 리플레이(Reprocess)가 가능합니다.Kafka는 메시지를 며칠~몇 달 이상 보존하고, 언제든 과거 데이터를 다시 읽어 처리합니다.시간이 흐르면서 메시지 구조가 바뀌어도 과거 이벤트를 해석할 수 있어야 합니다.Avro는 작성 시점(writer) 스키마와 읽기 시점(reader) 스키마를 맞추는 규칙으로, 과거 데이터도 안전하게 디코딩할 수 있게 합니다.Producer/Consumer의 독립 배포가 이루어집니다마이크로서비스 환경에서 Producer와 Consumer는 동시에 배포되지 않습니다. Producer가 필드를 추가/변경했는데 만약, Consumer가 구버전이라면..? JSON처럼 암묵적인 약속에만 의존하면 쉽게, 시스템이 깨질 수 있습니다. Avro + Schema Registry는 호환성 모드(BACKWARD/FORWARD/FULL 등)를 통해 “새 스키마 등록” 자체를 통제하고, 깨지는 변경을 차단합니다.다양한 Consumer(Analytics, ETL, Alert 등)에 대응해야 합니다.하나의 토픽을 여러 Consumer Group이 서로 다른 목적/언어/프레임워크로 읽는 환경입니다.모든 Consumer가 정확한 필드 타입, 기본값, null 허용 여부를 알아야만 안정적으로 처리할 수 있습니다.Kafka와 RabbitMQ(전통적인 Message Queue)의 차이Kafka와 RabbitMQ는 그 사용 목적과 디자인된 방향성에 차이가 있습니다.메시지 전송 방식 RabbitMQ: 메시지 큐 방식으로 메시지를 큐에 저장한 후, 소비자가 가져가면 메시지가 삭제됩니다. 메시지의 지속성보다는 즉각적인 전달이 중요할 때 적합합니다. Kafka: 로그 스트림 방식으로, 메시지가 브로커에 쓰여지면 로그처럼 유지됩니다. 소비자는 오프셋을 기반으로 메시지를 읽기 때문에 여러 소비자가 같은 메시지를 반복적으로 읽을 수 있으며, 메시지가 삭제되지 않고 설정된 기간 동안 저장됩니다.데이터 영속성 및 내구성 RabbitMQ: 메시지는 큐에 있고 기본적으로 소비 후 사라지므로 일시적인 데이터 전송에 적합합니다. 영구 큐 설정을 통해 메시지 영속성을 유지할 수 있지만, Kafka와 같은 로그 저장 방식과는 다릅니다. Kafka: 기본적으로 데이터가 로그로 유지되기 때문에 저장 기간을 설정하지 않는 한 삭제되지 않습니다. 따라서 데이터의 내구성이 높고 이벤트의 순차적 흐름을 유지하기 위해 적합합니다.성능 및 처리량 RabbitMQ: 낮은 지연 시간과 빠른 전송 속도를 제공하여, 단일 메시지의 빠른 처리가 필요한 경우 유리합니다. 하지만 대용량의 데이터 스트리밍이나 로그 수집에는 성능이 한계에 도달할 수 있습니다. Kafka: 초당 수백 MB의 데이터를 처리할 수 있는 고성능 스트리밍 시스템으로, 대용량의 실시간 로그 및 이벤트 스트리밍에 적합합니다.메시지 순서와 중복 처리 RabbitMQ: 메시지 순서를 보장하지는 않지만, 필요한 경우 순서가 보장되도록 설정할 수 있습니다. 중복 처리 방지를 지원하며 각 메시지를 고유하게 식별하고 소비자에게 전달합니다. Kafka: 파티션 내에서 메시지 순서를 보장하며, 메시지가 중복될 수 있습니다. 여러 소비자가 같은 메시지를 반복적으로 읽을 수 있으므로 순차적 로그 분석에 적합합니다.RabbitMQ는 빠른 메시지 전달과 작업 큐가 필요한 마이크로서비스 통신에 적합하며, Kafka는 대규모 데이터 스트리밍 및 로그 처리에 강점을 갖고 있습니다.적용 예시EspressoEspresso는 LinkedIn의 분산 NoSQL DB입니다.Kafka는 그 Espresso에서 발생한 변경 사항을 복제·전파하기 위한 내부 커밋 로그/스트림(backbone)으로 쓰입니다.즉, Espresso가 ‘Source of truth’라면, Kafka는 그 변경을 다른 레플리카나 시스템으로 정확하고 순서 있게 전달하는 파이프 역할을 합니다.Yelp에서 MySQL DB를 Kafka를 통해 Replication하는 Flow. Kafka에 이벤트를 전달하기까지만 보여줌. | engineeringblog.yelp.comKubernetes에서 적용하기Kubernetes에서 Kafka를 구성할때는, Operator를 통해 구성합니다.StrimziStrimzi는 Kubernetes(또는 OpenShift) 위에서 Apache Kafka를 쉽게 배포·운영·보안·업그레이드할 수 있도록 해주는 오픈소스 Kafka 오퍼레이터 세트입니다.References Apache Kafka | kafka.apache.org Apache Kafka Kafka Overview | ibm-cloud-architecture.github.io Kafka Overview - IBM Automation - Event-driven Solution - Sharing knowledge LinkedIn and Apache Kafka | linkedin.com LinkedIn and Apache Kafka Kafka Ecosystem at LinkedIn | linkedin.com Kafka Ecosystem at LinkedIn Kafka at LinkedIn: Current and Future | engineering.linkedin.com Kafka at LinkedIn: Current and Future Kafka on Kubernetes: Reloaded for fault tolerance | engineering.grab.com Kafka on Kubernetes: Reloaded for fault tolerance The Fundamentals of Apache Kafka Architecture | developer.confluent.io Apache Kafka Architecture Deep Dive How LinkedIn Customizes Its 7 Trillion Message Kafka Ecosystem | blog.bytebytego.com How LinkedIn Customizes Its 7 Trillion Message Kafka Ecosystem Jay Kreps Hadoop Summit 2011 Building Kafka and LinkedIn’s Data Pipeline | youtube.com Jay Kreps Hadoop Summit 2011 Building Kafka and LinkedIn’s Data Pipeline Building a Real-time Data Pipeline: Apache Kafka at LinkedIn | youtube.com Building a Real-time Data Pipeline: Apache Kafka at LinkedIn How Kafka Producers, Message Keys, Message Format and Serializers Work in Apache Kafka? | geeksforgeeks.org How Kafka Producers, Message Keys, Message Format and Serializers Work in Apache Kafka? - GeeksforGeeks Apache Kafka | d3s.mff.cuni.cz 2.11 Apache Kafka Apache Avro | avro.apache.org Documentation About Schema Registry | docs.confluent.io Schema Registry for Confluent Platform | Confluent Documentation Kafka Partition Strategies | github.com/AutoMQ Kafka Partition: All You Need to Know &amp; Best Practices Kafka partition strategy | redpanda.com Kafka Partition Strategies: Optimize Your Data Streaming Consumer Group Protocol | developer.confluent.io Consumer Group Protocol: Scalability and Fault Tolerance Guide to Consumer Offsets: Manual Control, Challenges, and the Innovations of KIP-1094 | confluent.io Kafka Consumer Offsets Guide—Basic Principles, Insights &amp; Enhancements Kafka Rebalancing: Triggers, Side Effects, and Mitigation Strategies | redpanda.com Kafka Rebalancing: Triggers, Effects, and Mitigation Kafka consumer lag—Measure and reduce | redpanda.com Kafka consumer lag - Measure and reduce Kafka와 RabbitMQ비교 | ibm.com What is Apache Kafka? | IBM Introducing Espresso - LinkedIn’s hot new distributed document store | engineering.linkedin.com Introducing Espresso - LinkedIn’s hot new distributed document store Espresso Database Replication with Kafka | confluent.io Espresso Database Replication with Kafka - Confluent | KR Streaming MySQL tables in real-time to Kafka | engineeringblog.yelp.com Streaming MySQL tables in real-time to Kafka strimzi | strimzi.io Strimzi - Apache Kafka on Kubernetes Scaling Elasticsearch Across Data Centers With Kafka | elastic.co Scaling Elasticsearch Across Data Centers With Kafka" }, { "title": "What is CloudNativePG? | CloudNative", "url": "/posts/What-is-CloudNativePG/", "categories": "DevOps, CloudNative", "tags": "aws, kubernetes, cncf, k8s, db, postgresql", "date": "2025-07-21 16:50:00 +0900", "snippet": "CloudNativePG 소개 CloudNativePG(이하 CNPG)는 PostgreSQL을 Kubernetes 환경에 네이티브하게 배포 및 운영할 수 있도록 해주는 오픈소스 오퍼레이터(Operator)입니다. CNCF(Cloud Native Computing Foundation)의 인큐베이팅 프로젝트로 채택되어 있으며, PostgreSQL를 ...", "content": "CloudNativePG 소개 CloudNativePG(이하 CNPG)는 PostgreSQL을 Kubernetes 환경에 네이티브하게 배포 및 운영할 수 있도록 해주는 오픈소스 오퍼레이터(Operator)입니다. CNCF(Cloud Native Computing Foundation)의 인큐베이팅 프로젝트로 채택되어 있으며, PostgreSQL를 Kubernetes에서 안정적이고 확장 가능하게 운영할 수 있도록 도와줍니다.Kubernetes에서 Operator가 뭔가요? Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components.from kubernetes.io Kubernetes에서의 Operator는 Custom Resource를 구성하기 위한 ‘extensions’입니다. 애플리케이션이나 서비스의 수명 주기를 자동으로 관리하기 위한 컨트롤러 패턴입니다. 주로, Stateful 어플리케이션(DB, Message Queue 등)에 적합합니다.왜 필요한가요?Kubernetes에서 Stateful 어플리케이션을 구성하는것의 어려움Kubernetes에서, Stateful 어플리케이션을 안정적이고 효율적으로 운영하기 위해서는, 많은 설정들이 추가로 필요합니다.데이터 저장, 복제, 장애 조치, 업그레이드 등 운영 지식이 많이 필요한 서비스를 직접 구성하기에는 여러 어려움이 있고, Kubernetes의 기본 기능만으로는 이런 복잡한 요구사항을 자동화하는것에도 어려움이 있습니다.예를 들면, 개별 Pod마다 정해져 있는 PV(Persistent Volume)를 사용하도록 설정하거나, Instance에 대한 일관된 Discovery기능을 제공해야 합니다.이런 어려움을, Operator 패턴(여기서는 CNPG)이 해소시켜 줍니다.고가용성(HA, High Availability) 구성 자동화HA를 제공하기 위해, PostgreSQL을 3개 노드로 구성하고, streaming replication을 설정하고, 장애 시 리더를 자동 승격하는 등의 작업은 수동으로 하려면 매우 복잡합니다. CNPG는 단방향(‘Primary → Replica’)구조만 지원합니다.즉, 양방향(bi-directional replication, Multi master)환경을 지원하지 않습니다.(CNPG의 개발사인 EDB의 엔터프라이즈 솔루션에서는 지원한다고 합니다.)CloudNativePG는 이를 모두 자동으로 구성하고, 장애 발생 시 자동으로 복구할 수 있을정도로, DB를 추상화해줍니다.백업/복구 및 PITR 지원Kubernetes에서 PostgreSQL 백업을 주기적으로 S3 등에 저장하고, 시점 복구(PITR)를 제공하는 것은 쉽지 않습니다.CloudNativePG는 pgBackRest를 통합하여, YAML 설정만으로 백업과 복구를 자동화 구성할 수 있습니다롤링 업그레이드(무중단 업그레이드) 및 버전 관리PostgreSQL의 minor 버전 업그레이드를 다운타임 없이 하려면, 데이터 동기화와 ‘리더 전환’이 필요합니다.CloudNativePG는 버전 변경 시 롤링 업그레이드 자동 수행하여, 버젼관리에 대한 부담을 줄여줍니다.Deployment ArchitectureDeployment Architecture on GKE | from cloud.google.com‘CNPG’를 사용하여 배포하면, 여러 AZ(Availability Zone)에 걸쳐 DB의 Instance를 구축합니다.각각의 Instance에 할당되는 Disk도 AZ에 분산되어 있습니다.CNPG의 Primary구조와 Backup Storage | from learn.microsoft.comCNPG로 구성한 환경에서는, 단방향 Replication을 지원하며, Client(여기서는 App)은 Primary에 접속하게 됩니다.CNPG의 Service구조Primary Service와 Replica Service로 구분하여 사용가능.CNPG로 Postrgesql을 구성하면, Primary와 Replica Service가 만들어지며, 해당 Service를 통해 Client에서 DB에 접속할 수 있습니다.만약, ‘my-db’라는 이름의 클러스터를 구성했다고 할 때, 다음과 같은 Service가 만들어집니다.(CQRS패턴) my-db-rw항상 현재 Primary Pod를 가리킵니다.SELECT, INSERT, UPDATE, DELETE 등 쓰기 요청을 보낼 수 있는 엔드포인트를 제공합니다. my-db-roReplica Pod로 라우팅합니다.읽기 전용 쿼리(예: 분석용)를 분산시켜 부하를 줄여줍니다. my-db-r(Headless Service)my-db-0.my-db-r.default.svc.cluster.local 등의 DNS 이름을 통해 개별 Pod에 접근 할 수 있게해줍니다.CNPG Operator나 내부 복제 로직, 또는 pgbouncer pooler가 사용하기도 합니다.(즉 DB Pod끼리 서로 인식할때 사용합니다.) CloudNativePG 기능 소개 PostgreSQL 클러스터 생성 및 관리YAML 선언만으로 PostgreSQL 클러스터 생성할 수 있습니다.StatefulSet, PVC(Persistent Volume Claim) 등을 자동으로 구성해줍니다. HA(High Availability) 지원기본적으로 3개의 인스턴스를 통해 HA를 구성합니다.리더-팔로워 구조를 자동으로 구성해줍니다.patroni 대신 native streaming replication 기반의 고가용성(HA)을 제공합니다. ‘native streaming replication‘은PostgreSQL이 자체적으로 제공하는 Replication 기능으로, 기본 기능만으로 동작하며 별도의 외부 도구 없이도 실시간으로 데이터를 리더(Primary) → 팔로워(Standby) 노드로 복제할 수 있는 메커니즘입니다. 자동 장애 복구리더 노드 장애 시 자동으로 팔로워 중 하나를 승격하여, ‘Fail over(장애 극복)’를 제공합니다. 백업 및 PITR(Point In Time Recovery)‘pgBackRest’를 내장하여 백업/복구 지원합니다.S3 등 외부 스토리지 연동 가능합니다. 롤링 업그레이드PostgreSQL minor 버전 업그레이드를 다운타임 없이 수행합니다. Kubernetes 네이티브kubectl, CRD, Operator 패턴 등 K8s 생태계에 최적화되어 있습니다.Cluster, Backup, ScheduledBackup, Pooler 등의 CRD를 제공합니다.Patroni와 비교‘Patroni’는 CNPG와 마찬가지로, Postgesql의 HA를 위한 솔루션중 하나입니다. 다만, 철학과 구성 방식, 운영 환경이 다릅니다. 항목 Patroni CloudNativePG 목적의 차이 PostgreSQL 고가용성 구성 Kubernetes 네이티브 PostgreSQL 운영 배포 환경의 차이 VM, Bare Metal, Kubernetes 등 범용 Kubernetes 전용 복제 방식 PostgreSQL native streaming replication PostgreSQL native streaming replication 리더 선출 방식 etcd, Consul, Zookeeper 등 외부 저장소(DCS, Distributed Configuration Store) 필요 자체 리더 선출 로직 내장 (DCS 불필요) 클러스터 구성 수동 YAML 구성 + DCS 세팅 필요 YAML CRD만 작성하면 자동 구성 자동화 수준 설치 및 설정 수동, 자동 failover는 가능 설치부터 백업, 업그레이드까지 전면 자동화 백업 기능 직접 구성 필요 (pgBackRest 연동 등) pgBackRest 내장, ScheduledBackup CRD 제공 롤링 업그레이드 수동 처리. 자동 지원 (클러스터 단위) 읽기 전용 리플리카 가능 가능 (Pooler 연동) 운영 복잡도 중~고 낮음 (Kubernetes 사용 시) References CloudNativePG | cloudnative-pg.org CloudNativePG - PostgreSQL Operator for Kubernetes cloudnative-pg Git Repository | github.com/cloudnative-pg/cloudnative-pg https://github.com/cloudnative-pg/cloudnative-pg Kubernetes Operator pattern | kubernetes.io Operator pattern Deploy PostgreSQL to GKE using CloudNativePG | cloud.google.com Deploy PostgreSQL to GKE using CloudNativePG | Kubernetes Engine | Google Cloud Recommended architectures for PostgreSQL in Kubernetes | cncf.io Recommended architectures for PostgreSQL in Kubernetes Overview of deploying a highly available PostgreSQL database on Azure Kubernetes Service (AKS) | learn.microsoft.com Deploying a PostgreSQL Database on AKS with CloudNativePG - Azure Kubernetes Service EDB and Partner OptimaData Discuss Postgres, Kubernetes and the Power of Cloud Native | enterprisedb.com EDB and Partner OptimaData Discuss Postgres, Kubernetes and the Power of Cloud Native" }, { "title": "Networking Essentials | Core Concepts - System Design Interview", "url": "/posts/networking-essentials/", "categories": "System Design Interview, Core Concepts", "tags": "networking, System Design, interview, alb, tcp, udp, ois, http, https, restapi, Computer Science", "date": "2025-07-17 15:20:00 +0900", "snippet": "시스템 디자인(System Design)에 있어, 네트워킹(Netwoking)은 고려해야하는 필수적인 부분중 하나입니다.이 Post에선, 네트워킹에서도 가장 중요한 부분만 정리하려고 합니다.네트워킹 기초(Networking 101)OSI(Open Systems Interconnection) Model 7 LayersOSI Model 7 Layers‘O...", "content": "시스템 디자인(System Design)에 있어, 네트워킹(Netwoking)은 고려해야하는 필수적인 부분중 하나입니다.이 Post에선, 네트워킹에서도 가장 중요한 부분만 정리하려고 합니다.네트워킹 기초(Networking 101)OSI(Open Systems Interconnection) Model 7 LayersOSI Model 7 Layers‘OSI 7 Layers’는 네트워크의 구조를 추상화(Abstract)하여 계층(Layer)으로 표현하는 모델입니다.OSI의 배경초기 네트워크가 설계될 때부터, 물리적으로 구현된 Architecture는 아닙니다.이미 네트워크가 만들어진 이후, 네트워크를 정리(교육, 표준화, 문서화 목적으로)하기 위해, 네트워크 계층을 나누어 정리하기 시작했습니다.이후, ISO/IEC 7498 표준을 통해 OSI가 공식화 되었습니다. Q: OSI(Open Systems Interconnection)이라는 이름의 의미?A: OSI가 ‘서로 다른 벤더의 시스템들이, 개방된(open) 표준을 통해 상호연결(interconnect)되어야 한다’라는 목적을 담고 있어서, 지어졌다고 합니다.OSI가 만들어지던 시절, 각 벤더들은 자기만의 독자적인 프로토콜을 사용하여, 닫힌(Closed) 네트워크를 구축했다고 합니다.OSI 작동 FlowOSI 모델이 실제로 동작할 때, 데이터가 시스템 간에 오가는 흐름은 크게 송신 측(Send) 과 수신 측(Receive) 으로 나눠 볼 수 있습니다.간략하게 표현한 OSI FlowOSI 작동 Flow | from bytebytego.com각 계층은 자신에게 할당된 책임만 다루고, 상하위 계층은 인터페이스(헤더·트레일러, API 등)로만 통신하기 때문에 유연한 확장성과 호환성을 보장할 수 있습니다. Encapsulation상위 계층의 데이터를 받은 후, 각 계층이 자신만의 헤더(또는 트레일러)를 순서대로 “캡슐”처럼 씌워 물리 매체(Physical Layer)로 보낼 준비를 합니다. De-encapsulation물리 계층으로 들어온 비트를 받아, 프레임 → 패킷 → 세그먼트 → 데이터 순서로 ‘껍질’을 벗기듯 떼어내며 최종 애플리케이션 데이터로 복원합니다. Web Request 예시(example)‘Web Request’ 예시를 통해, ‘Request’가 처리되는 과정을 따라가 봅니다.Client → Server Request Example DNS ResolutionClient는 Domain name을 IP로 변환합니다.(DNS를 통해) TCP HandshakeClient는 TCP Connection을 생성하기 위한 작업(‘three-way handshake’)을 실행합니다. SYN: Client가 보내는 ‘synchronize’ Packet입니다. Connection 생성을 요청하기 위해 보냅니다. SYN-ACK: Server는 SYN-ACK(synchronize-acknowledge) Packet으로 Client에게 응답합니다. ACK: Client는 ACK(acknowledge) Packet을 서버에 보내며, Connection이 생성(establish)됩니다. HTTP RequestTCP Connection이 만들어지면, Client는 이를 통해 HTTP GET요청을 보냅니다.(서버로부터 웹페이지 가져오기) Server processingServer에서 Client의 Request를 처리하고, Web Page를 생성하여 HTTP Response를 수행할 준비를 합니다. HTTP ResponseServer가 만들어진 Response를 Client에게 전달합니다. 이 Response에는 생성한 Web Page도 포함되어 있습니다. TCP Teardown전송(HTTP Response에 대한)이 완료되면, Client와 Server는 TCP Connection을 종료합니다.(four-way handshake) FIN: Client가 FIN(finish) packet을 서버에 보냅니다. ACK: Server가 FIN을 받았다는 의미로 ACK를 보냅니다. FIN: Server또한 FIN Packet을 Client에 보냅니다.(Client와 Server 양쪽 모두 종료시키기 위함) ACK: Client는 Server가 보낸 FIN Packet을 잘 받았다는 의미로 ACK를 보냅니다. Network Layer Protocols(Layer 3)OSI 모델에서 ‘패킷(Packet)’ 단위의 전송을 담당하며, 주로 논리적 주소 지정과 경로 설정(Routing) 기능을 수행합니다. 패킷(Packet)과 IP 데이터그램(IP Datagram)패킷은 OSI의 네트워크 계층에서 사용하는 추상적이 데이터 단위입니다.IP 데이터그램은, IP 프로토콜에서 실제로 전송되는 단위를 말합니다.IP(Internet Protocol) AddressIP Address는 ‘전 지구적 라우팅을 위한 주소 체계’입니다. IP버젼에 따라, IPv4와 IPv6로 나뉩니다. IP의 역할에 따라, Public IP(공인 IP)와 Private IP(사설 IP)로 구분하여 설명합니다.IPv4와 IPv6 항목 IPv4 IPv6 주소 길이 32비트 128비트 주소 공간 약 42억 개 (2³²) 약 3.4×10³⁸ (2¹²⁸) 표기법 점으로 구분된 10진수 (예: 192.0.2.1) 콜론으로 구분된 16진수 (예: 2001:0db8:85a3::8a2e:0370:7334) IPv6의 필요성IPv4의 32비트 공간은 인터넷 기기 폭발적 증가로 고갈되어, NAT(Network Address Translation) 같은 우회책(Private IP를 사용하는)이 필요해졌습니다.IPv6는 128비트 주소 덕분에 거의 무제한에 가까운 주소를 제공하며, 각 기기에 고유 글로벌 주소를 부여할 수 있습니다. IPv4와 IPv6를 모두 사용하는 것을 ‘Dual-stack(듀얼 스택)’이라고 합니다.Public IP와 Private IPIPv4의 주소가 고갈되면서, ‘IPv4’의 체계를 유지하면서 시스템 확장에 대응하기 위해, ‘Private IP(사설 IP)’가 만들어졌습니다.Public Network와 Private Network사이에서, NAT(Network Address Translator)를 통해 IP가 치환되어 전송됩니다.NAT의 역할을 보여주는 이미지. 출처: https://en.wikipedia.org/wiki/Network_address_translationPrivate IP는 RFC 1918(사설 인터넷에 대한 주소 할당 표준)을 통해 별도의 IPv4의 영역으로 구분되어 있습니다.아래와 같은 주소 범위가 Private IP로 사용됩니다.10.0.0.0 - 10.255.255.255 (10/8 prefix)172.16.0.0 - 172.31.255.255 (172.16/12 prefix)192.168.0.0 - 192.168.255.255 (192.168/16 prefix) ‘RFC 1918’에 따라, Public Network에서는 해당 IP대역이 라우팅되지 않도록 필터링 됩니다.IPv4 Header 구조IPv4 Header 구조ICMP(Internet Control Message Protocol)ping www.google.com 으로 대표되는 Protocol입니다.주로 네트워크 통신문제를 진달할 때 사용되며, 데이터가 ‘의도한 대상에게 도달하는지’를 확인하는데 사용됩니다.IPSec(Internet Protocol Security)인터넷 프로토콜(IP)을 통해 전송되는 데이터를 안전하게 보호하기 위한 프로토콜 모음입니다.호스트 간, 게이트웨이(라우터/방화벽)간 터널링, VPN 등에 사용됩니다.IPSec은 보호 수준에 따라서, 2가지 모드를 제공합니다.전송 모드(Transport Mode) 원본 IP 헤더는 그대로 두고, 페이로드(예: TCP/UDP 데이터)만 보호합니다. 호스트 간(end-to-end) 보안에 주로 사용합니다.터널 모드(Tunnel Mode) 원본 IP 패킷 전체를 새로운 IP 헤더 뒤에 캡슐화(encapsulation)합니다. 게이트웨이 간(site-to-site) VPN 터널에 주로 사용합니다. 외부에서는 터널링 장비의 IP 두 개만 보이고, 내부 네트워크 구조는 은닉합니다.Transport Layer(Layer 4)Transport Layer (4계층) 은 호스트 간(혹은 어플리케이션 간)의 종단간(end-to-end) 통신을 책임지며, 주로 세그먼트(segment) 단위로 데이터를 처리합니다.주요 Protocol로는 TCP, UDP, QUIC가 있습니다. 대부분의 시스템 디자인 인터뷰에서는 TCP나 UDP(거의 TCP) 위주로 다룹니다.TCP(Transmission Control Protocol, 전송제어 프로토콜)TCP Connection 생성과 종료과정의 HandshakeHow TCP manages a byte stream. | tcpcc.systemapproach.orgTCP는 연결 상태에 대한 강한 신뢰가 필요할때 사용합니다. TCP는 다음과 같은 특징을 갖고 있습니다. 여기서 ‘신뢰’는 모든 Packet이 ‘결과적으로’ 정확하게 도착하는것을 말합니다. Connection-oriented(연결형 서비스)통신을 시작하기 전에, 송·수신 간에 3-way Handshake로 논리적인 ‘Session(세션)’을 설정합니다. Reliability(신뢰성, 신뢰할 수 있습니다.)전송된 Segment(세그먼트, 조각)에 대해 ACK응답을 받고(받았는지 확인하는 신호), 누락 혹은 손상된 경우에는 재전송합니다. In-order Delivery(순서 보장)수신측에서 순서가 뒤바뀐 Segment를 버퍼링하여 원래 순서대로 어플리케이션에 전달합니다. Flow Control(흐름 제어)수신 측이 처리 가능한 만큼만 보내도록 송신 윈도우 크기(Rwnd, Receiver Window)를 조절합니다. (슬라이딩 윈도우 사용) Congestion Control(혼잡 제어)네트워크 과부하를 방지하도록 송신 속도를 조절합니다. (Slow Start, Congestion Avoidance, Fast Recovery 등 이용) Error Detection(오류 검출)헤더와 페이로드에 대해 Checksum을 계산하여 손상 여부를 검사합니다. 사용 사례파일 전송(FTP/SFTP), 이메일(SMTP/IMAP), 데이터베이스 요청, 결제 트랜잭션, HTTP/HTTPS, SSH 등…TCP Segment Header FormatTCP header format | tcpcc.systemapproach.org Sequence Number: ‘이 세그먼트에 담긴 데이터 바이트 중 첫 번째 바이트가 전체 바이트 스트림에서 몇 번째 바이트인가‘를 나타냅니다. 전체 바이트 스트림중 세그먼트의 위치를 알 수 있게 해줍니다. 예를 들면, 다음과 같은 값이 설정되게 됩니다.첫 세그먼트에 바이트 #1~#500이 담겼다면 Sequence Number = 1두 번째 세그먼트에 바이트 #501~#1000이 담겼다면 Sequence Number = 501 Acknowledgment Number: 다음에 기대하는 바이트 위치(즉, 마지막으로 받은 바이트+1) Flags:각 플래그 값의 의미는 다음과 같습니다. SYN: 연결 요청 ACK: 응답 확인 FIN: 연결 종료 요청 PSH, RST, URG 등 Window Size: 수신자 버퍼 여유 공간(Rwnd, Receiver Window)을 나타냅니다. Checksum: 헤더+데이터 무결성 확인용으로 사용됩니다.TCP Keepalive TCP Keepalive는 TCP 연결이 유휴 상태(idle)가 된 후에도 피어(peer)가 살아 있는지 확인해 주기 위한 운영체제(OS) 수준의 기능입니다. ‘half-open 연결(TCP를 구성하는 Host중 한쪽만 열린 상태)’을 정리하거나 방화벽·NAT 매핑 유지를 목적으로 사용합니다. HTTP Header의 Connection: keep-alive 는 동작하는 Layer가 다르며, HTTP KeepAlive가 반드시 TCP KeepAlive를 기반으로 작동하는것은 아닙니다.TCP Keepalive가 connection이 닫히는걸 방지하는 Flow | from aws.amazon.com Idle 타이머 시작(Start Keepalive Idle Timer)연결이 ESTABLISHED 상태로 일정 시간(예: 2시간) 동안 데이터 교환이 없으면, 커널은 Keepalive Probe를 보내기 위해 타이머를 시작합니다. Probe 전송(Keepalive Probe)상대방에게 “아직 살아 있나?”를 묻는 빈 ACK 세그먼트(페이로드 0) 또는 윈도우 프로브(window=0) 형태로 전송합니다.이때 SEQ 번호는 SND.UNA-1 (즉, 마지막으로 성공적으로 ACK된 바이트 이전)으로 설정해, 상대가 ACK를 답하도록 유도합니다. 응답 대기 및 재전송(Keepalive Retransmit)첫 Probe 후 일정 시간(예: 75초) 동안 응답이 없으면 재전송 합니다.재전송 횟수(예: 9회)를 초과할 때까지 Probe를 반복합니다. 연결 해제(Keepalive Timeout)지정된 횟수만큼 Probe에 답이 없으면 “peer unreachable”로 간주합니다.소켓을 강제 종료하고, 애플리케이션에 오류(ETIMEDOUT)를 알리게 됩니다. Congestion Control TCP Congestion Control(혼잡 제어)는 네트워크 혼잡(congestion) 상황에서 패킷 손실과 지연(latency) 폭발을 방지하기 위해, 송신(Sender) 측이 전송 속도를 동적으로 조절하는 메커니즘입니다.다음과 같은 효과가 있습니다. 버퍼 오버플로우 방지라우터·스위치의 큐(buffer)가 가득 차면 패킷 손실이 급증하게 되는데, 이를 예방합니다. 네트워크 사용에 대한 공정성(Fairness) 제공여러 송신자가 하나의 링크(네트워크상의 경로를 의미)를 공유할 때, 서로 과도하게 대역폭을 가져가지 않도록 조절합니다. 효율성(Efficiency) 증대네트워크 용량(capacity)을 최대한 활용하되, 손실 없이 안정적으로 작동하게 합니다. 주요 Algorithm Flow Slow-Start적은 데이터를 전송하면서, 네트워크의 허용량을 가늠하며, 전송할 데이터의 양을 늘립니다.ACK(수신완료 신호)가 올 때마다 cwnd(Congestion Window)를 2배씩(지수적으로) 증가합니다. Congestion Avoidance패킷 손실이 발생하면, 해당 부분부터는 조금씩(1 MSS, 선형적으로)증가시킵니다.네트워크 용량 한계 근처에서 안정적으로 대역폭을 탐색하기 위한 과정입니다. TCP는 언제 선택하면 되나요? 신뢰성(신뢰할 수 있는 전송)이 필요할 때 순서 보장(in-order delivery)이 필요할 때 흐름 제어(flow control)와 혼잡 제어(congestion control)가 필요할 때 연결 지향(connection-oriented) 통신이 자연스러운 프로토콜UDP(User Datagram Protocol)TCP의 Handshake와 같은 과정이 없는 UDP.비연결형, 비신뢰성 전송 프로토콜입니다. TCP와 달리 최소한의 헤더만 붙여 빠른 전송을 목표로 합니다.UDP는 다음과 같은 특징을 갖고 있습니다. Connectionless(비연결형)세션 설정(handshake) 과정 없이 곧바로 데이터를 전송합니다.각 데이터그램을 독립적으로 취급합니다. Unreliable(신뢰할 수 없는)UDP에서는 데이터 손실이 발생할 수 있습니다.ACK/Re-transmission 없어, → 손실·순서 뒤바뀜 감지·복구 기능을 제공하지 않습니다.애플리케이션이, 필요 시 자체 재전송·순서 보장 로직을 구현해야 합니다. Low Overhead(낮은 오버헤드)헤더 크기 아주 작습니다.(8바이트) → 실시간성이 중요한 서비스에 유리합니다.각종 기능들이 최소화 되어 있어, 지연(latency)과 처리 부하가 적습니다. 애플리케이션 주도 제어흐름 제어나 혼잡 제어 기능 없어, → 개발자가 직접 조절하거나 라이브러리를 사용해야 합니다. 브라우져에서 Native로 지원하지 않아, Web App같은 경우 적합하지 않습니다.UDP HeaderFormat for UDP header사용 사례 실시간 멀티미디어가 필요한 경우: VoIP, 화상회의, 실시간 스트리밍(지연 최소화) DNS: 요청-응답이 작고, 그 과정이 빠르게 이루어져야 합니다. 재시도 로직은 클라이언트에서 처리합니다. Domain Resolve 과정 DHCP, SNMP: 간단한 질의응답 프로토콜 게임 서버: 일부 패킷 손실을 허용하지만, 높은 처리량이 요구될 때.언제 UDP를 사용하면 되나요? 지연(Latency) 최소화가 우선일 때 일부 데이터 손실을 애플리케이션 레벨에서 허용·복구할 수 있을 때 연결 설정·유지에 드는 오버헤드를 줄이고 싶을 때QUIC(Quick UDP Internet Connections)TCP + TLS와 QUIC를 비교UDP위에 구현된 차세대 전송 프로토콜입니다.UDP를 기반으로, TCP처럼 연결 지향적(end-to-end) 스트림을 제공하고, 흐름·혼잡 제어(congestion control), 신뢰성 보장(reliability) 메커니즘을 포함하고 있습니다.이에 따라, OSI의 Transport Layer로 포함시키고 있습니다.주요 특징 TLS와 Handshake과정 통합TLS 1.3 암호화 핸드셰이크를 QUIC 연결 설정 과정과 결합하여, 0-RTT 또는 1-RTT로 빠른 보안 설정을 제공합니다. Multiplexing(멀티플렉싱)하나의 QUIC 연결(connection) 위에 다중 스트림(stream)을 생성해, 개별 스트림 지연이 다른 스트림에 영향을 주지 않음 Connection Migration(연결 이주)클라이언트 IP나 포트가 바뀌어도(예: Wi-Fi→LTE 전환) 동일한 연결 ID를 유지해, 세션 끊김 없이 재개할 수 있습니다. 경량 헤더와 확장성을 갖추고 있습니다.UDP 기반으로 작동하므로 가벼운 헤더를 갖고 있고, 헤더 압축 및 확장 헤더 디자인을 통해 미래 기능 추가에 유연하게 대응합니다. User-Space에서 구현(OS 커널이 아닌, App으로 구현되어 있음)전통적인 TCP/IP 스택처럼 운영체제 커널 내부가 아니라, 애플리케이션 라이브러리나 프로세스 레벨의 코드로 동작합니다.덕분에 플랫폼에 독립적이며, 빠른 배포 및 업데이트가 가능합니다. 언제 QUIC를 사용하면 되나요? 높은 지연 민감도를 갖고 있을때: 초기 페이지 로드나 모바일 환경 전환 시 0-RTT로 빠른 연결할 수 있습니다. 멀티플렉싱이 필요할 때: HTTP/3으로 대량의 작은 리소스를 동시에 전송할 때 헤드-오브-라인 차단 없이 처리 네트워크 이주 환경이 필요할 때: 모바일 클라이언트의 IP 변경에도 세션 유지가 필요할 때TCP와 UDP 비교 구분 TCP UDP 연결 연결형 (3-way handshake) 비연결형 신뢰성 신뢰성 (ACK, 재전송, 순서 보장) 비신뢰성 (손실·순서 뒤바뀜 가능) 흐름·혼잡 제어 있음 (슬라이딩 윈도우, 혼잡 제어) 없음 헤더 오버헤드 최소 20바이트 (+옵션) 8바이트 전송 속도 상대적으로 느림 (오버헤드·제어 기능) 매우 빠르고 지연 낮음 Application Layer(Layer 7)‘Application Layer’는 대부분의 개발자가 가장 많은 시간을 보내는 layer입니다.전송계층(Transport Layer)의 최상단에서, 어플리케이션이 통신하는 방법을 정의합니다. 보통 Application Layer는 ‘User-space(사용자 공간)’에서 처리되는 반면에, 그 아래에 있는 Layer는 OS Kernel에서 처리됩니다.덕분에, Application Layer는 더 유연하고, 수정이 쉽습니다. 하위계층은 변경이 어렵지만, 그 처리능력이 매우 효율적입니다.HTTP(Hypertext Transfer Protocol)Web에서 Data를 교환하는데 사용하는 표준 Protocol입니다.‘Stateless Protocol’로서, 각각의 request가 독립적(서버가 이전 request를 알 필요가 없다)으로 작동합니다.대부분의 System Design에서 유용하게 사용됩니다.이런 부분이, ‘System Design’관점에서는 System의 표면(surface area)를 최소화할 수 있는 장점이 있습니다.HTTP Message 예시Key ConceptsHTTP는 다음과 같은 요소들로 구성됩니다. Request methods: GET, POST, PUT, DELETE, etc. Status codes: 200 OK, 404 Not Found, 500 Server Error, etc. Headers: Request나 Response의 Metadata역할을 합니다. Body: 전송되는 실제 데이터가 위치하는 곳 입니다.HTTP Content-negotiationContent-negotiation 메커니즘Agent-driven negotiation 메커니즘클라이언트가 원하는 표현 방식(미디어 타입·언어·인코딩·캐릭터 셋 등)을 명시하면, 서버가 그중 가장 적합한 표현을 골라 응답하는 메커니즘입니다.협상(negotiation)을 위해, HTTP Header특정 Field를 사용합니다. 협상 카테고리 요청 헤더 예시 값 미디어 타입 Accept text/html, application/json;q=0.8, /;q=0.1 언어 Accept-Language ko-KR, en-US;q=0.7, en;q=0.5 인코딩 Accept-Encoding gzip, deflate, br 캐릭터 셋 Accept-Charset utf-8, iso-8859-1;q=0.5 HTTP Versions다양한 HTTP version과 그 구조.HTTPS(Hypertext Transfer Protocol Secure)HTTP의 Secure Protocol입니다. Client와 Server간 데이터를 암호화된 상태로 주고 받는 프로토콜 입니다.HTTPS는 ‘Handshake’과정을 통해 작동되며, 크게 2가지 방식의 암호화 방식을 각 목적에 따라 다르게 사용합니다. ‘Handshake 과정’: 비대칭키(Asymmetric encryption, 공개키) 암호화 방식을 사용하여, ‘데이터 교환’과정에서 사용할 대칭 Key를 만들어냅니다. ‘데이터 교환 과정’: 대칭키(Symmetric encryption) 암호화 방식을 사용합니다.TLS Handshake![TLS Handshake full flow - from Wikipedia](/assets/img/for-post/Networking%20Essentials/Full_TLS_1.2Handshake.png)_TLS Handshake full flow - from WikipediaTCP Connection을 생성한 후에, TLS Handshake가 추가로 이루어집니다.(HTTP/2 이하에서, HTTP/3에선 QUIC와 결합)HTTP와 HTTPS의 용도 구분 HTTP내부 네트워크, 성능 테스트, 암호화가 필요 없는 간단 API 개발 등으로 사용됩니다. HTTPS사용자 개인정보, 로그인·결제 페이지, API 토큰 전송, SEO 최적화, 브라우저 보안 정책 준수하기 위해 사용됩니다. REST(Representational State Transfer) API REST는 웹의 특성(HTTP, URI, 미디어 타입 등)을 최대한 활용해 리소스 중심의 API를 설계하는 아키텍처 스타일입니다. API를 디자인하는 패러다임(Paradigm)중 하나입니다. 가장 흔하게 사용되는 패러다임입니다. REST의 핵심 원리는, ‘Client가 resource중심으로 작업을 수행하는 경우가 많다’는 것입니다. RESTful API Design에서, 가장 선행되어야 하는것은 ‘리소스(resource)와 그 작동(operation)을 모델링하는 것’입니다. REST를 ‘RESTful HTTP’로서 HTTP기반의 아키텍쳐로 많이 쓰이지만, HTTP가 필수조건은 아닙니다.REST의 제약들은 이론상 HTTP뿐 아니라 SMTP, CoAP, 심지어 메시지 큐 프로토콜 위에도 적용될 수 있습니다.주요 개념과 설계 원칙리소스(Resource) 중심으로 설계합니다.URI를 통해 모든 리소스를 고유하게 식별할 수 있어야 합니다.이때, 데이터 모델 객체(예: User, Order, Product 등)는 ‘명사’로 URI에 표현합니다.GET /users → 전체 사용자 리소스 컬렉션GET /users/123 → ID=123 사용자의 리소스POST /users → 새 사용자 생성PUT /users/123 → ID=123 사용자 전체 교체PATCH /users/123 → ID=123 사용자 일부 수정DELETE /users/123 → ID=123 사용자 삭제HTTP Method(Verb의 의미를 담고 있는)에 따른 ‘행위 표현’을 사용합니다. 메서드 의미 멱등성(Idempotent) GET 리소스 조회 예 (여러 번 호출해도 결과 동일) POST 리소스 생성 혹은 비멱등 작업 수행 아니요 PUT 리소스 전체 갱신(교체, upsert) 예 PATCH 리소스 일부 갱신 아니요 (설계 따라 다름) DELETE 리소스 삭제 예 OPTIONS 지원 가능한 메서드 조회 예 HEAD GET과 동일하지만 응답 본문 없이 헤더만 반환 예 Statelessness(상태를 저장하지 않습니다.)Server는 Client의 상태를 저장하지 않습니다.각 요청(Request)는 독립적으로 처리되어야 하며, 각 요청에 필요한 모든 정보를 담아 보내야 합니다. 이는 서버 확장성과 신뢰성을 높여주는 핵심적인 원칙입니다.각 요청이 독립적이라 서비스 구조가 단순해지고, ‘서버 확장’을 고려할때 제약사항이 많이 줄어듭니다.언제 사용하나요?REST는 인터뷰에서 기본적으로 사용하기에 좋습니다. 잘 알려져 있고, 확장가능한 시스템을 구축할때에 좋은 도구가 됩니다. 하지만, REST에서 데이터 형식으로 사용하는 JSON은 Serialization관련 작업이 매우 비효율적입니다. 이는 대부분의 어플리케이션에서 문제가 없지만(bottleneck현상), 관련 문제가 있다면, 다른 도구(gRPC)를 고려하는게 좋습니다.GraphQL GraphQL은 Facebook에서 개발하여 2015년에 오픈소스로 공개한 API 쿼리 언어이자 런타임으로, 비교적 최신의 패러다임(paradigm)입니다. Client로 하여금, 정확하게 자신이 원하는 데이터를 요청할 수 있게 합니다.기존 RESTful API가 가진 여러 한계를 극복하고, 클라이언트 개발자·서버 개발자 모두의 생산성을 높여 주기 위해 고안된 쿼리 언어이자 런타임입니다.REST의 Under-fetching과 Over-fetching 문제Under-fetchingREST의 under-fetching문제와 GraphQL 비교필요한 데이터가 여러 Endpoint에 흩어져 있어서, 여러 Endpoint를 호출해야하는 상황을 말합니다.예를 들면, /users/123로 받은 응답에는 포스트 목록이 없어서, /users/123/posts를 별도로 호출해야 할 때가 많습니다.GraphQL에서는, 하나의 요청에 여러 Entity에 대한 Query를 날릴 수 있습니다.Over-fetchingover-fetching 상황.불필요한 데이터까지 포함하여, 데이터를 과다 수신하는 문제를 말합니다.만약 Under-fetching문제를 해결하기 위해, End-point를 통합하게 된다면 불필요한 데이터 포함될 가능성이 높아집니다.또다른 예시로,REST API에서 /users/123 같은 엔드포인트가 항상 유저의 모든 속성(이메일, 주소, 프로필 사진 URL, 가입일 등)을 반환할 때, 화면에 이름·아이디만 필요해도 불필요한 데이터를 전부 내려받아야 합니다.GraphQL에서는, Client에서 주도적으로 Response데이터의 Filtering을 요청하여, 정확하게 필요한 데이터만 받아낼 수 있습니다.언제 사용하나요? Frontend팀이 빠르게 iteration할때에 적합합니다.Frontend가 주도적으로 쿼리를 할 수 있기 때문에. 클라이언트마다 서로 다른 데이터 요구사항이 있을 때 하지만, 이런 구조(GraphQL)는 Backend에 Latency와 Complexity를 높이게 됩니다. Cache를 CDN Level에서 적극적으로 사용해야 할때는 적합하지지 않습니다.(RESTful 디자인이 Cache에 더 직관적)gRPC(google Remote Procedure Call) gRPC는 Google이 개발한 오픈소스 원격 프로시저 호출(Remote Procedure Call, RPC) 프레임워크입니다. 특히 마이크로서비스 환경에서 높은 성능과 명확한 인터페이스 계약을 제공합니다. HTTP/2와 Protobuf(Protocol Buffers)를 사용합니다.gRPC를 하나의 이미지로 요약. | from bytebytego.comProtobuf(Protocol Buffers)Protocol Buffers(이하 protobuf)는 JSON과 같이 데이터를 Serializing(직렬화)하는 방법중에 하나입니다.message User { string id = 1; string name = 2;}// 0A 03 31 32 33 12 08 6A 6F 68 6E 20 64 6F 65 // binary encoding된 데이터JSON은 사람이 이해하기 쉽지만, 컴퓨터 입장에서는 연산(Serializing에 관련된)에 비효율이 있습니다.Protobuf는 데이터를 binary로 encoding하여 외부 시스템과 교환합니다.REST와 gRPC 작동방식 비교REST방식과 gRPC방식의 차이 | from https://refine.dev/blog/grpc-vs-rest/#step-3-1gRPC는 REST와 달리 함수를 호출하듯 인터페이스를 설계·사용합니다.HTTP를 기반으로하는 REST와 gRPC비교gRPC도 HTTP/2를 기반으로 작동하는 프로토콜이기 때문에, REST와 비교하며, 내부적으로 어떤 HTTP요청을 보내는지 비교하고자 합니다. REST Over HTTP POST /users HTTP/1.1Content-Type: application/json{ \"name\": \"Alice\"} gRPC POST /myapp.UserService/CreateUser HTTP/2Content-Type: application/grpc (binary payload - protobuf encoded) gRPC의 내장기능. Client-side Load Balancing(CSLB)Client Load Balancing 구조 | from grpc.ioServer에 대한 정보를 미리 Discovery하고 저장하는 Loolaside LB가 있는 CSLB구조.클라이언트가 호출할 서버 인스턴스를 직접 선택해 분산시키는 메커니즘입니다.서버 풀을 중앙에서 관리하는 대신, 클라이언트가 가져온 엔드포인트 목록을 기반으로 round-robin, pick_first 같은 정책을 적용하거나, Envoy xDS·Google grpclb 같은 외부 컨트롤 플레인과 연동할 수 있습니다.언제 사용하나요? gRPC는 Strong type(강타입)특성을 통해, 에러를 사전에 검출하고 Binary protocol을 통해 ‘JSON over HTTP’보다 훨씬 더 나은(어쩔때는 10배 이상)의 성능을 제공합니다. 서버와 클라이언트가 교환하는 데이터가 Binary라, 디버깅이 어렵습니다.때문에, gRPC는 MSA(Micro Service Architecture)에서 내부(internal) 서비스간 통신에서 큰 힘을 발휘합니다.Example of Using gRPC for Internal APIs, and REST and HTTP for ExternalInternal Service간에는 gRPC로 구성하여 성능을 높이고, public-facing APIs(외부로 노출되는 API)는 REST를 사용해 범용성 및 호환성을 높이는 구성으로 사용합니다. System Design Interview에서는 내·외부모두 REST로 구성해도 괜찮습니다. 다만, Interviewer나 과제 자체가 특별한 요구사항이 있다면 고려해봐야 합니다.SSE(Server-Sents Events)보통의 request/response 스타일의 API와는 다르게, Server가 Client에 데이터를 ‘push’하는 프로토콜 입니다.서버가 클라이언트(브라우저)로 단방향(Stream)으로 실시간 이벤트(데이터)를 푸시하는 기술이며,TCP연결을 전제로 하기 때문에, HTTP/1.1 혹은 HTTP/2 위에서 동작합니다.SSE Flow with Gateway작동 방식Client에서 ‘Event Stream’을 생성하기 위한 요청을 보내고, 서버에서는 해당 Stream을 계속 유지하여 Response를 보냅니다. 클라이언트 연결(EventSource 생성)브라우져(혹은 JS환경)에서 EventSource 객체를 만듭니다. const es = new EventSource('/events'); 내부적으로는, 아래의 과정을 거치게 됩니다. GET /events HTTP/1.1 요청 헤더에 Accept: text/event-stream 자동 추가 TCP 3-way 핸드셰이크 → TLS(HTTPS인 경우) → HTTP 요청 서버 응답(스트리밍 시작)서버는 HTTP Response를 끊임없이 유지합니다. HTTP/1.1 200 OKContent-Type: text/event-streamCache-Control: no-cacheConnection: keep-alive 이어서 텍스트 프레임을 차례로 전송합니다. data: 첫 번째 메시지 내용\\n\\ndata: 두 번째 메시지 내용\\nid: 42\\nevent: customEvent\\n\\n data: 한 줄에 메시지 내용을 기록 빈 줄(\\n\\n)이 하나의 이벤트 단위(Record) 를 구분 id: 로 이벤트 식별자 지정 → 재연결 시 Last-Event-ID 헤더로 이어받기 event: 로 커스텀 이벤트 이름 지정(클라이언트 addEventListener(‘customEvent’,…)) retry: 으로 재연결 대기(ms) 권고 가능 클라이언트에서 이벤트 처리 // 기본 Event 처리 방식 es.onmessage = e =&gt; { console.log(\"이벤트 데이터:\", e.data); }; // 혹은 커스텀 이벤트 Lister를 통해 처리 es.addEventListener('customEvent', e =&gt; { console.log(\"customEvent:\", e.data); }); 언제 사용하나요?Client에게 Notification이나 Event를 보내야 할때 사용합니다. 주식·환율 시세 업데이트 라이브 스포츠 스코어보드 로그 스트리밍 및 모니터링 실시간 피드(뉴스·소셜 피드) 실시간 알림(Notifications)WS(WebSockets), WSS(WebSockets Secure) WS는 웹 환경에서 양방향(full-duplex) 실시간 통신을 가능하게 해 주는 프로토콜로, HTTP의 한계를 넘어 클라이언트와 서버가 자유롭게 메시지를 주고받을 수 있도록 설계되었습니다. TCP 기반으로 동작합니다. 연결을 계속 유지(Keep-alive)하여 실시간 이벤트를 전달할 때에 적합합니다.연결 과정WebSocket over TCP Sequence diagram | from researchgate.net Client 요청Client에서 TCP연결을 생성하고, HTTP요청을 통해, HTTP연결을 Websocket으로 전환하는 요청을 보냅니다. GET /chat HTTP/1.1Host: example.comUpgrade: websocketConnection: UpgradeSec-WebSocket-Key: &lt;base64-encoded nonce&gt;Sec-WebSocket-Version: 13 Server 응답HTTP를 WebSocket으로 전환하는 것을 승인합니다. HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: &lt;SHA1&amp;base64 of nonce&gt; 언제 사용하나요?high-frequency, persistent, bi-directional communication이 필요할 때 사용합니다. 채팅 어플리케이션 실시간 협업 툴 온라인 멀티플레이 게임 라이브 대시보드 경매(옥션), 스포츠 베팅 등 실시간 입찰/배팅 System Design Interview에서는, WebSocket을 사용하기 전에 반드시 “왜 필요한지”설명해야 합니다.WebSocket은 강력한 기능을 제공하지만, 이를 지원하고 유지하는 인프라 비용이 많이 들고, 특히 대규모 연결 시 상태 저장(stateful) 연결의 오버헤드로 인해 설계에 상당한 조정이 필요할 수 있습니다.WebRTC(Web Real-Time Communication) WebRTC는 브라우저나 애플리케이션 간에 네이티브 P2P(peer-to-peer) 오디오·비디오·데이터 스트림을 전달하기 위한 오픈 웹 표준입니다. 별도의 플러그인 없이, 자바스크립트 API와 브라우저 내장 기능만으로 실시간 통신 기능을 구현할 수 있습니다. 다른 프로토콜과 다르게 UDP기반입니다.WebRTC의 구조 및 컴포넌트WebRTC 구조대부분의 Client는 inbound connection을 허용하지 않고(보안 문제로), NAT뒤에 있기 때문에, Client끼리 직접 연결하는것은 어려운 과제입니다.WebRTC는 여러 컴포넌트를 통해 이를 극복합니다. Signaling Server (신호 중계 서버)피어 간에 SDP(Session Description Protocol) 와 ICE 후보(candidate) 를 교환하기 위한 임시 채널입니다. 즉, 두 Client사이의 연결을 만드는 초기 협상단계에 사용합니다.단, 미디어·데이터 자체는 여기서 통과하지 않습니다. STUN(Session Traversal Utilities for NAT)“내가 밖에서 어떻게 보이는지”(public IP, NAT 매핑)를 알려줍니다.이를 통해, Peer-to-peer 연결을 가능하게 합니다. TURN(Traversal Using Relays around NAT)피어 간 직접 연결(hole punching) 실패 시, 중계(relay)로 동작합니다. WebRTC Connection 수립 4단계 Client는 Signaling Server에 연결하여 peer에 대한 정보를 얻습니다. Client는 STUN 서버에 접속하여 공용 IP 주소와 포트를 얻습니다. Client는 Signaling Server를 통해 이 정보를 서로 공유합니다. Client는 직접 P2P 연결을 설정하고 데이터 전송을 시작합니다.WebRTC의 기반이 되는 UDP Hole PunchingUDP 홀 펀칭(UDP Hole Punching)은 양쪽이 NAT 뒤에 있어도 서로 직접 UDP 패킷을 주고받을 수 있게 해 주는 대표적인 NAT 트래버설(Traversal) 기법입니다.UDP Multi Hole Punching Q: 필요한 이유?A: 대부분의 홈 라우터나 기업 방화벽은 사설 IP → 인터넷 방향(아웃바운드) 패킷만 허용하고,인터넷 → 사설 IP(인바운드)는 기본적으로 차단합니다.따라서 NAT 뒤에 있는 두 호스트 A, B가 서로 통신하려면, NAT 장비의 “구멍(hole)”을 동시에 뚫어줘야 합니다.언제 사용하나요?WebRTC는 브라우저나 애플리케이션 간에 플러그인 없이 직접 P2P(피어투피어)로 오디오·비디오·데이터를 주고받아야 할 때 사용합니다. 실시간 화상·음성 통화(Voice/Video Chat) 라이브 스트리밍 &amp; 브로드캐스트 파일 전송 &amp; 데이터 교환 스크린 공유 &amp; 원격 제어 실시간 협업 도구 실시간 협업 도구의 결과물을 저장하기 위해, CRDT(Conflict-free replicated data type)을 함께 사용하기도 합니다. 실시간 게임 &amp; IoT 제어Load Balancing서비스가 성장하고 트래픽이 기하급수적으로 늘어나면서, System을 설계할때, ‘Scalability를 고려한 설계’가 중요한 부분중 하나가 되었습니다.여기서는 Scaling의 종류와 동적으로 생성된 서버와 연결하기 위한 ‘Load Balancer’에 대해서 알아봅니다.ScalingScaling에는 2가지 옵션이 있습니다. 더 성능이 좋은 서버(Vertical Scaling) 더 많은 서버(Horizontal Scaling)Vertical vs Horizontal Scaling항상 ‘Horizontal Scaling’만이 답이 아니며, 상황에 따라 적절하게 예측하여 Scaling하는게 중요합니다. 실제 Interview에서는 ‘Horizontal Scaling’에 대한 시나리오가 더 자주 나옵니다.Client-Side Load Balancing(CSLB)Server에 대한 정보를 미리 Discovery하고 저장하는 Loolaside LB가 있는 CSLB구조. CSLB에서 Client는 스스로 어떤 서버에 연결할지 선택하게 됩니다. 보통, 서버 목록을 요청해서 Client에서 들고 있다가, 목록의 서버중 하나에 요청을 보냅니다. Server에 변화가 있는경우를 대비해, 주기적으로 polling하거나 update를 Push 받아야 합니다. CSLB는 Client입장에서 가장 빠른 서버를 선택할 수 있습니다. 중간에 요청 자체를 라우팅해주는 역할이 없다보니, 매우 효율적이고 빠릅니다.CSLB의 Examples Redis ClusterRedis Cluster에서 Key에 따른 Client Load Balancing | from aeraki.netRedis Cluster에서는 Key를 Hash한 결과를 통해, 어떤 shard(여러 Node에 분산되어 있는 데이터)에 데이터가 포함되어 있는지 구분하고, 해당 노드로 요청을 보냅니다. DNSDomain을 Resolve하는 과정에서, ‘DNS Resolver’가 CSLB를 수행합니다.만약 아래와 같이 service.example.com 에 대한 A/AAAA 다중 레코드를 구성한다면, service.example.com. 300 IN A 10.0.0.1 service.example.com. 300 IN A 10.0.0.2 service.example.com. 300 IN A 10.0.0.3 아래와 같이. 요청마다 Record List의 순서가 바뀌어서 응답하게 됩니다. $ dig +short service.example.com 10.0.0.2 10.0.0.3 10.0.0.1 $ dig +short service.example.com 10.0.0.1 10.0.0.2 10.0.0.3 이렇게 IP 목록의 순서가 변하면서, CSLB가 작동하게 됩니다. 언제 사용하나요? 우리가 통제해야하는 Client가 적을때. 많은 수의 Client가 있어도, ‘slow update(e.g. DNS)’를 견딜 수 있을때. 주로 internal service간의 통신에서 사용됩니다.Dedicated Load Balancers Dedicated Load Balancer는 애플리케이션 서버가 아니라 오직 트래픽 분산(로드 밸런싱) 목적으로 배포되는 전용 장치나 서비스입니다. 크게 하드웨어 어플라이언스와 소프트웨어(HAProxy, nginx, Apache)/클라우드 서비스 형태가 있습니다.Layer 4 Load Balancer (이하 NLB, Network Load Balancer)Simple HTTP Request with L4 Load Balancer NLB는 Transport Layer(TCP/UDP, Layer 4)에서 작동하는 Load Balancer입니다. Packet의 컨텐츠에 해당하는 부분을 보지 않고, Transport Layer의 도구(IP Addr, Port Num)으로만 Routing을 수행합니다.특징 Client와 Server사이에서 ‘Persistent TCP Connection(KeepAlive으로 알려진 connection)’을 유지합니다.(WebSocket같은) 빠르고 효율적입니다.(Packet을 최소한으로만 살펴보기 때문에) Application Data를 기준으로 Routing 하지 못합니다. 빠른 성능이 목적일때 사용합니다. TCP를 사용하기 때문에, 이를 사용하여 상위 계층에서 통신할 수 있습니다. NLB를 통해, TCP연결이 생성된다면, 해당 연결을 재사용한다면, balancing이 발생하지 않습니다.즉, Connection을 생성하는 시점에만 Balancing이 발생합니다. 생성된 Connection을 통해, 요청을 보내면 Connection과 연결된 Server에서만 처리합니다.언제 사용하나요? WebSocket Connection을 사용할때. ‘persistent connection’이 필요할때. Layer 7 LB보다 너 높은 성능이 필요할때. Interview에서는 WebSocket을 써야하는 경우가 아니라면, L7 LB가 대부분 더 적합합니다.Layer 7 Load Balancer (이하 ALB, Application Load Balancer)Simple HTTP Request with L7 Load Balancer ALB는 Application Layer에서 작동하는 load balancer입니다. Layer 7의 컨텐츠를 직접 들여다 보고, Routing할 수 있습니다.특징 ‘incoming(들어오는) connection’을 끊고, Backend Server와 새로운 connection을 생성합니다.(Client와의 Connection과 Server의 Connection을 끊어서 작동한다는 뜻)이를 기반으로 ALB를 통해 통합된 TLS설정을 할 수 있습니다. request의 내부 컨텐츠(URL, headers, cookies, etcd)를 기반으로 라우팅할 수 있습니다. NLB보다 더 CPU Intensive합니다. (Packet에 대해 더 많이 들여다 보기 때문에) 더 많은 기능과 유연성(라우팅 방법에 대한)을 제공합니다. cookie데이터를 기반으로 특정유저가 같은 서버에만 연결되도록 할 수 있습니다. HTTP기반의 traffic에 적합합니다.언제 사용하나요?HTTP 기반의 traffic을 다룰때, 대부분의 경우에 적합합니다.(WebSocket은 제외)Health Checks and Fault ToleranceLoad Balancer(이하 LB)가 traffic을 분배할때, 이를 완벽하게 수행하기 위해, Backend Server에 대한 모니터링 책임을 갖고 있습니다.LB는 ‘Health Check‘를 통해, 자동화된 failover를 구현합니다.(High availability 제공)‘Health Check’는 다양한 프로토콜(Protocol)을 사용하여 구성할 수 있으며, 일반적으로는 TCP나 HTTP요청을 사용합니다. TCP보다는 HTTP가 Application수준의 Availability를 측정하기에 적합하기 때문에, HTTP를 더 많이 사용합니다.아래는, AWS ALB Health Check Configuration 예시입니다.Protocol: HTTPPath: /healthzPort: traffic portInterval: 10 secondsTimeout: 5 secondsHealthy Threshold: 3Unhealthy Threshold: 2Matcher: 200–399Load Balancing AlgorithmsLoad Balancing 알고리즘 종류 | from blog.bytebytego.com적용실제로 적용할때는, 다음과 같은 LB들을 사용합니다. Hardware Load Balancers: Physical devices like F5 Networks BIG-IP Software Load Balancers: HAProxy, NGINX, Envoy Cloud Load Balancers: AWS ELB/ALB/NLB, Google Cloud Load Balancing, Azure Load Balancer Interview에서, 만약 LB의 처리량(throughput)이 매우 크다면, Hardware LB를 사용하는걸 언급하는게 좋습니다.Regionalization and Latency글로벌 서비스에서, 서버는 전 세계에 걸쳐 분포되어 있습니다.이때 필연적으로 발생하는 ‘물리적 거리’ 때문에, 많은 Latency가 만들어집니다. 빛은 진공 상태에서 광속의 약 2/3인 약 200,000km/s로 광섬유 케이블을 통해 이동합니다.즉, 뉴욕과 런던(약 5,600km)을 왕복하는 데는 신호 전파의 물리적 특성만으로 이론적으로 약 56ms의 지연 시간이 발생합니다. 이러한 물리적 제약 때문에 저지연 애플리케이션에는 지리적 분포가 필수적입니다.전세계 해저케이블 분포 | from submarinecablemap.com이런 물리적인 한계 때문에, 어느정도의 Latency를 피할 수 없는 부분이 있습니다.하지만, 이를 개선하기 위한, 몇가지 최적화(optimization) 전략을 소개합니다.CDNs(Content Delivery Networks)CDN Description | from cloudns.net전 세계에 분산된 캐시 서버(Edge 서버)를 이용해 사용자에게 가장 가까운 위치에서 정적·동적 콘텐츠를 빠르게 전달하는 서비스입니다.이는 캐싱(Caching) 덕분에 가능합니다.데이터가 많이 변경되지 않거나 자주 업데이트될 필요가 없다면, 엣지 서버에 캐싱하여 반환할 수 있습니다. 이는 이미지, 비디오 및 기타 자산과 같은 정적(static) 콘텐츠에 특히 효과적입니다. Interview에서, CDNs을 자주 보게 될것입니다. 데이터가 Cacheable하고 글로벌 환경에서 빠르게 응답해야 한다면, CDN은 좋은 전략이 될겁니다.Regional Partitioning서비스에 따라, 모든 데이터가 Global단위로 공유될 필요가 없을 수 있습니다. 즉, Regional하게 사용될 수 있습니다.이를 이용하여, 지역(Region, 인접 도시들을 묶은 개념)별로 데이터센터를 분리하여 운영하면, Latency를 줄이면서 데이터 유지비용도 줄일 수 있습니다.Handling Failures and Fault Modes우리는 종종 ‘server crash’, ‘solar flares(태양 표면 폭발의 전자기파로 인해 디지털 장비가 쟁이를 일으키곤 합니다.)’로 인해, 시스템 장애를 겪곤 합니다.여기서는, 우리는 이런 ‘시스템의 실패(Failures)를 어떻게 다뤄야 하는지’를 다룹니다. “네트워크는 안정적이다”라는 생각은, 분산 시스템에서 가장 위험한 가정입니다.네트워크는 언제든 실패할 수 있으며, 이를 염두해두어야 합니다.Timeouts and Retries with Backoff실패를 처리하는 가장 기본적인 방법은, Timeout과 Retry를 사용하는 것입니다.만약, 요청을 처리하는데에 너무 오래 걸린다면, Timeout처리 이후에 Retry로 다시 시도하면, 이 요청은 성공할 수 있습니다.(특히, 분산처리 환경이라면 더욱)이 과정에서 API의 Idempotency(멱등성)이 필요합니다.Idempotency(멱등성)같은 요청을 여러 번 처리해도 “결과가 처음 한번 처리한 것과 동일” 하도록 보장하는 특성을 말합니다.즉, Retry를 수행해도, 처음 요청한것과 동일하다는 것이 보장된다는 얘기입니다.장애가 발생해도, 이 요청을 다시 보냄으로서 처음 요청과 동일한 결과를 얻을 수 있습니다. 읽기 요청(GET)의 IdempotencyGET 요청은 서버 상태를 변경하지 않으므로, 몇 번 호출해도 문제가 없습니다. 이 경우, ‘Idempotency’를 구현하기 쉽습니다. 쓰기 요청(POST 등)에서의 Idempotency 보장‘쓰기 요청’의 Idempotency를 보장하는 것은 더 어렵고 복잡합니다. Idempotency Key를 도입합니다.클라이언트가 요청에 고유 키(예: 사용자ID+날짜)를 함께 전송하여, Server가 요청을 구분할 수 있게 합니다. 이를 통해, 서버가 중복 요청을 인식할 수 있습니다. 서버 처리 로직같은 키의 요청이 이미 처리(혹은 처리 중)된 경우 재실행 없이 결과만 반환합니다. Backoff Strategy‘Exponential’과 ‘Exponential and random incremental(Jitter의 한 종류)’비교.‘Backoff’는 재시도(Retry)시 “실패 후 다음 시도를 언제 할지”를 결정하기 위한 전략입니다.주로 네트워크 요청, 메시지 전송, 분산 잠금(Lock) 획득 등에서 일시적 오류를 처리할 때 사용합니다.‘Backoff’ 전략중에, 대표적으로 ‘Exponential’과 ‘Jitter’가 있습니다. Exponential재시도(retry) 간의 지연 시간을 지수 함수적으로 늘려가는 전략입니다.주로 네트워크 요청, 분산 락 획득, 메시지 큐 소비 등 일시적 실패를 견디기 위해 사용됩니다. 만약 동시 요청수가 많아서 Fail이 발생하고 있다면, Exponential 방식은 여전히 문제가 있을 수 있습니다. Jitter백오프(backoff) 전략에서 여러 클라이언트가 동시에 재시도하면서 발생하는 “동시 재시도 폭주(thundering herd)”를 막기 위해, 지연 시간(delay)에 임의성(randomness)을 섞는 기법입니다. System Design Interview에서, Backoff전략으로 ‘Exponential’만 얘기하곤 하지만, 시니어 인터뷰인 경우, Jitter를 알아두는게 좋습니다.Circuit Breakers여기서는, ‘cascading failures in a system(시스템 장애가 전파되는것)’에 대해서 다루며, 이를 해결하기 위한 ‘Circuit Breakers’를 다룹니다. 이 ‘cascading failures’에 대한 질문은 Interview에서 많이 나오는 좋은 질문중 하나입니다. 가장 치명적인 문제들을 예방하는 방법을 아는 지원자를 찾는 데 도움이 될 뿐만 아니라, 많은 면접관이 원하는 경험을 평가하는 좋은 기준이기도 합니다.면접 준비의 핵심은 하나의 실패가 새로운 실패로 이어질 수 있는 시나리오, 즉 연쇄적인 실패에 익숙해지는 것입니다. 이러한 패턴을 파악하고 이를 완화하는 방법을 아는 것은 면접에서 돋보일 수 있는 좋은 방법입니다.‘cascading failures’사례데이터베이스가 갑자기 다운되어 한 번에 한 인스턴스씩 부팅해야 하는 경우, 수많은 Retry와 사용자들의 과격한 반응으로 인해 인스턴스가 시작조차 되지 않을 수 있습니다(“thundering herd”라고 불리는 현상).첫 번째 인스턴스를 다시 시작할 수 없으니 전체 데이터베이스를 다시 온라인 상태로 만들 수 없습니다.이대로, 교착상태에 빠져버리게 됩니다. ‘Circuit Breakers’는 분산 시스템이나 마이크로서비스 환경에서, 하나의 서비스 실패가 시스템 전체로 전파되는 것을 방지하기 위해 고안된 안정화 기법입니다. 네트워크 호출(원격 API, DB, 캐시 등)에 적용해, 일정 수준 이상의 오류가 감지되면 빠르게 실패 응답을 반환하고, 이후 정상 복구 여부를 검사해 다시 호출을 허용합니다. 네트워크 통신에 직접적인 영향을 미치는, 견고한 시스템 설계를 위한 중요한 패턴입니다.동작 흐름 Closed 상태기본 상태로, 모든 요청을 정상적으로 외부 서비스(또는 리소스)로 전달하는 상태입니다.오류가 연속해서 일정 개수(failureThreshold) 이상 발생하면 → Open 상태로 전환합니다. Open 상태외부 호출을 즉시 차단(Short-circuit) 하고, 사전에 정의된 실패 응답(예: 503, fallback 데이터)을 바로 반환합니다.일정 시간(resetTimeout)이 지나면 → Half-Open 상태로 전환합니다. Half-Open 상태외부 호출을 “시험 삼아” 소수(maxTrialRequests)만큼 허용하는 상태입니다.이 중 성공이 연속 successThreshold 개수만큼 발생하면 → Closed로 복귀합니다.실패 발생 시 다시 Open으로 돌아가고, resetTimeout 카운트 재시작합니다. Circuite Breakers가 주는 이점(advantages) 빠른 실패(fail fast)이미 불안정한 리소스에 대량 요청을 보내지 않아 시스템 부하가 감소합니다. 장애 격리특정 서비스 문제를 빠르게 감지해, caller(상위 서비스)로 에러가 전파되는것을 제어합니다. 회복 탄력성Half-Open 상태로 복귀 검사를 통해, 외부 서비스가 정상화되면 자동 복귀할 수 있습니다. 언제 사용하나요? HTTP 클라이언트외부 API 호출 라이브러리에 회로 차단기가 내장되어 있습니다. (Netflix Hystrix, Resilience4j 등) 데이터베이스 연결DB 커넥션 풀에서 일정 실패율 초과 시 새 연결 시도를 차단합니다. 마이크로서비스 간 RPCgRPC나 REST 호출 시 Circuit Breaker 미들웨어를 삽입하여 사용합니다. Wrapping Up매우 많은 내용을 다루었지만, 아래의 Key Area를 기반으로 ‘System Design Interview’를 준비하는게 좋습니다. Understand the basics: IP addressing, DNS, and the TCP/IP model Know your protocols: TCP vs. UDP, HTTP/HTTPS, WebSockets, and gRPC Master load balancing: Client-side load balancing and dedicated load balancers Plan for practical realities: Regionalization and patterns for handling failures네트워킹 관련 결정은, 지연 시간, 처리량, 안정성, 보안 등 시스템의 모든 측면에 영향을 미친다는 점을 기억해야 합니다.네트워킹 구성 요소와 ‘패턴에 대한 정보’에 기반한 선택을 통해, 기능적(functional)일 뿐만 아니라 견고하고(robust) 확장 가능한(scalable) 시스템을 설계할 수 있습니다.References Networking Essentials | hellointerview.com Hello Interview | System Design in a Hurry OSI 7 Layers | aws.amazon.com OSI 모델이란 무엇인가요?- OSI 7계층 설명 - AWS OSI Model Tutorial | 9tut.com CCNA Training » OSI Model Tutorial What is the OSI Model? | cloudflare.com What is the OSI Model? | Cloudflare OSI Model Explained | bytebytego.com ByteByteGo | OSI Model Explained The Network Layer I | Addressing | utsa.pressbooks.pub 6. The Network Layer I | Addressing – Telecommunications and Networking ICMP | cloudflare.com ICMP란?| 네트워크레이어 프로토콜 | Cloudflare IPsec | cloudflare.com IPsec이란? | VPN 작동 방식 | Cloudflare IPSec이란 무엇인가요? | aws.amazon.com IPSec란 무엇인가요? - IPSec 프로토콜 설명 - AWS Transmission Control Protocol | ibm.com Transmission Control Protocol/Internet Protocol TCP Segment Format | tcpcc.systemapproach.org Chapter 2: Background — TCP Congestion Control: A Systems Approach Version 1.1-dev documentation TCP KeepAlive | tldp.org TCP keepalive overview Implementing long-running TCP Connections within VPC networking | aws.amazon.com Implementing long-running TCP Connections within VPC networking | Amazon Web Services What is UDP? | cloudflare.com What is the User Datagram Protocol (UDP)? | Cloudflare UDP | systemsapproach.org 5.1 Simple Demultiplexor (UDP) — Computer Networks: A Systems Approach Version 6.2-dev documentation HTTP Content negotiation | developer.mozilla.org Content negotiation - HTTP | MDN RFC 2295: Transparent Content Negotiation in HTTP | datatracker.ietf.org RFC 2295: Transparent Content Negotiation in HTTP HTTP Content negotiation | http.dev HTTP Content Negotiation GraphQL | graphql.org GraphQL | A query language for your API RESTful API | aws.amazon.com RESTful API란 무엇인가요? - RESTful API 설명 - AWS Best practices for RESTful web API design | learn.microsoft.com Web API Design Best Practices - Azure Architecture Center gRPC load balancing | grpc.io gRPC Load Balancing Server-sent events(SSE) | learn.microsoft.com Server-sent events and Application Gateway for Containers WebRTC Hompage | webrtc.org WebRTC What is WebRTC and how does it work? | antmedia.io WebRTC Tutorial: What Is WebRTC and How It Works? - Ant Media UDP hole punching | en.wikipedia.org UDP hole punching What is Load balancing? | aws.amazon.com What is Load Balancing? - Load Balancing Algorithm Explained - AWS What’s the difference between application, network, and gateway load balancing? | aws.amazon.com Application, Network, and Gateway Load Balancing - Difference Between Load Balancing Types - AWS Conflict-free replicated data type(CRDT) | en.wikipedia.org Conflict-free replicated data type What is DNS-based load balancing? | cloudflare.com What is DNS-based load balancing? | DNS load balancing | Cloudflare CDN이란 무엇인가요? | aws.amazon.com CDN이란 무엇인가요? - 콘텐츠 전송 네트워크 설명 - AWS Exponential Backoff And Jitter | aws.amazon.com Exponential Backoff And Jitter | Amazon Web Services" }, { "title": "Workloads | Kubernetes Deep Dive - 4", "url": "/posts/k8s-workloads/", "categories": "DevOps, kubernetes", "tags": "aws, kubernetes, cncf, k8s", "date": "2025-07-09 13:50:00 +0900", "snippet": "Kubernetes는 Infra에 대한 추상화를 제공하는 Framework입니다. 이때, 가장 기본이 되는 추상화 단위가 ‘Pod(파드)’입니다.이 Pod를 어떻게 다루느냐(Workload Management)에 따라, 한 단계 더 추상화된, ‘Deployments’, ‘ReplicaSet’, ‘DaemonSet’등의 Workload Object가 ...", "content": "Kubernetes는 Infra에 대한 추상화를 제공하는 Framework입니다. 이때, 가장 기본이 되는 추상화 단위가 ‘Pod(파드)’입니다.이 Pod를 어떻게 다루느냐(Workload Management)에 따라, 한 단계 더 추상화된, ‘Deployments’, ‘ReplicaSet’, ‘DaemonSet’등의 Workload Object가 있습니다.이 Post에서는 Kubernetes에서의 각 Wokrload를 살펴보고, 이해하려고 합니다.Pod(파드) Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.- from kubernetes.io Pod는 Kubernetes의 가장 작은 단위의 배포가능한(deployable) 컴퓨팅 단위입니다. 하나 혹은 여러 Container를 포함하는 Group입니다. Pod안에서, Container들 끼리 Storage와 Network 자원을 공유합니다.Pod 사용 방법 2가지하나의 Container를 돌리는 PodPod당 1개의 container를 포함하여 운영합니다. 이 경우, Kubernetes가 Container를 직접관리하지 못하니, Pod로 Wrapping하여 관리합니다.apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80여러개의 Container를 돌리는 PodContainer간 서로 강하게 엮여있는(tightly coupled) 경우, Pod안에 여러 Container를 포함시켜 운영할 수 있습니다. 만약, 동일한 Container를 여러개 돌리고 싶은 경우라면, 이 방식이 아니라, 뒤에 나올 ‘ReplicaSet’을 고려해야 합니다.Pod안에서의 자원(Resource) 공유 원리Linux technologies that contribute to containers The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container.- from kubernetes.ioPod안의 Container들끼리는 Storage(Volume), Network, 컴퓨팅 자원을 공유합니다.이런 Pod 내부 Container들끼리의 Context공유(Resource 공유)는, Linux의 namespaces와 cgroups(Control Groups)과 다른 격리 기술의 집합(set)으로 이루어져 있습니다. Namespaces로 Pod별로 Process 격리(Isolation)를 구현하고, cgroups로 Container별로(즉, Process별로) 자원(Resource)을 제어합니다.Linux의 namespacesPod와 linux namespace의 관계‘Namespaces’는 Linux kernel 기능중 하나입니다.프로세스별로 ‘가상화된’ 시스템 리소스를 제공하기 위해 도입한 기능입니다. 네임스페이스 덕분에 서로 격리된 프로세스 그룹이 각자 독립적인 환경(파일 시스템, 네트워크, 프로세스 ID 등)을 갖고 동작할 수 있습니다. 여기서, ‘시스템 리소스’는 ‘파일 시스템 마운트 정보’, ‘프로세스 식별자(PID) 공간’, ‘네트워크 스택’, ‘cgroup 계층 구조’등이 해당됩니다.Kubernetes에서, 하나의 Pod는 실제로 “네임스페이스 집합(namespace bundle, process나 network namespace의 집합을 얘기함)” 위에서 돌아갑니다.Pod에 사용하는 namespace의 컨셉을 이해하기 위해, 그 일부를 함께 정리하고자 합니다. Process isolation(PID namespace)OS의 ‘Process’단위의 격리(isolation)를 위한 namespace입니다.Pod는 각각의 PID namespace를 가지며, 같은 Host머신에서도 Process가 격리(isolation)되어 작동합니다. Network interfaces(net namespace)‘net namespace’는 ‘Process’에 새로운 IP(Virtual IP)를 부여하여, 독립적인 Port운영을 가능하게 해줍니다.예를 들면, 메일서버를 운영한다고 했을때, 해당 메일 서버가 25 Port를 요구하기 때문에, Host당 하나만 띄울 수 있습니다(PID 격리가 되어있다 하더라도).‘net namespace’는 IP레벨의 격리를 제공하여, 같은 Host에서도 Network interface를 Pod단위로 분리시켜 줍니다. CgroupsLinux안에서 시스템의 리소스를 조정하는 메커니즘(mechanism) 입니다.밑에서 더 자세히 다룹니다. 등등… Linux의 cgroups(Control Groups)‘cgroups’은 Process단위로 Resource(CPU, memory, disk I/O 등)을 격리(isolate)하고, 계산하고(accounts), 제한(limit)하는 Kernel 기능입니다.cgroups의 기능은 다음과 같습니다. 리소스 제한(Resource limiting)메모리 사용량(파일 시스템 캐시 포함), 디스크 I/O 대역폭, CPU quota, CPU set, 프로세스당 최대 열린 파일 수 등 그룹 전체에 대한 상한선을 설정할 수 있습니다. 우선순위 조절(Prioritization)CPU 스케줄링 비중(cpu.shares)이나 블록 디바이스 I/O 우선순위(blkio.weight)를 조절해, 특정 그룹이 더 많은 리소스를 확보하도록 할 수 있습니다 . 리소스 사용량 측정 및 계산(Accounting)그룹 단위로 CPU 사용량, 메모리 사용량, I/O 활동 등을 계측하여, 과금(billing)·모니터링·로그 수집 등에 활용할 수 있습니다. 프로세스 제어(Control)프로세스 그룹 전체를 freeze/unfreeze(일시 중단/재개)하거나, checkpoint &amp; restore(검사점 생성 후 재시작) 기능을 통해 상태를 보존·복원할 수 있습니다 ‘cgroups v1’에서는 다중 계층을 사용하여, 각각의 cgroup 정책에 따라 별도의 Controller를 사용할 수 있지만, 복잡성이 높아지는 원인이 되었습니다.최신버전인 ‘cgroups v2’에서는 변경되어, 단일 계층에서 Controller(리소스 관리를 위한)를 관리합니다.아래는 Kubenetes에서 Pod를 정의하는 부분에서, cgroup과 관련이 있는 부분을 표시하고 있습니다.apiVersion: v1kind: Podmetadata: name: cgroup-demo-podspec: containers: - name: web-server image: nginx:latest # ─────────────────────────────────────────────────────────────── # 이 부분이 cgroup으로 구현되는, 리소스 격리 설정입니다. resources: # 최소 보장(request)와 최대 제한(limit)을 지정합니다. requests: cpu: \"250m\" # 이 컨테이너에 최소 0.25 CPU 코어를 보장. memory: \"128Mi\" # 최소 128MiB 메모리를 보장. limits: cpu: \"500m\" # 최대 0.5 CPU 코어까지만 사용 가능. memory: \"256Mi\" # 최대 256MiB 메모리까지만 사용 가능. # ─────────────────────────────────────────────────────────────── ports: - containerPort: 80Pod Lifecycle Like individual application containers, Pods are considered to be relatively ephemeral (rather than durable) entities.- from kubernetes.ioPod는 영속적(durable)이라기 보다, 일시적인(ephemeral) entity로 여겨집니다.Kubernetes에서는 Pod의 Lifecycle를 통해, 일시적으로 사용되는 Pod의 상태를 추상화하였습니다. 반복적이고 빠르게, 제거되고 생성되기 때문에, Lifecycle이 필요하다는 뜻Pod와 Fault Recovery(Self-Healing)Pod는 포함하고 있는 Container중 하나가 ‘fail’상태에 빠지면, 해당 Container를 재시작하려고 시도합니다.하지만, Pod을 복구할 수 없는 상태에 빠진 경우, Kubernetes는 더 이상 Pod자체를 복구하려고 하지 않습니다.문제가 있는 Pod을 제거하고, 다른 구성요소(Control plane의 Control Manager)를 통해 Pod을 재생성하여 복구 합니다. A given Pod (as defined by a UID) is never “rescheduled” to a different node; instead, that Pod can be replaced by a new, near-identical Pod.- from kubernetes.io이미 생성된 Pod는 절대로 다른 노드에 재배치 되지 않습니다. 대신에, 새로운 Pod으로 대체됩니다.(Pod의 .metadata.uid 가 바뀝니다. 즉, 새로 생성됩니다.) 이 경우, 새롭게 생성된 Pod이 같은 Node에 배치되는것을 보장하지 않습니다.(Pod를 새로 생성하는거기 때문에, 당연한 얘기일 수도.. filtering을 통해 같은 Node로 배치되게 유도할 수는 있습니다.)Pod phasePod의 status는 여러 정보를 포함한 Object Field로 구성되어 있습니다.여기에는 phase 라는 field가 있는데, 이는 추상화된 lifecycle의 ‘high-level summary’입니다.Pod의 phase는 아래의 List로 제한되어 있으며, 이외의 다른 어떤 값도 존재해선 안됩니다. Pending클러스터가 Pod ‘정의’를 수용했으나, 아직 스케줄링이 완료되지 않았거나 컨테이너 이미지 다운로드·생성 과정이 진행 중인 상태입니다. Running최소 하나의 “주요(main)” 컨테이너가 정상적으로 시작된 상태. 모든 컨테이너가 생성되어 Running 상태가 되면 이 Phase로 진입. Succeeded모든 컨테이너가 정상 종료(Exit 0)하고, 재시작 정책에 따라 재시작되지 않을 때. 주로 일회성 작업(Job/CronJob이 생성한 Pod)에서 볼 수 있습니다. Failed적어도 하나의 컨테이너가 비정상 종료(Non-zero Exit)하거나, 시스템(OOM 등)에 의해 강제 종료된 상태.이 Phase로 들어가면 다시 다른 Phase로 전환되지 않습니다. Unknownkube-apiserver가 해당 Pod의 상태를 확인할 수 없을 때. 네트워크 단절이나 kubelet 오류 등으로 인해 노드와의 통신이 끊긴 경우에 주로 발생합니다. CrashLoopBackOff이 phase라고 혼동하지 마세요Pod가 반복적으로 ‘시작 실패’를 겪을 때, CrashLoopBackOff 값이 kubectl명령어로 나오는 status field에 노출 될 수 있습니다.(아래 예시 참고)$ kubectl get pods --namespace=alessandras-namespace NAMESPACE NAME READY STATUS RESTARTS AGEalessandras-namespace alessandras-pod 0/1 CrashLoopBackOff 200 2d9h마찬가지로, Terminating 값이 Status에 표시될 때도 있습니다.이때, 이 CrashLoopBackoff 와 Terminating 값이 Pod의 Phase값은 아닙니다.kubectl에서 표시되는 Status는 유저 친화적인(직관적인) 값일 뿐이고, 반드시 Pod의 Phase값이 위치하는건 아닙니다.Init Containers ‘Init Containers’는 Pod initialization 과정에서, Main App container가 실행되기전에 실행되어, initialization작업을 하는 Container입니다. 항상 Container 작업이 성공적으로 완료(complete successfully)되어야 합니다. 작업이 실패하게 되면, 성공할때까지 재실행 합니다.(restartPolicy 가 있다면, 해당 정책에 따라 ‘Pod fail’로 다루게 됩니다.)일반 Container와의 차이점 ‘lifecycle’, ‘livenessProbe’, ‘readinessProbe’, or ‘startupProbe’ fields를 지원하지 않습니다. Pod이 ‘Ready’상태에 들어가기전에 모든 작업을 완료하고, 종료되어야 합니다. 만약, 여러개의 init Container를 정의하였다면, 정의한 순서대로 실행되며, 반드시 이전의 container 작업이 성공적으로 완료되어야 합니다. Main App Container와 같은 시스템 Resource를 사용하지만, 상호작용하지 않습니다. Init containers share the same resources (CPU, memory, network) with the main application containers but do not interact directly with them. They can, however, use shared volumes for data exchange.- from kubernetes.io 아래는 Init Container의 예시입니다.apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app.kubernetes.io/name: MyAppspec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]Sidecar Containers ‘Sidecar Containers’는 Pod 안에서, Main Container와 함께 계속 실행되는 Container입니다. Main Application Container가 시작되기 전에, 먼저 시작합니다. App Container의 기능을 확장하거나 강화하기 위해 사용됩니다(App code변경 없이). 예를 들면, 로깅, 모니터링, 보안, 데이터 동기화 같은것이 있습니다.Kubernetes에서의 Sidecar containers ‘init container’의 special case 입니다.(즉, init container의 한 종류입니다.)Kubernetes에서, ‘Sidecar container’는 ‘init container’의 special case 입니다.‘Sidecar container’는 Pod의 부팅(startup)이후에도 계속 실행된 상태로 있습니다. (보통의 ‘init container’는 Pod의 부팅이후에는 종료됩니다.)‘init container’와 다르게, sidecar container가 ‘running’상태라면, 종료되지 않고 다음 container가 실행됩니다. 동일 Pod의 네임스페이스(namespaces)와 볼륨(volumes), 네트워크를 공유합니다. Cluster단위에서 feature를 enable해줘야 합니다. --feature-gates=..,SidecarContainers=true 위와 같이 Sidecar를 사용하도록 설정하면, initContainers.restartPolicy 를 설정할 수 있게 됩니다.(재실행 가능하도록 설정) 종료시엔, Main Container가 완전히 종료되고, Sidecar의 shutdown이 실행됩니다.(Pod에 정의된 순서의 역순으로 실행) 이는 종료시에도, Sidecar가 다른 Container를 보조하는 역할이 빈틈없이 수행되도록 합니다. 아래는 Sidecar 정의 예시입니다.apiVersion: apps/v1kind: Deploymentmetadata: name: myapp labels: app: myappspec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp image: alpine:latest command: ['sh', '-c', 'while true; do echo \"logging\" &gt;&gt; /opt/logs.txt; sleep 1; done'] volumeMounts: - name: data mountPath: /opt initContainers: - name: logshipper image: alpine:latest restartPolicy: Always command: ['sh', '-c', 'tail -F /opt/logs.txt'] volumeMounts: - name: data mountPath: /opt volumes: - name: data emptyDir: {}Pod의 리소스 요청 기준Pod에는 여러 Container가 포함될 수 있습니다. 이때, Container의 리소스(CPU같은) 정의는 어떻게 처리되어 Pod로서 스케쥴링 되는지 알아봅니다.컨테이너들은 Pod 단위로 네임스페이스(namespaces)와 볼륨을 공유하지만, 리소스(CPU·메모리·I/O) 관리는 컨테이너별 cgroup을 통해 개별적으로 격리・제어됩니다. requests 합산스케줄러는 Pod 스펙의 각 컨테이너 resources.requests 값을 모두 더한 만큼의 CPU/메모리 리소스가 노드에 남아 있는지 보고, Pod를 배치합니다.예: 컨테이너 A가 cpu: 200m, memory: 100Mi, 컨테이너 B가 cpu: 100m, memory: 50Mi 라면, 스케줄러는 “이 Pod는 최소 300m CPU와 150Mi 메모리를 필요로 한다”고 판단합니다.만약 Pod으로만 Application을 운영한다면, Pod자체로는 HA(High Availability) 관련 기능을 제공하지 않기때문에, 관리하는데에 많은 노력이 필요합니다.때문에 별도의 ‘Workload Management(Deployment, Daemonset 등)’를 통해 Pod을 관리합니다.Deployment와 ReplicaSetDeploymentKubernetes에서, ‘Deployment’는 선언적(Declarative)으로 애플리케이션의 Pod 복제본Set(ReplicaSet)을 관리하고, 롤링 업데이트·롤백 같은 배포 전략을 자동화해 주는 상위 리소스입니다.아래는 ‘Deployment’의 예시입니다.apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1주요 기능 ReplicaSet 관리Deployment는 내부적으로 ReplicaSet을 생성·관리하여, 지정한 수(spec.replicas)만큼 Pod이 항상 가동되어 있도록 보장합니다. 롤링 업데이트(RollingUpdate)새로운 버전의 컨테이너 이미지로 점진 교체하면서 가용성(Availability)을 유지합니다.기본값은 최대 25% 오버프로비저닝, 최대 25% 미달 허용(maxSurge: 25%, maxUnavailable: 25%). 롤백(Rollback)문제가 생기면 이전 버전의 ReplicaSet으로 자동 혹은 수동 복귀가 가능합니다. 버전 관리(Revision)각 변경은 Revision 번호를 부여받아, kubectl rollout history로 추적할 수 있습니다. # deployment의 history를 확인합니다 $ kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.161 배포전략(Strategy)들.spec.strategy 를 통해, Deployment의 배포 전략을 선택할 수 있습니다. RollingUpdate(기본값)점진적으로 새로운 ReplicaSet의 Pod를 늘리고, 구 버전 Pod를 줄이며 교체합니다.maxSurge·maxUnavailable로 스피드·가용성 균형 조정할 수 있습니다. Recreate먼저 모든 기존 Pod를 삭제한 뒤(Zero-downtime 없이) 새 버전 Pod를 생성하는 방식입니다.Stateless 애플리케이션에서 단순하게 사용할때만 사용합니다. This will only guarantee Pod termination previous to creation for upgrades.‘Recreate’은 새로운 버젼이 생성되기 전에, 반드시 그 이전버전이 제거되는것만 보장합니다. 즉, zero downtime을 보장하지 않습니다. 운영관련 팁(Tip)들 컨테이너 이미지 벡터 태그 대신 SHA digest(my-app@sha256:…)를 쓰면, 동일 버전 재배포 시에도 불필요한 롤아웃을 방지할 수 있습니다. Blue–Green 혹은 Canary 배포: Deployment를 여러 개 만들고, Deployment사이에서 서비스(Service) 라우팅을 전환하거나, ‘Argo Rollouts, Flagger’ 같은 툴을 활용하여 구현합니다.ReplicaSet A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.- from kubernetes.io‘ReplicaSet’은 Pod의 복제본이 어느때든(즉, 항상) 지정한 수 만큼 가동되어 있도록 보장하는 역할을 합니다.ReplicaSet의 작동 방식apiVersion: apps/v1kind: ReplicaSetmetadata: name: frontend labels: app: guestbook tier: frontendspec: # modify replicas according to your case replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5 여러 필드를 통해, 특정 ReplicaSet으로 관리되고 있는 Pod을 구분합니다. spec.selector (Label Selector)ReplicaSet 컨트롤러가 ‘지금 클러스터에 매칭되는 Pod이 몇 개인지’ 세고, 부족하면 생성, 초과면 삭제하기 위해 사용하는 핵심 필터입니다.스케줄러가 ‘이 ReplicaSet의 Pod’를 노드에 스케줄링하거나, Service가 ‘어떤 Pod로 트래픽을 보낼지’ 결정할 때도 이 Selector를 활용합니다.처음부터 .spec.template.metadata.labels(Pod의 Template에 있는 label정보)와 일치하도록 정의해야 하며, 라벨 구조를 바꾸면 스케일링 대상이 달라집니다. metadata.ownerReferences (Owner Reference)‘이 Pod는 이 ReplicaSet의 자식’이라는 관계 정보로, ReplicaSet이 삭제될 때 자동으로 Pod를 정리(garbage collect)하도록 Kubernetes에 알려 줍니다. ReplicaSet이 유지해야 하는 상태를 정의하고, 이 상태를 유지합니다. spec.selector여러 Pod들 사이에서, ReplicaSet에 포함된 Pod을 식별하는데 사용하는 field입니다. 이를 통해 현재 개수와 Desired 개수를 확인하여, 일치하도록 조정합니다. spec.replicas유지되어야할 Pod의 수(a number of replicas)를 표현합니다. spec.templatePod를 생성할때 사용하는 Template입니다. Deployments와 ReplicaSet의 관계 ReplicaSet은 ‘원하는 개수의 Pod가 항상 구동되도록 보장’하는 역할에 집중한 리소스인 반면, Deployment는 ReplicaSet 위에 ‘업데이트 관리’, ‘버전 관리’, ‘롤백’ 같은 추가 기능을 제공하는 상위 추상화입니다. 때문에, ReplicaSet을 직접 쓰는것보다, Deployment를 사용하는것을 추천합니다.StatefulSet A StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.- from kubernetes.io‘StatefulSet’은 쿠버네티스에서 상태(stateful)를 갖는 애플리케이션을 안정적으로 배포·확장·업데이트하기 위해 설계된 Workload 객체입니다. Deployment와 달리 각 Pod에 고정된 네트워크 ID와 영속 스토리지(Persistent Volume)를 보장하며, 생성·삭제·업데이트 시에도 순서(order)와 안정성(stability) 을 제공합니다.사용 사례 데이터베이스 클러스터 **(예: Cassandra, MongoDB, MySQL Replication) 분산 캐시 (예: Redis Sentinel, ZooKeeper) 메시지 브로커 (예: Kafka, RabbitMQ) 상태 저장 애플리케이션 (예: Elasticsearch, Etcd) 반드시 Stateful이 필요한 APP이 아니라면, 되도록 Deployment를 사용하길 추천합니다.Scalable을 고려해야하는 시스템에서, Stateful 시스템으로 App을 만든다면, Pod이 영속적인 개념이 아니기 때문에, Stateful을 위해 많은 작업이 추가로 필요해집니다.StatefulSet과 Headless ServiceStatefulSet과 Headless Service의 관계. mysql CQRS패턴을 예시로 사용. | alibabacloud.comStatefulSet에 ‘Service’가 아닌, 별도의 ‘Headless Service’가 필요한 이유?StatefulSet이 ‘상태 저장(stateful)’ 애플리케이션을 다루기 위해 제공하는 핵심 기능 중 하나가 각 Pod에 고정된 네트워크 ID를 부여하는 것입니다.그런데 쿠버네티스의 기본 ‘Service’는 클러스터 IP와 로드밸런싱(LB)을 전제로 동작하기 때문에, StatefulSet이 원하는 ‘Pod별로 고정된 DNS 이름’을 제공해 주지 않습니다. 정리하면, Kubernetes에서는 보통, Pod가 아닌 ‘Service’를 통해 Pod에 접속합니다. 이 ‘Service’를 이용하면, LB를 통해 Pod에 random하게 접속하게 됩니다. 하지만, DB와 같은 App들은 구분을 위해, Pod에 대한 고정된 DNS주소가 필요할 수 있습니다. StatefulSet은 이를 위해, 고정된 네크워크 ID(대표적으로 IP)를 제공하고, 이를 Headless Service를 통해, Pod에 접속하기 위한 개별 IP를 조회할 수 있게 합니다.(DNS에 개별 Pod에 접속하기 위한 Domain을 등록하는 방식.)Headless Service의 역할 Service에 속한 Pod별로 A 레코드를 생성합니다 일반 Service(clusterIP가 할당됩니다)동일한 Service 이름에 대해 하나의 VIP(가상 IP)만 DNS에 등록되는 방식입니다. 이후, LB와 kube-proxy를 통해 Pod로 연결됩니다. Headless Service(clusterIP가 부여되지 않습니다.)selector에 걸리는 각 Pod의 IP를 개별 A 레코드로 DNS에 등록하여, Service Discovery가 되도록 합니다. Stable Network Identity 보장statefulSet이 생성하는 Pod 이름(mysql-0, mysql-1…)과 Headless Service 이름(headless-mysql-svc)을 조합해mysql-0.headless-mysql-svc, mysql-1.headless-svc 와 같은 영구적인 DNS 이름을 제공합니다.Pod IP가 변경되더라도 DNS 이름은 그대로 유지되어, 애플리케이션은 항상 같은 호스트명으로 자신(또는 다른 노드)을 참조할 수 있습니다. Service Discover(서비스 디스커버리) 지원ZooKeeper, Cassandra 같은 분산 시스템은 클러스터 토폴로지를 구성할 때 피어(peer) 노드의 정확한 주소가 필요합니다.Headless Service를 통해 “내 토폴로지 멤버 리스트”를 DNS 기반으로 조회할 수 있게 해 줍니다. 로드밸런싱이 아니라 직접 연결Headless Service는 프록시나 로드밸런싱 기능을 제공하지 않습니다. DNS 조회 결과로 얻은 Pod IP 리스트를 클라이언트가 직접 사용하게 됩니다.이 방식이 StatefulSet이 요구하는 ‘개별 Pod에 대한 직접 연결’하는 시나리오에 딱 맞습니다.DaemonSet A DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.- from kubernetes.ioDaemonSet은 클러스터의 모든(또는 지정한) 노드에 ‘하나씩’ Pod를 배포·유지하도록 보장하는 컨트롤러입니다. 주로 노드별 에이전트(로그 수집기, 모니터링 에이전트, 네트워크 플러그인 등)를 배포할 때 사용합니다.노드가 추가되면, 해당 노드에 대한 Pod이 추가로 생성됩니다.사용 사례 로그 수집 &amp; 모니터링 에이전트: Fluentd, Filebeat, Telegraf, Datadog Agent 네트워크 플러그인: Calico, Cilium, Weave Net 스토리지 드라이버: CSI 플러그인 데몬 보안 에이전트: Falco, Istio 데몬 서비스DaemonSet Spec 예시apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-loggingspec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # these tolerations are to have the daemonset runnable on control plane nodes # remove them if your control plane nodes should not run pods - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log # it may be desirable to set a high priority class to ensure that a DaemonSet Pod # preempts running Pods # priorityClassName: important terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/logJobs Jobs represent one-off tasks that run to completion and then stop.- from kubernetes.io‘Jobs’는 1회성 작업(실행을 완료하고 멈추는)을 안전하게 실행하기 위해 사용합니다.‘Job’은 작업의 완료를 위해, Pod를 1개이상 생성할 수 있습니다. 이때, ‘작업의 완료’기준은, 성공적으로 종료된(successfully terminate) Pod의 갯수입니다.이를 정리하면, ‘Jobs’는 지정한 개수의 완료된 Pod을 보장해주는 컨트롤러 입니다.반복적이거나 장기 실행 서비스인 ‘Deployment’와 달리, 특정 작업이 한 번만 또는 정해진 횟수만큼 실행되어야 할 때 사용합니다.Jobs Spec 예시apiVersion: batch/v1kind: Jobmetadata: name: example-jobspec: completions: 3 # 총 3개 Pod가 성공 종료되어야 Job이 완료됩니다. parallelism: 2 # 동시에 최대 2개의 Pod을 실행합니다. backoffLimit: 4 # 실패 시, 최대 4회 재시도합니다. template: spec: restartPolicy: OnFailure containers: - name: task image: busybox args: [\"sh\", \"-c\", \"echo Hello; exit 0\"]응용하기 Indexed Jobs (.spec.completionMode: “Indexed”)각 Pod에 JOB_COMPLETION_INDEX 환경 변수를 주어, 인덱스별 작업(partition으로 )을 분리하여 처리할 수 있습니다. Work Queue 패턴메시지 큐(RabbitMQ, Kafka)와 연동해, parallelism 을 높여 대량 데이터를 분산 처리합니다. TTL(Time-to-Live) ControllerJob 완료 후 일정 시간(.spec.ttlSecondsAfterFinished)이 지나면 자동 삭제되게 할 수 있습니다. 사용 사례 데이터 마이그레이션: 데이터를 일괄로 로드하거나 변환할때 사용합니다. 백업/정리 스크립트: 주기적인 백업작업이나 로그 아카이빙 작업에 사용합니다. 머신러닝 학습: 단일 배치(1회 실행) 훈련 작업용으로 사용합니다. 크론작업 대체: CronJob과 조합해 주기적 Batch 실행할 때 사용합니다.References Pods | kubernetes.io Pods Workload Management | kubernetes.io Workload Management Deployments | kubernetes.io Deployments ReplicaSet | kubernetes.io ReplicaSet StatefulSet | kubernetes.io StatefulSets Headless Service | kubernetes.io Service DaemonSet | kubernetes.io DaemonSet Linux namespaces | en.wikipedia.org Linux namespaces The 7 most used Linux namespaces | redhat.com The 7 most used Linux namespaces Linux cgroups(Control Groups) | docs.redhat.com Chapter1.Introduction to Control Groups (Cgroups) | Resource Management Guide | Red Hat Enterprise Linux | 6 | Red Hat Documentation A Linux sysadmin’s introduction to cgroups | redhat.com A Linux sysadmin’s introduction to cgroups 4 Linux technologies fundamental to containers | opensource.com 4 Linux technologies fundamental to containers Evolution of Docker from Linux Containers | baeldung.com Evolution of Docker from Linux Containers | Baeldung on Linux Building a Linux container by hand using namespaces | redhat.com Building a Linux container by hand using namespaces Indexed Jobs | kubernetes.io Introducing Indexed Jobs Work Queue pattern with Jobs | kubernetes.io Coarse Parallel Processing Using a Work Queue Fine Parallel Processing Using a Work Queue" }, { "title": "HTTPS 이해하기", "url": "/posts/Understanding-https-certificates/", "categories": "Programming, networking", "tags": "networking, https, http, web, security, protocol", "date": "2025-07-07 18:47:00 +0900", "snippet": "Production 레벨에서 사용되는 HTTP통신. 이때 보안의 기본이 되고 있는 HTTPS(Hyper Text Transfer Protocol Secure)에 대해서 알아보고자 합니다.HTTP와 HTTPS의 차이 HTTP는 Client ↔ Server간 암호화 없이 평문(Plain Text)로 데이터를 주고 받습니다.(암호화 되어 있지 않다는 뜻...", "content": "Production 레벨에서 사용되는 HTTP통신. 이때 보안의 기본이 되고 있는 HTTPS(Hyper Text Transfer Protocol Secure)에 대해서 알아보고자 합니다.HTTP와 HTTPS의 차이 HTTP는 Client ↔ Server간 암호화 없이 평문(Plain Text)로 데이터를 주고 받습니다.(암호화 되어 있지 않다는 뜻) 때문에, 패킷을 감청하여 조합하면, 그 데이터를 쉽게 읽을 수 있습니다. HTTPS는 HTTP의 스펙을 지키면서, SSL(Secure Socket Layer)와 TLS(Transport Layer Security)를 통해, 데이터를 암호화 합니다.HTTPS(over TLS) 작동 원리HTTPS는 ‘Handshake’과정을 통해 작동되며, 크게 2가지 방식의 암호화 방식을 각 목적에 따라 다르게 사용합니다. ‘Handshake 과정’: 비대칭키(Asymmetric encryption, 공개키) 암호화 방식을 사용하여, ‘데이터 교환’과정에서 사용할 대칭 Key를 만들어냅니다. ‘데이터 교환 과정’: 대칭키(Symmetric encryption) 암호화 방식 여기서는 범용적으로 사용되는 TLS기반으로 알아봅니다. 세부사항에는 차이가 있지만, Flow는 TLS와 SSL 모두 비슷합니다.TLS Handshake TLS Handshake과정중에는 사용할 TLS버전(TLS 1.0, 1.2, 1.3 등)을 지정합니다. 통신에 사용할 암호(cipher)를 선택합니다. 서버의 Public key와 CA(Certificate Authority)정보를 기반으로 서버를 신뢰 할 수 있는지 검증합니다. 데이터 전송때 사용할 대칭키(symmetric-key) 암호화에 사용하기 위해, ‘세션키(session key)’를 생성합니다.TLS Handshake full flow - from WikipediaTCP 연결이 생성된 후(TCP Handshake후)에 TLS Handshake가 이루어집니다. Client Hello지원 가능한 TLS 버전, 암호 스위트(cipher suite) 목록, 랜덤값(Client Random)을 전송합니다. Server HelloClient의 spec에 따라, 선택된 TLS 버전, 암호 스위트, 서버 랜덤값(Server Random)을 전송합니다. 서버 인증 &amp; 키 교환서버가 인증서(X.509)를 보내어 자신을 증명합니다. ‘X.509’는 공개키(Public key)를 포함한 ‘인증서 구조’에 대한 표준을 말합니다. (TLS 1.2 이하) 서버 키 교환 메시지로 공개키 파라미터 전달. 클라이언트가 서버의 인증서(X.509) 검증클라이언트가 인증서 체인을 검증하고 인증 기관(CA)의 신뢰를 확인합니다.이 과정은, 서버가 ‘누구’이고, 도메인의 실제 소유자인지 확인합니다. 클라이언트에서 프리마스터 시크릿(premaster secret) 생성 및 전송클라이언트 측에서 ‘premaster secret’이라는 Random string을 하나 더 보냅니다.이 ‘premaster secret’은 서버의 Public key로 암호화(encrypt) 되어있어, 서버에서만 복호화(decrypt)가 가능합니다. 서버의 Public key는 ‘X.509’안에 포함되어 있는걸 사용합니다. 서버에서 프리마스터 시크릿 복호화(decrypt) 세션 키 생성클라이언트, 서버 양쪽의 랜덤값과 프리마스터 시크릿(pre-master secret)을 기반으로 세션 키를 결정합니다. 이때, 이 세션키(Session Key)는 Client와 Server가 각각 별도로 생성합니다. 하지만, 동일한 Session key를 얻게 됩니다(seed와 대상과 알고리즘이 동일하니).즉, 생성된 Session Key를 서로 공유하지 않습니다. Finished 메시지 교환 클라이언트 → 서버암호화된 ‘Finished’ 레코드를 전송합니다.(내부에 verify_data_client 포함) 서버 수신 및 검증수신한 레코드(‘Finished’)를 복호화합니다.자신이 계산한 verify_data_client와 일치하는지 비교합니다.일치하지 않으면 핸드셰이크 실패로 처리하고, 세션을 종료합니다. 서버 → 클라이언트서버도 마찬가지로, 암호화된 ‘Finished’ 레코드를 보냅니다.(내부에 verify_data_server 포함) 클라이언트 수신 및 검증클라이언트에서, 서버의 verify_data_server를 복호화하고 검증합니다.성공하면 핸드셰이크 전 과정이 안전히 완료됨을 상호 확인합니다. 암호화된 데이터 전송이후 모든 HTTP 메시지는 세션 키를 이용한 대칭키 알고리즘(Symmetric-key algorithm)으로 암호화 하여 보호합니다.데이터 교환TLS 1.2 이전‘TLS Handshake’과정에서 도출된 ‘Session Key’를 ‘Master Secret’으로 부르고, 이 ‘Master Secret’을 기반으로, 암호화 Key 와 MAC Key(무결성 검증용)을 생성합니다. 암호화와 무결성 검증을 별도의 알고리즘으로 수행합니다.TLS 1.3TLS 1.3부터, 데이터 교환시에는 AEAD(Authenticated Encryption with Associated Data)모드만을 사용합니다.때문에, ‘AEAD’라는 하나의 알고리즘으로 암호화와 무결성 검증을 모두 수행하며, ‘TLS Handshake’를 통해 만들어진 Session Key(정확하게는 Session key를 기반으로 만들어진 Secret)가 ‘AEAD’에 주입됩니다.데이터 교환시에는 왜 대칭키 암호화(Symmetric-key algorithm for cryptography) 방식을 사용할까? 성능(Performance)의 이점 대칭키 암호화는 비대칭키(RSA, ECDHE 등)보다 수백 배 빠르고, 하드웨어 가속(AES-NI) 지원도 풍부합니다. 비대칭키(RSA, ECC)는 복잡한 연산(‘Modular Exponentiation’과 같은)으로 이루어져 있고, 키의 길이가 길어야 안정성을 보장하는 부분때문에, 상대적으로 성능이 느립니다. 확장성(Throughput) 이점 HTTP/2, HTTP/3 같이 대량의 바이트를 빠르게 처리해야 할 때, 대칭키는 CPU·메모리 비용이 낮아 효율적입니다. 리소스(Resource) 제약에서의 이점 모바일,IoT 기기처럼 계산 능력이 제한적인 환경에서도 충분히 빠르게 암호·복호화가 가능합니다. 보안 관점(Security)에서의 이점 AEAD 모드(AES-GCM, ChaCha20-Poly1305)는 암호화와 무결성 검증을 한 번에 제공해, 암호화·무결성 결합 보안(authenticated encryption)을 구현합니다.X.509공개키 기반구조(PKI, Public Key Infrastructure)의 ‘표준 인증서 형식’으로, TLS(및 SSL)에서 서버(또는 클라이언트) 인증을 위해 사용됩니다.X.509의 역할 신원 증명 역할인증서에 담긴 도메인 이름(Subject)과 발급자(Issuer) 정보를 통해 서버 신원을 검증 공개키 배포 수단인증서 내부의 ‘Subject Public Key Info’ 필드로 서버의 공개키를 안전하게 전달합니다. 무결성 확보발급자(CA, Certification Authority)의 디지털 서명으로 인증서 위·변조 가능성을 방지합니다. X.509 예시 및 주요 필드Certificate: Data: Version: 3 (0x2) Serial Number: 10:e6:fc:62:b7:41:8a:d5:00:5e:45:b6 Signature Algorithm: sha256WithRSAEncryption Issuer: C=BE, O=GlobalSign nv-sa, CN=GlobalSign Organization Validation CA - SHA256 - G2 Validity Not Before: Nov 21 08:00:00 2016 GMT Not After : Nov 22 07:59:59 2017 GMT Subject: C=US, ST=California, L=San Francisco, O=Wikimedia Foundation, Inc., CN=*.wikipedia.org Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 00:c9:22:69:31:8a:d6:6c:ea:da:c3:7f:2c:ac:a5: af:c0:02:ea:81:cb:65:b9:fd:0c:6d:46:5b:c9:1e: 9d:3b:ef ASN1 OID: prime256v1 NIST CURVE: P-256 X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Agreement Authority Information Access: CA Issuers - URI:http://secure.globalsign.com/cacert/gsorganizationvalsha2g2r1.crt OCSP - URI:http://ocsp2.globalsign.com/gsorganizationvalsha2g2 X509v3 Certificate Policies: Policy: 1.3.6.1.4.1.4146.1.20 CPS: https://www.globalsign.com/repository/ Policy: 2.23.140.1.2.2 X509v3 Basic Constraints: CA:FALSE X509v3 CRL Distribution Points: Full Name: URI:http://crl.globalsign.com/gs/gsorganizationvalsha2g2.crl X509v3 Subject Alternative Name: DNS:*.wikipedia.org, DNS:*.m.mediawiki.org, DNS:*.m.wikibooks.org, DNS:*.m.wikidata.org, DNS:*.m.wikimedia.org, DNS:*.m.wikimediafoundation.org, DNS:*.m.wikinews.org, DNS:*.m.wikipedia.org, DNS:*.m.wikiquote.org, DNS:*.m.wikisource.org, DNS:*.m.wikiversity.org, DNS:*.m.wikivoyage.org, DNS:*.m.wiktionary.org, DNS:*.mediawiki.org, DNS:*.planet.wikimedia.org, DNS:*.wikibooks.org, DNS:*.wikidata.org, DNS:*.wikimedia.org, DNS:*.wikimediafoundation.org, DNS:*.wikinews.org, DNS:*.wikiquote.org, DNS:*.wikisource.org, DNS:*.wikiversity.org, DNS:*.wikivoyage.org, DNS:*.wiktionary.org, DNS:*.wmfusercontent.org, DNS:*.zero.wikipedia.org, DNS:mediawiki.org, DNS:w.wiki, DNS:wikibooks.org, DNS:wikidata.org, DNS:wikimedia.org, DNS:wikimediafoundation.org, DNS:wikinews.org, DNS:wikiquote.org, DNS:wikisource.org, DNS:wikiversity.org, DNS:wikivoyage.org, DNS:wiktionary.org, DNS:wmfusercontent.org, DNS:wikipedia.org X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Key Identifier: 28:2A:26:2A:57:8B:3B:CE:B4:D6:AB:54:EF:D7:38:21:2C:49:5C:36 X509v3 Authority Key Identifier: keyid:96:DE:61:F1:BD:1C:16:29:53:1C:C0:CC:7D:3B:83:00:40:E6:1A:7C Signature Algorithm: sha256WithRSAEncryption 8b:c3:ed:d1:9d:39:6f:af:40:72:bd:1e:18:5e:30:54:23:35: ... 버전(Version)X.509v1, v2, v3 중 대개 ‘v3’를 사용 일련번호(Serial Number)CA(Certification Authority)가 고유하게 부여하는 숫자 서명 알고리즘(Signature Algorithm)인증서 자체에 대한 서명에 사용된 해시·암호 알고리즘 (예: sha256WithRSAEncryption) 발급자(Issuer)인증서를 발급한 CA의 DN(Distinguished Name) 유효기간(Validity)Not Before, Not After로 표현되는 시작일·종료일 주체(Subject)인증 대상(일반적으로 서버)의 DN, 도메인 이름(CN 또는 SAN)에 담김 주체 공개키 정보(Subject Public Key Info)공개키 알고리즘·키 값 (예: RSA 2048-bit 공개키) 확장 필드(Extensions) (X.509v3)Subject Alternative Name (SAN): 도메인, IP 주소 등Key Usage: 인증서 사용 목적(예: digitalSignature, keyEncipherment)Extended Key Usage: TLS 서버 인증, 클라이언트 인증 등Basic Constraints: CA 인증서 여부 및 경로 길이 제약기타 CRL 분산 포인트, Authority Key Identifier 등 CA 서명(Signature)발급자 CA가 위 필드 전체에 대해 생성한 디지털 서명 TLS의 데이터 암호화에 사용되는 Session Key A session key is any symmetric cryptographic key used to encrypt one communication session only. In other words, it’s a temporary key that is only used once, during one stretch of time, for encrypting and decrypting datasent between two parties; future conversations between the two would be encrypted with different session keys.- From Cloudflare‘세션키(Session Key)’는 한번의 커뮤니케이션에 사용되는, 암호화(encrypt)에 사용되는 Key입니다.세션키는 임시키이며, 특정 시간동안 2개의 대화 구성원 사이에서 암호화(encrypt)하고 복호화(decrypt)합니다.미래의 대화는 새롭게 생성된, 또 다른 세션키로 암호화, 복호화 됩니다.각 순간에 임시로 생성되어 사용되는 Session key - from Cloudflare데이터 Example평문 HTTPGET /index.html HTTP/1.1Host: example.comUser-Agent: curl/7.68.0Accept: */*(빈 줄)TLS 1.2로 암호화 했을때16 03 03 00 5a ← TLS 레코드 헤더 (type=Application Data, TLS1.2, length=0x005a=90바이트)8b 4e a3 5f 2d ... ← 실제 암호화된 페이로드 (총 90바이트)... ← (중략)00 1d 9c b4 7e ← MAC + 패딩 포함TLS 1.2와 1.3의 주요 변경점표준 문서에 따르면, 다음과 같은 주요 변경점이 있습니다. 암호 스위트(Cipher Suite)를 AEAD’만’ 허용하여 단순화 핸드셰이크 지연 최소화 (1-RTT + 0-RTT)TLS 1.2는 완전한 핸드셰이크에 2회 왕복(RTT)이 필요했지만,TLS 1.3은 1-RTT만으로 완료되며, 이전 세션 재개 시 일부 데이터는 0-RTT로 즉시 전송할 수 있게 지원해 지연을 크게 줄였습니다. Static RSA/DH(Diffie-Hellman) 제거하고, Forward Secrecy(FS, 세션키가 나중에 유출되도 과거의 기록을 안전하게 보호하는 속성을 말합니다.)를 기본으로 지원합니다. Q: Static RSA/DH(Diffie-Hellman)을 왜 제거 했나요?A: ‘Forward Secrecy’를 지원하기 위해, ‘임시(Ephemeral) 키 교환방식’만 지원하기 위해 제거되었습니다. SSL과 TLSSSL과 TLS모두 비대칭 키와 대칭키 방식을 이용하여, 암호화된 데이터 교환방식을 제공합니다.‘SSL’은 처음 Netscape가 만든 사설 규격이었습니다. 이후, SSL기능을 재설계하고, 표준화하면서 IETF(Internet Engineering Task Force)에 의해 ‘TLS’로 명명되었습니다.(비영리 표준화 과정을 거치며 투명한 검증과 호환성 보장)SSL(Secure Socket Layer)의 결함SSL에는 다음과 같은 결함이 있었습니다. SSLv2의 구조적 결함암호 스위트(Cipher suite) 협상 과정의 취약점으로 중간자 공격(MITM)에 취약합니다. Cipher Suite는 TLS(또는 SSL) 연결에서 사용할 암호화 알고리즘들의 조합(combination)을 의미합니다. 복수의 암호 모드가 혼재되어 프로토콜 복잡도와 취약성이 높습니다. SSLv3의 한계MAC 계산에 MD5를 사용 → 충돌 공격에 노출되기 쉽습니다.(동일한 Hash값을 갖는 다른 데이터를 쉽게 생성 가능) MD5는 Hash알고리즘으로, 출력데이터 크기 자체가 작아(128비트 출력), Hash 충돌이 일어날 가능성이 높습니다.(+ 여러가지 다른 이유들 포함) 핸드셰이크 메시지 무결성 검증 불완전나중에 발견된 POODLE 공격으로 더 위험해짐 이런 여러 이유 때문에, SSL은 deprecated(더 이상 사용되지 않음)되었습니다.(RFC 7568, Deprecating Secure Sockets Layer Version 3.0)TLS(Transport Layer Security)SSL의 결함으로 인해, 보안 연결을 재설계하고 표준화 하여 TLS가 되었습니다. 표준화된 HMAC 도입TLS는 MD5 기반 MAC 대신 HMAC-SHA1·SHA256 등 표준화된 HMAC 사용 → 무결성 검증 강화하였습니다. 키 교환·난수 처리 개선프리마스터 시크릿(pre-master secret) 교환 과정을 간소화하고 안전성을 증가시켰습니다.랜덤 넘버(zero-byte padding 등)의 처리 방식 보완하였습니다. 확장(extension) 프레임워크를 지원합니다.암호 스위트, 압축 방법, 인증 방식 등을 메시지 교환 중에 동적으로 협상 가능합니다.이 확장성 덕분에, 새로운 알고리즘을 추가하기가 수훨합니다. 새로운 알고리즘이 추가된다면, extension field를 이용해 해당 알고리즘에서 필요한 field를 추가할 수 있습니다.(프로토콜 변경 없이!) References HTTPS란 무엇입니까? | cloudflare.com HTTPS란 무엇입니까? | Cloudflare HTTPS를 사용하는 이유는 무엇입니까? | cloudflare.com HTTPS를 사용하는 이유는 무엇입니까? | Cloudflare HTTP와 HTTPS의 차이점은 무엇인가요? | aws.amazon.com HTTP와 HTTPS 비교 - 전송 프로토콜 간의 차이점 - AWS DNS over TLS와 DNS over HTTPS의 비교 | 안전한 DNS | cloudflare.com DNS 보안과 TLS HTTPS 비교 | Cloudflare SSL과 TLS의 차이점은 무엇인가요? | aws.amazon.com SSL과 TLS 비교 - 통신 프로토콜 간의 차이점 - AWS SSL/TLS 인증서란 무엇인가요? | aws.amazon.com SSL 인증서란 무엇인가요? - SSL/TLS 인증서 설명 - AWS TLS Handshake | cloudflare.com TLS 핸드셰이크란? | 세션키 교환 | Cloudflare RFC 5280. X.509에 대한 표준 문서 | datatracker.ietf.org RFC 5280: Internet X.509 Public Key Infrastructure Certificate and Certificate Revocation List (CRL) Profile RFC 8446. TLS Protocol 1.3 | datatracker.ietf.org RFC 8446: The Transport Layer Security (TLS) Protocol Version 1.3 RFC 5246. TLS Protocol 1.2 | datatracker.ietf.org RFC 5246: The Transport Layer Security (TLS) Protocol Version 1.2 What is a session key? | cloudflare.com What is a session key? | Session keys and TLS handshakes | Cloudflare Cipher suite | developer.mozilla.org 암호화 스위트 (Cipher suite) - MDN Web Docs 용어 사전: 웹 용어 정의 | MDN SSL/TLS 주요 보안 이슈 | spri.kr SSL/TLS 주요 보안 이슈 - SPRi" }, { "title": "Components - Data Plane(Node) | Kubernetes Deep Dive - 3", "url": "/posts/k8s-data-plane/", "categories": "DevOps, kubernetes", "tags": "aws, kubernetes, cncf, k8s", "date": "2025-07-04 21:35:00 +0900", "snippet": "이번에는 Kubernetes에서 사용자의 Application이 돌아가는 ‘Data Plane(Node)’에서, Kubernetes 시스템을 위해 돌아가는 컴포넌트(Components)들을 알아보고자 합니다.Node와 Node의 컴포넌트들Node에 대해서컴포넌트들에 대해 이해하기에 앞서, Kubernetes에서 Node의 의미를 짚고 가고자 합니다.K...", "content": "이번에는 Kubernetes에서 사용자의 Application이 돌아가는 ‘Data Plane(Node)’에서, Kubernetes 시스템을 위해 돌아가는 컴포넌트(Components)들을 알아보고자 합니다.Node와 Node의 컴포넌트들Node에 대해서컴포넌트들에 대해 이해하기에 앞서, Kubernetes에서 Node의 의미를 짚고 가고자 합니다.Kubernetes에서 ‘Node’는, 사용자의 Pod(container로 이루어진)이 실제로 돌아가는 머신(machine)을 의미합니다.이 머신은, VM(Virtual Machine, virtualbox와 같은)일수도 있고, 물리(physical) 머신일 수도 있습니다. Kubernetes에서 Node로서 머신을 인식하기 위해서는, Network interface와 함께, ‘kubelet’이라는 컴포넌트가 중요한 역할을 합니다. 즉, OS자체라기 보다는 OS위에서 구동되는 kubelet이 중요합니다.이 Node에는 Kubernetes의 구성요소로서 역할하기 위해, 필수적(necessary)으로 필요한 서비스(컴포넌트)들을 포함하고 있습니다.(뒤에 나올 kubelet과 같은 컴포넌트들을 말합니다)이 컴포넌트들을 통해, ‘Control Plane’과 계속 통신하며 Node로서의 지위를 유지하고, Pod이 실제로 Node위에서 동작(run)하기 위한 일련의 과정을 수행합니다.Node 관리하기(추가하기)노드를 kubernetes 클러스터(cluster)에 등록하기 위해, 주로 2가지 방법을 사용합니다.노드 스스로 Control plane에 등록하는 방법(Self-registration of Nodes)노드를 관리하는 ‘kubelet’의 Flag중 --register-node 가 true(default값이 true) 라면, ‘kubelet’이 스스로 API Server를 통해 자신의 노드를 등록합니다.사용자가 수동으로 직접 ‘Node Object’를 생성하는 방법Kubernetes를 조작하기 위한 Client인 kubectl을 통해, Node Object를 수동으로 생성할 수 있습니다.만약 Node를 수동으로 등록하고자 한다면, ‘kubelet’의 --register-node=false 로 설정해야 합니다. 노드를 등록하는 과정에서, 노드의 이름을 유니크(unique)하게 관리하는게 중요합니다.Cluster에서는, 노드의 이름이 동일하다면, 동일한 ‘Node Object’로 인식합니다.이 부분을 주의하지 않으면, 이름이 동일한 다른 머신으로 인해 Cluster의 장애를 유발할 수 있습니다.kubelet‘kubelet’은 Node에서 돌아가는 Agent(사용자를 대신하여 자율적으로 작업을 수행하는 소프트웨어)입니다.‘Pod’안에 있는 container들이 계속해서 동작하도록(running상태 이도록) 하며, 노드를 운영하는 핵심적인 역할을 수행합니다. kubelet은 kubernetes를 통해 생성된 container만 관리합니다.container의 label / tag를 이용하여, cluster에 있는 container인지 구분합니다.(해당 머신에 접속하면, kubelet과 공유하는 Container runtime을 통해 다른 container를 실행할 수 있습니다)kubelet이 노드에서 하는 역할은 아래와 같습니다.PodSpec을 동기화 합니다.‘kubelet’은 API Server(kube-api-server)로 부터 자신의 노드에 할당된 PodSpec을 계속 감시합니다.Pod에 변경사항(생성되거나 수정됨)이 있다면, 노드의 Container Runtime을 통해 container를 실행하거나 종료합니다.아래는, PodSpec의 예시입니다.apiVersion: v1kind: Podmetadata: name: my-podspec: # PodSpec containers: - name: web image: nginx:1.25.0 ports: - containerPort: 80 restartPolicy: Always nodeSelector: disktype: ssd volumes: - name: config configMap: name: web-config컨테이너 런타임 인터페이스(Container Runtime Interface, CRI)과 연동되어 작동합니다.여러 컨테이너 런타임(Docker, containerd, CRI-O)에 호환성이 있어, gRPC를 통해 컨테이터 런타임을 작동시킵니다.컨테이너 런타임을 통해, 외부에서 이미지를 가져오거나(pull), container를 생성, 삭제하며, 메트릭과 로그를 수집합니다.상태 보고(Status Reporting)‘kubelet’은 Node(노드)와 Pod(파드)의 상태를 실시간으로 파악하여 보고합니다. 이러한 보고는, ‘kube-apiserver’를 통해, etcd에 최종 기록됩니다.Node 상태 보고 capacity / allocatableCPU, 메모리, 디스크와 같은 리소스 할당량 및 한계량을 API Server를 통해 Control Plane에 보고합니다. Node의 현재상태Ready(Node가 Pod을 받을 수 있는 상태)인지,DiskPressure, MemoryPressure, PIDPressure와 같은 리소스 압박 상태인지NetworkUnavailable과 같이 네트워크 문제가 있는지확인하여 보고합니다. Internal IP, Hostname, External IP등의 address를 보고합니다. Node에서 돌아가는 daemon의 endpoints(daemonEndpoints)를 보고합니다.kubelet과 같은 Daemon에 대한 포트(port)정보를 보고합니다. 이 Port정보는, Control plane과 통신하기 위한 gRPC와 HTTPS 포트입니다. 과거에는 dockershim에 대한 정보도 함께 공유됬지만, dockershim이 deprecated되면서 제거되었습니다. Pod 상태 보고 Pod의 phase를 보고합니다.‘phase’는 Pod의 ‘상태’를 말합니다. 다만, status와 다른것은, Pod Lifecycle로 추상화된 ‘high-level summary’입니다.‘Pending’, ‘Running’, ‘Succeeded’, ‘Failed’, ‘Unknown’과 같은 값이 있습니다. Pod의 Conditions를 보고합니다.Pod은 ‘PodStatus’를 가지는데, 이는 여러 condition으로 이루어져 있습니다.‘PodScheduled’, ‘PodReadyToStartContainers’, ‘Ready’등의 값이 있습니다. Pod에 포함된 Container상태 보고Pod의 Container상태를 보고합니다.ready, restartCount(재시작 횟수), state와 함께, 상세사유를 보고합니다. Host IP와 Pod IP를 보고합니다. startTime을 보고합니다.Pod에 대한 헬스체크 및 라이프사이클(Lifecycle) 관리‘kubelet’은 Pod의 현재상태를 체크하며, 라이프 사이클에 따라 적당한 작업을 수행합니다.Pod 상태 확인개별 Container에 대한, ‘Liveness’, ‘Readiness’, ‘Startup Probe’를 실행해 헬스(health) 상태를 판단합니다.Lifecycle 관리실패한 Container를 재시작하거나, Ready상태를 해제해, 서비스 트래픽에서 제외시킵니다.볼륨 마운트 관리Pod에 정의된 PV(PersistentVolume), ConfigMap, Secret 볼륨을 마운트(mount)하거나 언마운트(unmount) 합니다.노트 상태 관리노드의 리소스 사용량(CPU, 메모리, 디스크)과 헬스(Ready/NotReady)를 판단해 API Server에 보고합니다.리소스 부족 시에는 evict(축출) 정책을 실행합니다.kubelet의 구성요소 Pod ManagerPodSpec을 해석하여, runtime 명령어로 변환하고 실행합니다. Probe ManagerLiveness / Readiness / Startup Probe 을 스케쥴링 하고 실행합니다. Volume ManagerCSI(Container Storage Interface) 플러그인을 연동하고, 볼륨을 마운트 / 언마운트 합니다. CSI(Container Storage Interface):Container Runtime을 위한 Storage Interface를 말 합니다.AWS나 GCP와 같이 kubernetes환경마다 다양한 storage를 지원하기 때문에, 이런 다양한 storage를 지원하기 위해 만들어진 interface입니다. Eviction ManagerNode의 자원(CPU, RAM, Disk 같은) 압박(pressure) 상황에서 Pod을 축출(eviction)합니다 Status ManagerAPI Server를 통해 Pod/Node의 상태를 업데이트 합니다.확장 가능한 부분 Device PluginsGPU, FPGA와 같은 특수 하드웨어 리소스를 할당하기 위해 별도의 플러그인을 설치할 수 있습니다. Custom Metrics AdapterApplication의 메트릭을 Pod레벨로 노출시켜 사용할 수 있게 합니다. ‘Prometheus Adapter(커스텀 메트릭 어댑터)’를 통해, App에서 제공하는 메트릭을 HPA(Horizontal Pod Autoscaler)가 참조하도록 할 수 있습니다. Static PodsNode에서 직접 정의된 YAML로 노드수준의 kubelet이 직접 관리하는 Pod입니다. 즉 API Server없이 실행되는 Pod입니다.kube-proxykube-proxy와 Service의 machanism ‘kube-proxy’는 각각의 Node에서 작동하는 network proxy 입니다. Kubernetes의 ‘Service’라는 추상화된 컨셉을 적용하기 위한 컨포턴트 입니다. Node의 network rule을 관리하며, 이를 통해 Pod이 Cluster내부/외부 모두 통신할 수 있게 해줍니다.만약 OS에서 ‘Packet filtering layer’가 있다면 해당 기능을 사용하고, 그렇지 않으면 ‘kube-proxy’가 직접 그 역할을 수행합니다(Golang 기반의 App). 리눅스 커널에는 Netfilter(iptables)나 IPVS 같은 ‘Packet filtering / routing’ 기능을 포함하고 있습니다.이를 이용할 수 있으면, ‘kube-proxy’의 packet 처리 기능을 커널레벨에서 처리하므로, 매우 빠르게(CPU효율적으로) 처리할 수 있습니다.주요 역할 Service IP와 Pod IP를 매핑(mapping)합니다.Kubernetes의 ‘Service’에 할당된 ClusterIP(Cluster수준에서 사용되는 가상 IP)를 통해 traffic을 받으면, 이를 개별 Pod으로 포워딩 해줍니다. Load-balancing(부하 분산)을 수행합니다.‘kube-proxy’가 작동하는 ‘모드’에 따라, 다른 balancing이 이루어 집니다. 노드 간 / 노드 밖에 대한 트래픽을 라우팅 합니다. 동작 모드‘kube-proxy’는 크게 2가지의 모드로 구분됩니다. kube-proxy는 Host의 OS에 따라, 사용할 수 있는 mode가 제한됩니다.ipTables 모드(default설정)ipTables Rule을 통해 보는, LB부터 Pod에 이르는 packet flow - from cncf.io‘Service’생성 시에, ‘서비스에 대한 ClusterIP + 포트’ 조합에 대해 iptables 체인을 설정합니다.노드의 커널 레벨에서, ‘서비스의 ClusterIP + Port’로 들어오는 패킷을 RR(Round Robin) 방식으로 뒷단의 Pod IP로 리다이렉트 합니다. 장점은 커널 레벨에서 처리하기 때문에, 높은 성능을 제공하므로, 낮은 지연시간을 보여줍니다. 단점은 규칙 수가 너무 많아지면 iptables 룰 체인이 커져서, 관리 오버헤드가 발생합니다.아래는, ‘iptables’에서 ‘my-service’라는 ClusterIP서비스(10.96.0.10:80)가 2개의 백엔드 Pod(192.168.1.11:8080, 192.168.1.12:8080)로 트래픽을 분산하는 Rule의 모습을 보여줍니다.$ iptables -t nat -L KUBE-SERVICES -n --line-numbersChain KUBE-SERVICES (2 references)num target prot opt source destination1 KUBE-SEP-ABCDEF123456 tcp -- 0.0.0.0/0 10.96.0.10 /* default/my-service: cluster IP */ tcp dpt:802 KUBE-MARK-MASQ all -- 0.0.0.0/0 10.96.0.10 /* default/my-service: cluster IP */ 3 RETURN all -- 0.0.0.0/0 0.0.0.0/0 1번 룰: Service IP(10.96.0.10:80, Cluster IP) 로 들어오는 TCP 패킷을 KUBE-SEP-ABCDEF123456 체인으로 점프시킵니다. 2번 룰: SNAT(소스 마스커레이드) 표시를 위해 KUBE-MARK-MASQ 체인으로 점프. 3번 RETURN: 더 이상 매칭되지 않으면 원래 체인으로 복귀.$ iptables -t nat -L KUBE-SEP-ABCDEF123456 -n --line-numbersChain KUBE-SEP-ABCDEF123456 (1 references)num target prot opt source destination1 DNAT tcp -- 0.0.0.0/0 192.168.1.11 /* default/my-service */ tcp dpt:80802 DNAT tcp -- 0.0.0.0/0 192.168.1.12 /* default/my-service */ tcp dpt:80803 RETURN all -- 0.0.0.0/0 0.0.0.0/0 1번 DNAT 룰: 첫 번째 백엔드 Pod로 DNAT. 2번 DNAT 룰: 두 번째 백엔드 Pod로 DNAT. 3번 RETURN: 매칭 실패 시 복귀.$ iptables -t nat -L KUBE-MARK-MASQ -n --line-numbersChain KUBE-MARK-MASQ (1 references)num target prot opt source destination1 MARK all -- 0.0.0.0/0 10.96.0.0/12 MARK set 0x4000 MASQ 표시: Service 외부(외부IP 또는 NodePort)로 나가는 패킷에 마스커레이드를 적용해야 할 때, 이 마크를 보고 SNAT을 수행합니다.IPVS(IP Virtual Server) 모드kube-proxy가 IPVS 모드로 동작할 때는, 리눅스 커널의 IP Virtual Server 기능을 이용해 가상 서버(Virtual Server) 를 띄우고, 여기에 실제 백엔드 Pod(Real Servers) 를 등록하는 방식으로 로드밸런싱을 수행합니다.작동 과정 리눅스의 커널 레벨의 IPVS 프레임워크에 Service Virtual Server(VS)를 생성합니다. 각 Endpoint(Backend) 서버를 Virtual Server에 등록합니다. IPVS 스케줄러(rr, lc, wlc 등)로 트래픽 분배합니다. 장점은 대규모 Service 처리 시 iptables 대비 더 빠르고, 룰 관리가 간단합니다. RR(Round Robin)뿐만 아니라, 다양한 스케줄러 모드를 지원합니다. 단점은 커널 모듈 의존성(CentOS/RHEL 등 커널 패치 필요)이 있어서, 초기 세팅이 어렵습니다.내부 구성요소 WatcherAPI Server의 /services 및 /endpoints 리소스를 계속해서 감시합니다. Proxier모드(iptables/IPVS)별로 룰을 생성하고 갱신합니다. Local ManagerNode 로컬 네트워크에 바인딩 혹은 해제된 포트를 관리합니다. Metrics Serverkube_proxy_metrics 을 통해, 연결 / 종료 건수나 오류율 등을 노출시킵니다. Service 처리 Flow클라이언트 Pod에서 다른 노드의 서버 Pod로의 트래픽 흐름 kube-proxy에서 iptables를 최신상태로 갱신합니다.‘kube-proxy’는 API Server를 통해, Pod 목록(라우팅 대상)을 갱신하며, 이를 각 Node의 ‘iptables’에 Rule로 반영합니다. 클라이언트 Pod에서는 Cluster내부의 Service를 호출합니다. Service 호출 traffic은 Client Pod이 있는 Node의 iptables에 의해, 목적지 IP와 Port번호가 갱신되어, Backend Pod에 전달됩니다.iptables의 DNAT(Destination NAT) Rule을 이용해, ‘목적지 IP(Cluster 수준의 IP)’ → ‘Backend Pod의 IP’로 변경됩니다. kube-proxy를 다른것으로 대체할 수 있습니다.Kubernetes의 공식문서에 보면, ‘kube-proxy’는 Optional로 표현되어 있는데, 이는 서비스 트래픽에 대한 proxy역할을 ‘kube-proxy’가 아니어도 대체할 수 있기 때문입니다.Service Mesh 솔루션Istio, Linkerd, Kuma 같은 서비스 메시를 쓰면, Envoy, dataplane 에이전트가 Pod 간·외부 트래픽을 가로채어 처리하며, kube-proxy를 건너뛰고도 충분한 로드밸런싱/리트라이정책 적용이 가능합니다.Headless 서비스ClusterIP를 쓰지 않고 DNS 기반으로 Endpoint IP 리스트를 Pod가 직접 조회해 접속하는 패턴(headless service)을 쓰면, kube-proxy가 아예 개입할 여지가 없습니다.Container runtime A fundamental component that empowers Kubernetes to run containers effectively.- kubernetes.io‘Container runtime’은 Kubernetes의 Pod에 있는 Container를 돌리는 runtime환경입니다.Container에 대한 실행과 Lifecycle관리에 대한 책임을 갖고 있습니다.Container Runtime Interface(CRI)Kubernetes에선, kubelet과 ‘Container Runtime’이 gRPC로 통신할 수 있는 Interface를 지원합니다. 이를 ‘Container Runtime Interface(CRI)’라고 합니다.Kubernetes에서는 CRI를 이용하여, 여러 Container Runtime을 지원합니다.container.dCNCF 프로젝트였으며, Docker의 핵심만 분리하여 경량화한 runtime입니다.Kubernetes v1.24 이후부터는 ‘container.d’가 default runtime입니다. Q: Docker Engine에서 container.d로 default runtime이 바뀌게된 이유?A: Docker Engine은 OCI(Open Container Initiative) 규격보다 훨씬 많은 기능(빌드, 네트워크 관리, 로그 드라이버 등)을 포함한 “풀 스택” 플랫폼이었고, 이를 CRI로 감싸기 위해 dockershim이라는 중간 계층(shim)을 유지해야 했습니다.containerd는 애초에 OCI 런타임에만 집중한 경량 서비스로, CRI 플러그인을 붙이면 kubelet과 직접 통신할 수 있어(shim계층 불필요) default runtime으로 자리잡게 되었습니다.CRI-O‘Red Hat’의 주도로 만들어진, OpenShift(Red hat의 k8s기반 오케스트레이션 플랫폼)에 최적화된 runtime입니다.cgroup(control groups) DriversLinux의 ‘cgroup(Control Groups)’은 컨테이너 런타임이 ‘리소스 격리 및 할당’을 위해 사용하는 핵심 커널 기능입니다.cgroup은 프로세스(또는 프로세스 그룹)에 대해 CPU, 메모리, 블록 I/O, PID 수 등을 강제로 제한 / 계측하고, 우선순위를 지정할 수 있게 해 줍니다.Kubernetes에서의 활용kubelet에서는 --cgroup-driver 를 통해, cgroup으로 사용할 드라이버를 설정할 수 있습니다.아래와 같은 2가지 드라이버가 가능한 옵션입니다. cgroupfs systemd실습Data Plane의 Node 조회Cluster의 노드 목록 조회$ ~ kubectl get nodesNAME STATUS ROLES AGE VERSIONip-10-20-99-219.ap-northeast-2.compute.internal Ready &lt;none&gt; 8m58s v1.32.1-eks-5d632ec개별 Node의 세부 사항 조회$ kubectl describe nodes ip-10-20-99-219.ap-northeast-2.compute.internalName: ip-10-20-99-219.ap-northeast-2.compute.internalRoles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=t3.large beta.kubernetes.io/os=linux ...Annotations: alpha.kubernetes.io/provided-node-ip: 10.20.99.219 node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Thu, 10 Jul 2025 17:27:37 +0900Taints: &lt;none&gt;Unschedulable: falseLease: HolderIdentity: ip-10-20-99-219.ap-northeast-2.compute.internal AcquireTime: &lt;unset&gt; RenewTime: Thu, 10 Jul 2025 17:38:31 +0900Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Thu, 10 Jul 2025 17:34:34 +0900 Thu, 10 Jul 2025 17:27:33 +0900 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Thu, 10 Jul 2025 17:34:34 +0900 Thu, 10 Jul 2025 17:27:33 +0900 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Thu, 10 Jul 2025 17:34:34 +0900 Thu, 10 Jul 2025 17:27:33 +0900 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Thu, 10 Jul 2025 17:34:34 +0900 Thu, 10 Jul 2025 17:27:56 +0900 KubeletReady kubelet is posting ready statusAddresses: InternalIP: 10.20.99.219 InternalDNS: ip-10-20-99-219.ap-northeast-2.compute.internal Hostname: ip-10-20-99-219.ap-northeast-2.compute.internalCapacity: cpu: 2 ephemeral-storage: 31379436Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 7999148Ki pods: 35Allocatable: cpu: 1930m ephemeral-storage: 27845546346 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 7241388Ki pods: 35System Info: Machine ID: ... System UUID: ... Boot ID: ... Kernel Version: 6.1.127-135.201.amzn2023.x86_64 OS Image: Amazon Linux 2023.6.20250203 Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.7.25 Kubelet Version: v1.32.1-eks-5d632ec Kube-Proxy Version: v1.32.1-eks-5d632ecProviderID: aws:///ap-northeast-2c/i-0288f536d015d5e34Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system aws-node-496nj 50m (2%) 0 (0%) 0 (0%) 0 (0%) 9m23s kube-system coredns-586c6dd46b-5s5cc 100m (5%) 0 (0%) 70Mi (0%) 170Mi (2%) 9m36s kube-system coredns-586c6dd46b-vsdfd 100m (5%) 0 (0%) 70Mi (0%) 170Mi (2%) 9m36s kube-system kube-proxy-nc5tm 100m (5%) 0 (0%) 0 (0%) 0 (0%) 10mAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 350m (18%) 0 (0%) memory 140Mi (1%) 340Mi (4%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 10m kube-proxy Normal Starting 10m kubelet Starting kubelet. Warning InvalidDiskCapacity 10m kubelet invalid capacity 0 on image filesystem Normal NodeHasSufficientMemory 10m (x2 over 10m) kubelet Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 10m (x2 over 10m) kubelet Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 10m (x2 over 10m) kubelet Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 10m kubelet Updated Node Allocatable limit across pods Normal Synced 10m cloud-node-controller Node synced successfully Normal RegisteredNode 10m node-controller Node ip-10-20-99-219.ap-northeast-2.compute.internal event: Registered Node ip-10-20-99-219.ap-northeast-2.compute.internal in Controller Normal NodeReady 10m kubelet Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeReadyNode의 System Component 조회Node의 System Component 조회$ kubectl get pods --namespace=kube-system --field-selector spec.nodeName=ip-10-20-99-219.ap-northeast-2.compute.internalNAME READY STATUS RESTARTS AGEaws-node-496nj 2/2 Running 0 17m ## Amazon VPC CNI 플러그인coredns-586c6dd46b-5s5cc 1/1 Running 0 17mcoredns-586c6dd46b-vsdfd 1/1 Running 0 17mkube-proxy-nc5tm 1/1 Running 0 18mkube-proxy 조회$ kubectl get daemonSets --namespace=kube-system kube-proxyNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkube-proxy 1 1 1 1 1 &lt;none&gt; 18mCluster의 DNS Deployments 조회$ kubectl get deployments --namespace=kube-system corednsNAME READY UP-TO-DATE AVAILABLE AGEcoredns 2/2 2 2 25mDNS에 대한 Service조회$ kubectl get services --namespace=kube-system kube-dnsNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 172.20.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 28mReferences Nodes | kubernetes.io Nodes Node Components | kubernetes.io Kubernetes Components Pod Lifecycle | kubernetes.io Pod Lifecycle Container Storage Interface(CSI) for Kubernetes GA | kubernetes.io Container Storage Interface (CSI) for Kubernetes GA Container Storage Interface(CSI) spec | github.com/container-storage-interface/spec https://github.com/container-storage-interface/spec/blob/master/spec.md kube-proxy | kubernetes.io kube-proxy Virtual IPs and Service Proxies | kubernetes.io Virtual IPs and Service Proxies Iptables proxy mode | kubernetes.io Virtual IPs and Service Proxies IPVS proxy mode | kubernetes.io Virtual IPs and Service Proxies Service ClusterIP allocation | kubernetes.io Service ClusterIP allocation Kubernetes’s IPTables Chains Are Not API | kubernetes.io Kubernetes’s IPTables Chains Are Not API Kubernetes networking demystified: a brief guide | cncf.io Kubernetes networking demystified: a brief guide kube-proxy | GKE networking overview | Google Cloud 네트워크 개요 | Google Kubernetes Engine (GKE) | Google Cloud Container Runtime Interface(CRI) | github.com/kubernetes/community https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md Container Runtimes | kubernetes.io Container Runtimes" }, { "title": "ChatGPT API 비용최적화 및 Bulk요청 보내기 | Prompt Engineering", "url": "/posts/ChatGPT-API-Pricing-N-Bulk-Request/", "categories": "Prompt Engineering", "tags": "chatgpt, ai, llm, openai, pricing", "date": "2025-07-01 17:03:00 +0900", "snippet": "LLM을 이용한 서비스를 개발할때, 처음에는 ChatGPT와 같은 SaaS(Software as a Service)의 API를 사용하게 됩니다.이 API를 기반으로 Production 서비스를 운영하게 되면, 생각보다 많은 비용이 나오게 됩니다.ChatGPT API PricingChatGPT API PricingChatGPT API의 Pricing 정...", "content": "LLM을 이용한 서비스를 개발할때, 처음에는 ChatGPT와 같은 SaaS(Software as a Service)의 API를 사용하게 됩니다.이 API를 기반으로 Production 서비스를 운영하게 되면, 생각보다 많은 비용이 나오게 됩니다.ChatGPT API PricingChatGPT API PricingChatGPT API의 Pricing 정책에 따르면, 다음과 같은 내용에 대해, 1M token당 과금정책을 사용합니다. InputLLM에 입력으로 사용되는 모든 텍스트에 대한 토큰수 입니다. Cached inputCache정책에 따라, ChatGPT 서버에 캐시된 요청을 재사용할 경우, 별도의 token당 과금 정책이 적용됩니다.(좀 더 저렴합니다.) OutputLLM의 Output에 해당하는 모든 텍스트의 토큰수 입니다. ChatGPT의 tokenizer를 통해, token갯수를 계산할 수 있습니다.API 비용 최적화 위의 Pricing정책을 기준으로 보면, 비용최적화 방법은, Token 수를 줄이면서 동일한 작동을 하게 합니다. Pricing 정책 자체를 저렴하게 적용받도록 합니다.이렇게 나뉘어 집니다.이에 따라 비용 최적화 방법을 알아보겠습니다.Prompt Cache가 적용되는 모델 사용하기 Prompt Caching is automatically applied on the latest versions of GPT‑4o, GPT‑4o mini, o1‑preview and o1‑mini, as well as fine-tuned versions of those models.- from OpenAI BlogChatGPT API의 최신 모델을은 이전에 사용한 Input 데이터를 Cache해서 재사용합니다.이 Cache된 Token들은, 5~10분정도 후에 비활성화 되고, 마지막으로 사용한지 1시간 안에 완전히 제거된다고 합니다.아래는, API 요청에 대한 Response Body입니다.여기서, cached_tokens 를 통해, cache를 사용했는지 알 수 있습니다.{ \"...\": \"\", \t\"usage\": {\t \"total_tokens\": 2306,\t \"prompt_tokens\": 2006,\t \"completion_tokens\": 300,\t \t \"prompt_tokens_details\": {\t \"cached_tokens\": 1920,\t \"audio_tokens\": 0,\t },\t \"completion_tokens_details\": {\t \"reasoning_tokens\": 0,\t \"audio_tokens\": 0,\t }\t}}Fine-tuning시에는 저렴한 이전세대 모델을 사용합니다.만약 모델을 Fine-tuning해서 사용한다면, 별도의 Pricing이 적용됩니다(좀 더 비쌉니다).이때, 무조건 최신 모델을 쓰는 것보다, 이전 세대의 모델을 Fine-tuning해서 사용하는게 더 적합합니다. ‘Fine-tuning’하게 되는 의도를 봤을때, 사용 목적이 뚜렷하기 때문에, 이전 세대의 모델에서도 충분한 성능을 발휘해서 그렇습니다.이때, 트레이닝 데이터를 만들때, 상위 모델을 사용해서 데이터를 만들면 더 효과적입니다.References GPT-3.5-Turbo is cheap and fast, but is’s not as smart as GPT-4 | OpenAI YoutubeThe New Stack and Ops for AIPrompt내 JSON에 대해, minify하기Input Prompt내에, JSON형식을 사용하는 부분이 있다면, 이를 minify하면, Token절약에 도움이 됩니다. minify하게 되면, 들여쓰기(indentation)와 같은 공백과 줄바꿈이 제거됩니다.아래 JSON 데이터를 기준으로 비교해 보겠습니다.{ \"orders\": [ { \"orderId\": \"A1\", \"date\": \"2025-07-01\", \"customerName\": \"김철수\", \"items\": [ { \"productId\": \"P1\", \"qty\": 2 }, { \"productId\": \"P2\", \"qty\": 1 } ] }, { \"orderId\": \"A2\", \"date\": \"2025-07-01\", \"customerName\": \"이영희\", \"items\": [ { \"productId\": \"P3\", \"qty\": 1 } ] }, { \"orderId\": \"A3\", \"date\": \"2025-07-01\", \"customerName\": \"박민수\", \"items\": [ { \"productId\": \"P4\", \"qty\": 3 }, { \"productId\": \"P5\", \"qty\": 2 } ] } ]}원본의 Token 수215개의 Token으로 나누어 집니다.원본 JSON의 Token 수Minified된 JSON의 token 수기존의 215 → 133으로 Token수 가 줄었습니다.Minified JSON의 Token 수Bulk 요청으로 System Prompt 비용 최소화 하기ChatGPT API를 Production 서비스에서 사용할때, Input 데이터로 가장 반복적이고 큰 부분을 차지하는 것이 ‘System Prompt’입니다.이 ‘System Prompt’를 재사용 하는 방법으로, LLM에 여러 요청을 한번에 보내는 ‘Bulk’요청을 생각하게 되었습니다. 처음에는, ChatGPT앱에서 처럼, 대화창을 하나 생성해서, Context를 재사용하는 방법을 찾아보았지만, 그런 방법은 없었습니다. ‘System Prompt’는 ChatGPT에게 지시사항을 입력하는 용도의 Prompt입니다.이는, ChatGPT API의 ‘Structured Output’기능을 통해, LLM에 Bulk요청에 대한 의도를 전달하는 방법입니다.ExampleSystem Prompt를 다음과 같이 구성합니다.‘Few shot’을 위해, 몇가지 Example을 전달합니다.## System prompt**Role** - English-speaking coach**Goal**- Assess grammar and non-native difficulty.**Few-Shot Examples****Example 1** Input Sentence: 1. “And now I'm going to clean up or remove the clutter in this space.” 2. Build your muscle memory, so in the interview, it will naturally be easier to pronounce.Output:{\"0\":{\"difficulty\":\"medium\",\"grammar\":\"correct\"},\"1\":{\"difficulty\":\"medium\",\"grammar\":\"correct\"}}JSON Schema를 이용해, ‘Structured Output’을 설정합니다.Response Schema{ \"name\": \"check-grammar\", \"strict\": true, \"schema\": { \"$schema\": \"https://json-schema.org/draft/2020-12/schema\", \"type\": \"object\", \"patternProperties\": { \"^[0-9]+$\": { \"$ref\": \"#/definitions/body\" } }, \"additionalProperties\": false, \"definitions\": { \"body\": { \"type\": \"object\", \"properties\": { \"difficulty\": { \"type\": \"string\", \"enum\": [ \"low\", \"medium\", \"high\" ] }, \"grammar\": { \"type\": \"string\", \"enum\": [ \"correct\", \"incorrect\" ] } }, \"required\": [ \"difficulty\", \"grammar\" ], \"additionalProperties\": false } }, \"properties\": {}, \"required\": [] }}이제 User Prompt안에서 Bulk로 처리할 수 있습니다.## User PromptInput Sentence:1. Not until the culmination of her exhaustive research did Dr. Emerson realize the profound implications her findings would have on contemporary epistemology.2. DevOps is everywhere, but too often, people think they can buy “DevOps in a box” and just sprinkle some tools and automation over your broken or slow (or even super-fast AWS) stack.Output{\"0\":{\"difficulty\":\"high\",\"grammar\":\"correct\"},\"1\":{\"difficulty\":\"medium\",\"grammar\":\"correct\"}}Batch 요청 사용하기꼭 동기식(Synchronous) 처리가 필요한게 하니라면, 비동기식(Asynchronous)처리인 ‘Batch API’를 통해 50%정도의 비용을 줄일 수 있습니다.ChatGPT Batch API PricingReferences ChatGPT API Pricing | ChatGPT API Reference OpenAI Platform Prompt Caching int the API | OpenAI Blog Prompt Caching in the API Prompt Caching | ChatGPT API OpenAI Platform Structured Outputs | ChatGPT API Reference OpenAI Platform GPT-3.5-Turbo is cheap and fast, but is’s not as smart as GPT-4 | OpenAI Youtube The New Stack and Ops for AI ChatGPT Batch API | ChatGPT API Reference OpenAI Platform" }, { "title": "Components - Control Plane | Kubernetes Deep Dive - 2", "url": "/posts/k8s-control-plane/", "categories": "DevOps, kubernetes", "tags": "aws, kubernetes, cncf, k8s", "date": "2025-06-30 20:14:00 +0900", "snippet": "Kubernetes에서 ‘Control Plane’은 Cluster를 운영하는 Core혹은 ‘뇌’의 역할을 합니다.Cluster를 다루기 위한 API요청을 검증하고, 상태를 저장하고, 요청을 실행시키고, Cluster를 유지하는 역할을 합니다.여기서는 ‘Control Plane’이 어떤 Component를 통해 이런 역할을 수행하는지 알아봅니다.Kub...", "content": "Kubernetes에서 ‘Control Plane’은 Cluster를 운영하는 Core혹은 ‘뇌’의 역할을 합니다.Cluster를 다루기 위한 API요청을 검증하고, 상태를 저장하고, 요청을 실행시키고, Cluster를 유지하는 역할을 합니다.여기서는 ‘Control Plane’이 어떤 Component를 통해 이런 역할을 수행하는지 알아봅니다.Kubernetes의 Cluster Architecture | https://kubernetes.io/docs/concepts/architecture/kube-apiserverKubernetes를 조작(control)하기 위한 API를 외부에 노출하는 서비스입니다. Kubernetes 입장에서 보면(backend로 두고 보면), ‘kube-apiserver’는 frontend에 해당합니다. ‘kube-apiserver’는 수평확장(scale horizontally, scale-out)이 가능합니다. 이는 Node가 늘어남에 따라, apiserver도 확장되는것을 의미합니다. 인증(Authentication), 인가(Authorization), Admission Control(승인 제어)를 수행합니다. 요청 Schema에 대해 검증 하고 변환합니다. etcd에 대한 CRUD Interface라고 말하기도 합니다.Admission Control Admission control mechanisms may be validating, mutating, or both.- from Admission Control in Kubernetes‘Admission Control’은 요청의 ‘승인(Admission)’을 ‘제어(Control)’하는 기능을 말합니다.‘kube-apiserver’로 들어온 요청을 가로채서(intercept), ‘변경, 검증’에 대한 별도의 과정을 수행하는 기능입니다. 이 ‘Admission Control’은 요청에 대한 인증(authentication)과 인가(authorization)이 완료된 요청에 대해서 반영됩니다. 이는 크게 2가지 과정으로 이루어집니다. 요청 변경(mutating). Mutating controller를 통해 요청 내용을 수정할 수 있습니다. 요청 검증(validation). Validating controllers를 통해 요청을 검증하여, 더 이상 진행되지 못한게 할 수 있습니다.위 기능중 하나만 작동하게 할 수도 있고, 모두 작동하게 할 수도 있습니다. 단 ‘Read(get, watch or list)’는 막을(block) 수 없습니다. CREATE, UPDATE, DELETE와 같은 ‘변경(mutating)’ 요청에 대해서만 적용됩니다.혹은, 연결(connect) 같은 커스텀 동작에 대해서 적용됩니다.(ex: kubectl exec)Admission Control FlowAdmission Control Flow Request가 kube-apiserver에 도착합니다 인증(authentication)과 인가(authorization)을 수행합니다. 변경(mutating)에 대해 등록된 모든 Webhook을 실행합니다.‘Mutating Admission controllers’가 수행합니다. 검증(validation)에 대한 정책(policy)에 대해서 validation을 수행합니다.‘Mutating’단계가지 마치고, 만들어진 최종 객체에 대해서, Cluster 내부에서 정의된 정책(policy)를 기반으로 평가하는 단계 입니다. 검증(validation)에 대해 등록된 모든 Webhook을 실행합니다. 이전 단계인 정책(policy) validation과는 다르게, 외부의 HTTP endpoint에 요청을 보냅니다. etcd에 저장합니다.MutatingAdmissionWebhook Controller요청(request) 객체를 변경(patch)합니다.이때, 기본값을 주입하거나, 사이드카(sidecar)가 삽입됩니다. 여기서 ‘사이드카(sidecar)’는 패턴으로 자리잡아, Kubernetes에서 핵심적인 역할을 하게 됩니다.ValidatingAdmissionWebhook Controller요청(request) 객체가 정책에 부합하는지 검사하고, 허용하거나 거부합니다.모든 Mutating이 끝나고 나서 실행됩니다.성능 혹은 스케일링 시에 튜닝할 수 있는 부분 API Server의 QPS(Queries Per Second)나 Burst를 조정할 수 있습니다 # QPS 조정 # default값이 20 $ kube-controller-manager --kube-api-qps 20 # Burst 조정 # default값이 30 $ kube-controller-manager --kube-api-burst 30 Source codehttps://github.com/kubernetes/apiserveretcd etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines.- from etcd homepage‘etcd’는 key-value 데이터 저장소입니다. 강한 일관성(strongly consistent), 분산형(distributed) 특징을 통해 매우 안정적인 저장소 역할을 합니다. Kubernetes에서 etcd는 클러스터에 대한 상태 저장소 역할을합니다. 모든 k8s객체(Deployment, Service, ConfigMap등등)의 Desired State를 저장합니다. 컨트롤 loop와 연동되어, 컨트롤러 매니저와 스케줄러 같은 제어 컴포턴트가 변경을 감지하여, 행동을 시작하도록 합니다. TLS를 통한 암호화 통신, 클라이언트 인증(Cert-based), Role-Based Access Control(RBAC) 설정으로 데이터를 보호합니다.etcd는 다음과 같은 기능을 제공합니다.강한 일관성(strongly consistent)모든 읽기(read)/쓰기(write) 요청에 대해 최신상태를 보장하며, ‘읽기 후 쓰기(Write after read, WAR)’ 시점에 stale 데이터(구버젼 데이터)가 반환될 위험이 없습니다. ‘읽기 후 쓰기(Write After Read, WAR)’:CPU레벨에서 명령어를 병렬처리 할때, 처리 순서에 따라 반환값이 달라질 수 있는 문제를 말합니다.즉, 여기서는 ‘데이터를 읽을때 마다 값이 다를 가능성’에 대한 얘기를 하고 있습니다.분산형(distributed) 시스템의 리더, 팔로워 구조 etcd는 리더(Leader)와 다수의 팔로워(Follower) 노드로 분산처리 시스템을 구성합니다. 쓰기(write) 요청은 리더가 처리한 후에, 팔로워에 복제합니다. 커밋이 완료되면, 해당 데이터를 클라이언트에게 노출합니다.(강력한 일관성 보장)etcd의 분산 시스템은, Raft 알고리즘을 사용합니다.Raft 합의 알고리즘분산 환경에서 데이터의 일관성과 가용성을 보장하기 위해 사용하는 알고리즘입니다.‘리더 선출’, ‘로그 복제(Log Replication)’, ‘안정성 보장’이 특징입니다. 리더 선출 팔로워가 리더의 신호를 못 받으면, 팔로워들이 후보상태로 전환되어 새로운 리더를 선출합니다. 이런 이유로, etcd의 인스턴스는 홀수개로 이루어져야 합니다. 짝수개인 경우, 리더가 실패시 후보로 나온 인스턴스를 빼면, 다시 짝수개가되어 선출과정에서 동수가 나올 수 있습니다. 로그 복제 클라이언트의 key-value 변경 요청을 받으면, 이를 리더에게 보냅니다. 리더는 이를 받아서 Log에 추가하며, 이를 팔로워들에게 전파시킵니다. 과반의 팔로워가 저장했다고 하면, 이를 commit합니다. 안정성 보장 commit된 로그는 반드시 과반의 노드에 복제되어 있습니다.데이터 모델etcd는 key-value 쌍(pair)를 B+ tree (BoltDB)형태로 저장합니다.etcd의 데이터 모델 etcd는 별도의 In-memory btree도 함께 운영하는데, 이는 key에 대한 query속도를 높여주는 역할을 합니다. in-memory btree를 통해 revision정보를 얻고, 이를 기반으로(b+ tree의 key로 사용해서) b+ tree에서 value값을 가져옵니다.TuningTuningkube-scheduler‘kube-scheduler’는 Pod을 Node에 배치(binding)하는 역할을 합니다.Cluster의 리소스 요청, 정책(affinity/taint, toleration 등), 사용자 정의를 모두 종합하여 최적의 Node를 선택합니다. 이 과정에서 Pod이 스케쥴링 될 수 있는 Node를 ‘feasible Node’라고 합니다.만약, 적합한 Node가 없다면, Pod은 스케쥴 되지 못한 채로 있습니다. 이는 조건에 부합한 Node가 나타날때가지 계속됩니다.Pod이 스케쥴링 되는 과정‘kube-scheduler’를 통해서 Pod이 배치되는 과정은 2개의 step으로 이루어져 있습니다. Filtering특정 조건에 따라 ‘feasible Node’를 찾습니다. 이 과정이 완료되면, 적합한 Node List가 반환됩니다. Scoring‘Filtering’에서 만들어진 Node List를 기반으로, 순위를 매깁니다(rank). 이를 기반으로 가장 적합한 Node를 선택하여 Pod을 배치합니다.만약 동점인 Node가 있다면, 이 Node들 사이에서 random하게 배치됩니다.‘kube-scheduler’는 이 결과를 API서버에 알려줍니다(notify). Pod이 스케쥴링 되는 과정Scheduler와 다른 Component의 상호작용스케쥴링 정책(Scheduling Policies)를 통해 스케쥴링을 조정할 수 있습니다. (v1.23이전 한정)$ kube-scheduler --policy-config-file &lt;filename&gt;# or$ kube-scheduler --policy-configmap &lt;ConfigMap&gt;스케쥴러 설정(Scheduler Configuration)을 통해 위와 동일한 설정을 할 수 있습니다.(v1.23 이상)$ kube-scheduler --config &lt;filename&gt;# configuration 파일 예시apiVersion: kubescheduler.config.k8s.io/v1kind: KubeSchedulerConfigurationclientConnection: kubeconfig: /etc/srv/kubernetes/kube-scheduler/kubeconfigSource Codehttps://github.com/kubernetes/kube-schedulerkube-controller-manager‘kube-controller-manager’는 ‘control loops’를 운영하는 컴포넌트입니다.여기서 ‘loop’는 ‘종료되지 않음’을 의미합니다.‘kube-controller-manager’는 계속해서 시스템의 현재 상태(the state of system)를 관찰하고, 현재상태와 목표 상태(Desired State)가 다르다면, 이에 도달하기 위해 변화(changes)를 시도합니다.Control Loop FlowControl loop의 flow‘kube-controller-manager’는 ‘Control Loop’를 통해 Cluster의 현재상태와 목표 상태(Desired State)를 계속 비교하며, 동일하게 맞춥니다.이 Loop안에서, 각각의 Kubernetes Object에 맞는 ‘Controller’를 사용합니다.controller 종류 Node Controller노드의 상태 변화를 감시(Ready/NotReady), 노드 삭제 감지 시 인그레스/서비스 정리 Replication controller / Replicaset controllerreplicas 수를 보장하기 위해 Pod을 증감을 관장 Endpoints controllerService와 Pod IP 매핑 정보 유지 Namespace controllerNamespace 리소스 생성·삭제 후, 네임스페이스 내부 리소스 정리 Service controller클라우드 로드밸런서 생성·삭제, EndpointSlice 동기화 Deployment Controller롤링 업데이트, 롤백, 배포전략(Blue/Green, Canary) 관리 Horizontal Pod Autoscaler(HPA) …등Horizontal Pod Autoscaler(HPA)Kubernetes에서 워크로드의 부하(CPU 사용률, 메모리 사용량, 커스텀 메트릭 등)에 따라 자동으로 Pod 개수를 수평(horizontal) 으로 늘리거나 줄여 주는 컨트롤러입니다.작동 Flow 특정 주기(기본 15초)마다 Metrics API(보통 metrics-server 또는 Prometheus Adapter 등)를 호출해 대상 리소스의 현재 메트릭을 가져옵니다. 현재 값과 목표(target)값을 비교합니다.이때, 아래 알고리즘을 사용하여, 필요한 Pod의 개수를 계산합니다.\\[\\text{desiredReplicas} = \\left\\lceil \\frac{\\text{currentMetricValue}}{\\text{targetMetricValue}} \\times \\text{currentReplicas} \\right\\rceil\\] 스케일 결정2에서 산출된 ‘desiredReplicas’를 minReplicas ~ maxReplicase 범위의 값으로 클램프(clamp)하여 최종 스케일 개수를 확정합니다. Scale API를 호출합니다.Deployment/ReplicaSet/StatefulSet 등 대상 리소스에 대한 scale 서브리소스를 업데이트하여, 실제 Pod 수를 조정합니다.Config파일 예시apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata: name: myapp-hpaspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: myapp minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization # 평균(cpu) 사용률 비율 (%) averageUtilization: 50 - type: Resource resource: name: memory target: type: Value # 절대 메모리 사용량(bytes) averageValue: 200Mi - type: Pods # Pod당 커스텀 메트릭 pods: metric: name: transactions_per_second target: type: AverageValue averageValue: \"100\"cloud-controller-manager(optional)AWS와 같은 특정 Cloud에 대응하는 Control Logic을 담고 있는 component입니다.처음에는 Core Controller manager(kube-controller-manager)에 포함되어 있었지만, 현재는 Cloud Provider(AWS와 같은, 이하 클라우드)의 Control logic과 Kubernetes logic이 분리되어 있습니다. 이런 ‘분리 설계’를 통해, Kubernetes와 각 Cloud Provider사이에 분리된 Feature release가 가능해졌습니다. 클라우드별 기능(로드밸런서 생성, 볼륨 프로비저닝, Route 설정 등등)을 외부 플러그인 형태로 관리하게 되었습니다.(클라우드 플러그인만 바꾸면, 다양한 환경에서 사용가능)‘cloud-controller-manager’에 포함된 Controller들 Node Controller클라우드 서비스에서 제공하는 Node(VM환경)에 대한 controller입니다.VM들이 Kubernetes의 Node로 역할하도록 합니다. Route Controller클라우드에서 제공하는 Routing설정을 통해 Pod간 통신이 가능하게 합니다. Service Controller클라우드의 LB(Load Balancer)나 IP, networking packet filtering, health checking등을 Kubernetes에 적용하는 controller입니다. Volume ControllerKubernetes의 Persistent Volume을 클라우드 서비스에서 제공하는 볼륨과 연결시켜 줍니다.(NAS와 같은 것들) Pod배치를 위한 kube-scheduler과 kube-controller-manager의 상호작용‘kube-scheduler’는 Pod을 Node에 배치하는 역할을 하고, ‘kube-controller-manager’는 Desired State를 달성하는 역할(Pod을 늘리는 역할)을 하는데, 이 과정은 어떻게 이루어 질까?Deployment반영을 위한 component간 상호작용 Flow만약, Deployment를 통해 Pod을 생성하고 있다면, Controller manager에서 Pod Object 생성Deployment → ReplicaSet → Pod 오브젝트를 생성합니다.이때 Pod은 spec.nodeName이 비어 있는 “Pending” 상태입니다 scheduler에서 Pod을 Node에 배치Pending Pod를 발견 → 노드 리스트 중 필터(Filter)·스코어(Score) → Binding 호출 (spec.nodeName 설정) Controller manager에서 후속작업 수행(optional)예: DaemonSet 컨트롤러로 데몬 배포, HPA 컨트롤러로 Replica 수 조정 등 kubelet에서 바인딩(Binding) 실행kubelet이, 바인딩된 Node에서 Pod을 실제로 실행합니다. kube-controller-manager가 “무엇을 몇 개”를 만들 것인지(Pod 등 리소스 생성·삭제)를 결정하고 API 호출을 수행하면, kube-scheduler는 “어디에” 배치할지(어떤 노드에 놓을지) 결정하여 Binding을 수행합니다.실습 AWS EKS환경에서 Control Plane은 고객이 직접 접근할 수 없는 완전 관리형(Managed) 서비스로 운영됩니다.때문에, kubectl로 직접 Control Plane을 접속하진 못하고, AWS CLI도구를 통해 Cluster의 상태를 조회합니다.kubectl와 Cluster 연결kubectl은 한번에 하나의 Cluster를 연결하여 명령어를 실행합니다.이때, ‘Context’라는 개념으로 Cluster와의 연결을 생성합니다.kubectl과 EKS Cluster 연결AWS CLI 명령어를 통해, EKS Cluster에 대한 Context를 쉽게 생성할 수 있습니다.$ AWS_PROFILE=**** aws eks update-kubeconfig --region ap-northeast-2 --name workshopdapne2-yeovAdded new context arn:aws:eks:ap-northeast-2:0000000:cluster/workshopdapne2-yeov to /Users/****/.kube/configLocal환경에 있는 Cluster목록(Context) 조회$ kubectl ctxarn:aws:eks:ap-northeast-2:0000000:cluster/workshopdapne2-yeovCluster 상태 조회$ AWS_PROFILE=**** aws eks describe-cluster --name workshopdapne2-yeov --query \"cluster.status\"\"ACTIVE\"References Componentes | kubernetes.io Kubernetes Components Cluster Architecture | kubernetes.io Cluster Architecture Admission control | kubernetes.io Admission Control in Kubernetes etcd Learning | etcd.io Learning Etcd란? | IBM etcd란 무엇인가요? | IBM Raft 알고리즘 Raft (algorithm) kube-scheduler | kubernetes.io Kubernetes Scheduler Scheduling Framework | kubernetes.io Scheduling Framework kube-controller-manager, Controllers | kubernetes.io Controllers Horizontal Pod Autoscaling | kubernetes.io Horizontal Pod Autoscaling Understanding Kubernetes Architecture | devopscube.com Understanding Kubernetes Architecture: A Comprehensive Guide" }, { "title": "Kubernetes의 철학(Philosophy) | Kubernetes Deep Dive - 1", "url": "/posts/k8s-philosophy/", "categories": "DevOps, kubernetes", "tags": "aws, kubernetes, cncf, k8s", "date": "2025-06-26 23:46:00 +0900", "snippet": "어떤 기술을 사용함에 있어, 기술에 담긴 철학(Philosophy)을 이해하는건, 그 기술을 제대로 활용하기 위해 필수적입니다.이번 Post에선, Kubernetes에 담긴 철학을 알아봄으로서, 제대로 활용하기 위한 첫 걸음을 떼고자 합니다.Kubernetes의 철학(Philosophy)Immutable(불변성)‘한 번 생성된 객체(서버나 컨테이너, ...", "content": "어떤 기술을 사용함에 있어, 기술에 담긴 철학(Philosophy)을 이해하는건, 그 기술을 제대로 활용하기 위해 필수적입니다.이번 Post에선, Kubernetes에 담긴 철학을 알아봄으로서, 제대로 활용하기 위한 첫 걸음을 떼고자 합니다.Kubernetes의 철학(Philosophy)Immutable(불변성)‘한 번 생성된 객체(서버나 컨테이너, 데이터까지..)는 변경될 수 없음’을 의미합니다.생성된 이후에는 내부 값을 수정할 수 없고, 변경이 필요하면 새로운 값을 가진 객체를 생성해야 합니다.Immutable의 중요성이 ‘Immutable’원칙은 Kubernetes의 Declarative(선언적) 모델과 Self-Healing(자가 복구)가 제대로 동작하도록 해 주며, 운영 안정성,보안,자동화,확장성 등에 대한 Kubernetes 기능의 기반이 됩니다. Improved Reliability‘Immutable infra’는 시스템과 App이 항상 알려지고(투명하다는 뜻, 변경내역을 쉽게 추적할 수 있다.), 일관된 상태를 유지하도록 보장합니다. 이를 통해 의도하지 않은 ‘변경 가능성’이 제거되어 시스템이 예측 가능해집니다. Enhanced Security‘Immutable infra’는 모든 시스템 구성요소가 투명하게 공개(개발자에게)되게 만들며, 검증되도록하여 더 높은 수준의 보안을 제공합니다. 시스템 자체가 계속 새롭게 생성되기 때문에, 더 높은 보안 수준을 만들 수 있습니다. Reduced Complexity‘Immutable infra’를 사용하면, 복잡하고 오류가 발생하기 쉬운 ‘수동 프로세스’를 관리할 필요가 없어집니다. 기존의 Instance를 수정하는 대신에, 완전히 새롭게 구성된 인스턴스를 생성하기 땜누에, 시스템 관리가 단순화 됩니다. Immutable Infrastructure는 어떤걸까? 서버는 배포후에는 수정되지 않습니다. 서버에 문제가 생기면, 종료하고, 새로 생성합니다. 서버 업데이는 새로운 서버를 배포하여 구현합니다.Immutable과 Mutable 비교App을 update할때, mutable과 immutable의 차이. | https://www.opsramp.com/guides/why-kubernetes/infrastructure-as-code/‘Mutable infra’는 reboot나 script를 통해, 동일한 instance에서 Application을 update합니다. 반면에, ‘Immutable infra’는 새로운 instance를 배포하여, update된 Application을 구동합니다. 신뢰성 vs 속도 ‘Immutable Infra’는 “항상 동일한 환경”을 보장하지만, 매번 이미지를 빌드·검증하는 과정이 들어가 배포 속도가 느려질 수 있습니다. ‘Mutable Infra’는 빠른 패치 적용이 가능하지만, 환경 불일치로 인한 예기치 않은 오류가 발생할 가능성이 높습니다. 운영 복잡도 vs 관리 유연성 ‘Immutable Infra’는 빌드 파이프라인과 아티팩트 저장소 관리가 필수라 초기 구성과 학습 곡선이 높습니다. ‘Mutable Infra’는 기존 툴체인(SSH, CM 툴 등)으로 바로 적용 가능하지만, 일관성을 유지하기 위해 복잡한 스크립팅이나 태스크 관리가 필요합니다. 롤백 용이성 vs 스토리지 비용 ‘Immutable Infra’는 이미지 레지스트리에 버전을 쌓아두어 쉽고 빠른 롤백이 가능합니다. 다만 저장소 비용이 증가할 수 있습니다. ‘Mutable Infra’는 롤백 스크립트나 백업을 따로 관리해야 하므로, 자동화되지 않으면 복구가 더디고 복잡합니다. 보안 대응 vs 운영 효율 ‘Immutable Infra’는 이미지를 새로 빌드함으로써 전체 시스템 취약점을 완전히 제거할 수 있어 보안성이 높습니다. ‘Mutable Infra’는 패치 누락·드리프트(drift) 발생 시 보안 리스크가 커지지만, 소규모 패치만 빠르게 적용할 수 있어 운영 효율은 높을 수 있습니다. Declarative(선언형)보통의 프로그래밍 언어(명령형)와는 다르게, 원하는 최종 결과를 지정하는데에 중점을 둔 디자인 방법입니다.Kubernetes에서는 사용자가 원하는 ‘최종 상태’를 정의하고, 시스템이 이를 자동으로 달성하도록 구성되어 있습니다. 상태 중심: 현재 시스템의 상태와 목표 상태(Desired State)를 비교하고, 필요한 변경만 적용합니다. 반복 가능성(멱등성): 동일한 선언형 설정 파일을 여러 번 적용해도 결과가 일관됩니다. 자동화: ‘선언형’은 결과만 기술하기 때문에, 그 과정에 이르는것은 시스템으로 자동화 되어 있습니다.Declarative의 중요성 관리 효율성 증대최종 상태만 정의하고 시스템이 자동으로 이를 유지하기 때문에, 반복적이고 복잡한 ‘설정 관리’를 단순화 시켜줍니다. 유지보수가 용이해집니다.Git을 통해 버젼관리가 가능해지면서, 추적 및 협업에 도움이 됩니다. 자동화에 적합합니다.최종 결과만 정의하고, 그 과정은 ‘시스템화’하기 때문에, 자연스럽게 자동화가 이루어집니다. apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80Self-healing(자동 복구)예기치 않은 Node(Instance)장애나 Container 충돌과 같은 문제가 생긴다면, 스스로 선언했던 상태(Desired state)로 돌아가게 합니다.‘Immutable(불변성)’을 갖춘 시스템이기 때문에, 예기치 않은 변화가 발생하면, 원래의 상태로 자동으로 돌아가게 합니다.References ‘Kubernetes’의 전신인 Google의 ‘Borg’에 관한 논문 | research.google Large-scale cluster management at Google with Borg What is Mutable vs. Immutable Infrastructure? | HashiCorp Immutable Infrastructure: Benefits, Comparisons &amp; More Imperative programming(명령형 프로그래밍)과 Declarative programming(선언형 프로그래밍) | blog.devpour.net Imperative programming(명령형 프로그래밍)과 Declarative programming(선언형 프로그래밍) Kubernetes Self-Healing | Kubernetes.io Kubernetes Self-Healing Decoding the self-healing Kubernetes: step by step | cncf.io Decoding the self-healing Kubernetes: step by step" }, { "title": "Kubernetes(Orchestration) 개론", "url": "/posts/k8s-introduction/", "categories": "DevOps, kubernetes", "tags": "aws, kubernetes, cncf, k8s, ecs, eks", "date": "2025-06-25 00:09:00 +0900", "snippet": "오늘날, IT서비스를 구성할때, Kubernetes와 같은 Orchestration 환경을 기본으로 시작하게 됩니다.여기서는, 이런 배경을 알아보고자 합니다.Kubernetes의 성장 배경Kubernetes가 오늘날 처럼 인프라 운영의 핵심으로 자리 잡기 까지, 여러 배경이 있습니다. 더 빨라진 비지니스 변화 → 잦은 배포가 발생합니다. 확장성(s...", "content": "오늘날, IT서비스를 구성할때, Kubernetes와 같은 Orchestration 환경을 기본으로 시작하게 됩니다.여기서는, 이런 배경을 알아보고자 합니다.Kubernetes의 성장 배경Kubernetes가 오늘날 처럼 인프라 운영의 핵심으로 자리 잡기 까지, 여러 배경이 있습니다. 더 빨라진 비지니스 변화 → 잦은 배포가 발생합니다. 확장성(scalability)이 더 중요해진 업계의 변화.Monolithic → MSA(Micro Service Architecture)로의 아키텍쳐 변화‘잦은 배포’가 ‘시스템 전체에 영향을 미치는 경우’를 피하고, Scale의 단위를 전략적으로 선택할 필요성이 생기면서, MSA가 시스템 Architecture의 중심이 되었습니다. 그러나, MSA는 아래와 같은 어려움이 있습니다. 각 서비스 마다, 개발, 빌드, 운영 환경을 일치시켜야 하는 문제가 있습니다. 분리된 각 서비스를 통합하여 관리하기가 어렵습니다. 예를 들면, 모니터링(Log, System Metric)하기가 어렵습니다.(각각의 서비스에 별도로 접속해야 해서 확인해야 하는 환경)Docker(Container 기술)의 탄생Docker가 탄생하면서, MSA환경의 어려운 점들을 해결해 줬습니다.Docker의 ArchitectureVM의 ArchitectureVM은 별도의 OS와 Kernel를 가지고 있어, Layer추가로 인해 컴퓨팅 비용 손실이 많습니다.반면에, Container는 Host의 OS와 Kernel을 공유하여, 컴퓨팅 비용 손실을 최소화 하였고,Container를 경량화 하였습니다.Public Cloud의 발전기존의 서버는 임대 계약을 하고, 물리 장치(하드웨어)를 할당 받아야 합니다.Public Cloud(AWS, GCP, Azure..)가 등장하면서, 서버 자원을 더 빠르게 확장하고, 사용한 만큼 비용을 지불할 수 있게 되면서, Scaling에 대한 장벽이 낮아지게 되었습니다.Orchestration(Kubernetes)기술의 등장Container(Docker와 같은) 기술이 MSA와 결합하면서, 어려웠던 ‘관리의 문제’를 해결해줄 ‘Orchestration System’이 만들어지게 되었습니다.그 대표격인 Kubernetes는, 처음에는 Google 내부에서 사용하던 System이었습니다. Q: 왜 Orchestration이라 부르나요?A: 시스템을 구성하는 여러 서비스 사이에서 ‘조율(orchestrate)’을 하는 기술이라고 해서, ‘Orchestration’이라고 부릅니다.Cloud Native와 KubernetesCloud Native는 애플리케이션을 클라우드 환경에서 최적화하여 개발·운영하는 방식을 지칭합니다.전통적인 모놀리식(monolithic) 애플리케이션과 달리, Cloud Native 애플리케이션은 마이크로서비스(MSA), 컨테이너, 동적 오케스트레이션, 선언적 선언(declarative) 방식을 활용하여 유연성과 확장성, 복원력(resilience)을 확보합니다.이러한 패러다임은 클라우드 인프라(퍼블릭·프라이빗·하이브리드)에 구애받지 않고, 자동화와 관찰성(observability)을 기반으로 빠른 배포·변경이 가능하도록 돕습니다.CNCF(Cloud Native Computing Foundation)CNCF는 이러한 Cloud Native 패러다임을 정의·보급하고, 관련 오픈소스 생태계를 조성하기 위해 설립된 조직입니다.Kubernetes, Prometheus, Envoy, Fluentd 등 다양한 프로젝트를 호스팅하며, 벤더 중립적인 환경에서 Cloud Native 기술 표준과 모범 사례를 확산합니다.CNCF를 졸업(Graduated)한 프로젝트들Cloud Native와 Kubernetes의 관계 Kubernetes는 Cloud Native 패러다임을 구현하기 위한 핵심 기술입니다. CNCF의 Seed 프로젝트 였습니다.Kubernetes 구성지휘자(Orchestrator) 역할을 하는 ‘Control Plane’과 구성원 역할을 하는 ‘Data Plane’으로 나뉩니다.Kubernetes cluster components. (from https://kubernetes.io/docs/concepts/architecture/)Control Plane etcdKubernetes의 현재상태와 Desire State등을 저장하는 key-value Storage서비스 입니다.Cluster의 핵심적인 역할을 하기 때문에, HA(High-availability)를 위한 장치들이 되어 있습니다. kube-api-serverKubernetes를 control하는 API를 제공하며, Cluster 규모에 따라 scale-out이 가능한 구조로 되어 있습니다. kube-schedulerPod(Cluster안에서 다루는 최소 단위, Container의 집합)을 Data Plane에 할당하고 관리하는 역할을 합니다.‘kube-api-server’를 통해, ‘명령’을 입력 받으면, ‘kube-scheduler’를 통해 실제 Pod에 대한 control이 이루어 집니다. Data Plane실제 서비스가 위치하는, Node(Host Machine. ex: AWS EC2, VM, Bare metal…)로 구성되어 있습니다.각 Node안에는, Control Plane과 통신하는 서비스가 구동되게 됩니다. kubelet‘Control Plane’의 ‘kube-api-server’의 명령을 받아서, 각 Node에서 실제로 명령을 실행하는 역할을 합니다. kube-proxyKubernetes로 들어오는 network packet들을 핸들링합니다.주로, Node내의 Routing정책(iptable을 통해)을 관리하며, 이를 통해 Node내 Packet의 이동을 관장합니다. Kubernetes의 핵심 철학, Desired StateDesired State는 사용자가 Kubernetes에 정의한 ‘궁극적으로 도달해야하는 애플리케이션의 상태’를 의미합니다.Kubernetes는 ‘Desired State’와 ‘Actual State(현재 상태)’를 지속적으로 비교하고, 두 State(상태)가 일치하도록 조정하게 됩니다.AWS EKS와 AWS ECSAWS EKS ‘Control Plane’이 AWS 자체에서 운영하는 VPC에서 관리됩니다. 이 때문에, Control Plane 관리에 대한 추가 비용이 부과되며, old version인 경우 추가 비용을 부담해야 합니다. Kubernetes와 호환성이 있습니다.(Vanilla Kubernetes기반) 즉, CNCF에 포함된 모든 서비스들을 바로 적용할 수 있습니다AWS EKS Cluster의 ArchitectureAWS ECSAWS에서 자체 개발하고 제공하는 Orchestration 서비스입니다.‘Control Plane’으로 인한 별도의 비용이 발생하지 않습니다.References A Brief History of the Cloud events.static.linuxfound.org Docker와 VM의 차이 Docker 및 VM 비교 - 애플리케이션 배포 기술 간의 차이 - AWS Top Questions for Getting Started with Docker Top Questions for Getting Started with Docker | Docker What is Kernel? 리눅스 커널(Linux kernel)이란 - 개념, 구성요소, 인터페이스 CNCF Charter CNCF Charter CNCF Landscape CNCF의 오픈소스 프로젝트를 볼 수 있는 페이지 입니다. CNCF Landscape Kubernetes Concepts Overview Kubernetes Cluster Architecture Cluster Architecture EKS Blueprints Bootstrapping clusters with EKS Blueprints | Amazon Web Services" }, { "title": "What is 'RFC 1918' about 'Private IP'?", "url": "/posts/What-is-RFC-1918/", "categories": "Programming, networking", "tags": "aws, nat, ip, network, rfc", "date": "2025-06-24 15:33:00 +0900", "snippet": "IPv4는 전세계 네트워크의 근간입니다.하지만, 이 IPv4의 데이터 대역은, 다른 데이터(int등)와 같이, 표현하기 위한 데이터 ‘갯수’에 제한이 있습니다.Private IP(사설 IP)의 필요성이 제한을 극복하기 위해, ‘Private IP’개념을 사용하게 됩니다.‘Private IP’는 사설망(Private Network, 내부망이라고도 합니다...", "content": "IPv4는 전세계 네트워크의 근간입니다.하지만, 이 IPv4의 데이터 대역은, 다른 데이터(int등)와 같이, 표현하기 위한 데이터 ‘갯수’에 제한이 있습니다.Private IP(사설 IP)의 필요성이 제한을 극복하기 위해, ‘Private IP’개념을 사용하게 됩니다.‘Private IP’는 사설망(Private Network, 내부망이라고도 합니다)에서 사용하는 IP주소를 의미합니다. Public IP대역의 IP갯수에 제한이 있다보니, 사설망에서는 별도의 IP를 사용합니다. 네트워크를 사용하는 Device(장치)들이 급격하게 늘어나면서, Public IP로 모두 대응하기에는 어려워 지면서, Private IP를 별도로 사용하게 되었습니다.NAT(Network Address Translator)를 통해 Public Net ←→ Private Net 통신하기이 Private IP를 사용하는 Device들도, 외부 Network와의 통신이 필요하곤 합니다. 이럴때, Private IP를 사용한다면, Public대역에서는 알지 못하는 IP이기 때문에, 통신이 불가하게 됩니다.이때 사용하는것이 NAT입니다.NAT에는 Public Network에서 사용하는 Public IP가 할당되며, 이를 이용해 Private IP가 Public IP로 치환되어 외부로 전송됩니다.NAT의 역할을 보여주는 이미지. 출처: https://en.wikipedia.org/wiki/Network_address_translationNAT가 단순히 IP만 치환한다면, Private Network에 있는 여러 Device를 구분하지 못하게 됩니다.이를 위해, NAT가 외부에 노출하는 ‘Port번호’를 Private IP와 Mapping하여, Private Network에 있는 device가 외부와 Packet을 주고 받을 수 있게 됩니다. NAT에서 사용하는 외부 Port는 ‘포트 주소 변환(PAT, Port Address Translation)’을 통해 동적으로 할당됩니다. 외부에서 Private Network에 있는 device에 접속하려면, 별도의 포트 포워딩(Port forwarding)혹은 DNAT(Destination NAT)이 필요합니다.NAT 동작의 핵심이 되는 “Translation Table”과 그 작동방식을 보여주는 이미지. 출처: https://en.wikipedia.org/wiki/Network_address_translationPrivate IP에 대한 RFC 1918표준 그렇다면, 왜 ‘RFC 1918’로, 별도의 IP대역이 Private 대역으로 할당되어 있을까? Private Network가 외부(주로 인터넷)과 연결될때, 해당 Private IP대역과 동일한 Public IP 대역과 충돌할 가능성이 있기 때문입니다. 때문에, 별도의 IP대역을 분리하여, Public IP대역과 구분해서 사용하게 되었습니다. 이러한 주소는 ISP(Internet Service Provider, 인터넷 회선 제공자)나 IANA(Internet Assigned Numbers Authority, 최상위 도메인 관리자)등에서 별도 승인 없이 누구나 내부 네트워크에 자유롭게 사용할 수 있도록 예약된 범위입니다. 공인 인터넷망에서는 이 주소들이 라우팅되지 않도록 관례상 필터링됩니다.RFC 1918로 할당된 Private IP 대역은 다음과 같습니다.10.0.0.0 - 10.255.255.255 (10/8 prefix)172.16.0.0 - 172.31.255.255 (172.16/12 prefix)192.168.0.0 - 192.168.255.255 (192.168/16 prefix)적용AWS VPCAWS에서 VPC의 내부망의 IP는 RFC 1918를 따르도록 권장됩니다.AWS ALBAWS의 ALB에서 Listener로 사용가능한 IP대역은, RFC 1918를 만족해야 합니다.즉, RFC 1918에 대응되는 Private IP대역만 사용할 수 있습니다.References RFC 1918 RFC 1918: Address Allocation for Private Internets What Is Network Address Translation (NAT)? [from Cisco] What Is Network Address Translation (NAT)? NAT (wikipedia) NAT (wikipedia)" }, { "title": "AWS Summit Seoul 2025 Insight", "url": "/posts/AWS-SUMMIT-SEOUL-2025/", "categories": "AWS, Summit", "tags": "aws, conference, aws summit", "date": "2025-06-14 12:12:00 +0900", "snippet": "들어가면서이번 AWS Summit은 생성형 AI를 중심으로, 데이터 엔지니링, 운영, AI모델 활용 측면의 주제가 많았습니다.생성형 AI(이하 Gen AI)가 업계의 새로운 광산(금을 케는)이 되었고, 이에 따라 기술적 전략을 수립하기 위한 Insight를 얻기에 좋은 자리였습니다.본문업계의 트렌드가 보편적인 목적의 AI모델(GPT와 같은 Founda...", "content": "들어가면서이번 AWS Summit은 생성형 AI를 중심으로, 데이터 엔지니링, 운영, AI모델 활용 측면의 주제가 많았습니다.생성형 AI(이하 Gen AI)가 업계의 새로운 광산(금을 케는)이 되었고, 이에 따라 기술적 전략을 수립하기 위한 Insight를 얻기에 좋은 자리였습니다.본문업계의 트렌드가 보편적인 목적의 AI모델(GPT와 같은 Foundation Model)을 활용하여, 빠르게 서비스를 구축하여 제공하는것으로 이미 진행되고 있습니다. 이에 따라, AI모델에 대한 사용방법과, Custom을 위한 데이터 수집 관점의 세션을 주로 참석하였습니다.생성형 AI(Generative AI, 이하 Gen AI)‘Amazon’ 서비스에서는, 모든 서버 요청에 대해, latency(혹은, 서버 응답시간)에 대해 ‘p99 1 sec 미만’을 유지하는걸 목표로 하고 있습니다. latency에 대한 p99 값은, 전체 요청을 모집단(Population)으로 했을때, 99%에 위치하는 latency를 의미합니다.즉, 여기서는 99%에 위치한 유저의 지연시간이 1초 미만임을 의미합니다.이는, Gen AI에 대해서도 마찬가지인데, 실제로 Amazon에서 쇼핑과 관련한 GenAI에 대한 시연도 1 sec 정도의 latency를 보여주었습니다.(엄청 빨랐습니다)‘Amazon’은 모두 AWS서비스를 통해 구성하기 때문에, AWS에서 제공하는 AI서비스들을 알 수 있었습니다.Unified StudioAI를 개발하는 과정의 통합환경.AI를 개발할 때에는, 여러 데이터와 서비스들에 대한 ‘접근’이 필요한데, 이를 통합된 환경으로 제공해주는 서비스입니다.Unified Studio에서 사용 가능한, AI 작업의 목적에 따른 AWS서비스 분류. 각 서비스별로 어떤 역할을 하는지 쉽게 파악할 수 있습니다.Amazon BedrockAWS의 AI서비스중, Bedrock은 여러 Foundation Model(Anthropic, Deepseek, Amazon Nova등)을 기반으로 빠르게 생성형 AI를 구축하게 해주는 Serverless Service입니다. ‘Foundation Model’은 보편적인 목적으로 미리 학습된 모델을 의미합니다.이런 개념이 생기기 전에는, 각 목적(이미지 편집, 목소리 모사)에 맞는 모델을 직접 만들어야 했는데, 이제는 보편적인 목적으로 만들어진 ‘Foundation Model’을 시작점으로 하여, Custom하는 방식으로 접근합니다.Amazon Bedrock의 여러 기능들 모델 증류(Model distillation)란?더 큰 모델(파라미터가 더 많은)을 가지고, 더 작은 모델을 지도학습 시켜서, 작은 모델의 퀄리티를 높이는 테크닉입니다.이렇게 하는 이유는, AI가 발전함에 따라, 더 적은 비용, 더 빠른 서비스를 제공하기 위함입니다.뿐만아니라, ‘On device AI’(각자의 핸드폰에서 AI를 돌리는것으로 생각하시면 됩니다)에 대한 요구사항을 위해 사용됩니다.OpenAI Platform딥러닝 모델 지식의 증류기법, Knowledge DistillationBedrock에서는 여러 추론(Inference) 옵션(사용 요금 정책 개념)이 있습니다.각각 ‘On demand(사용한 만큼)’, ‘Provisioned(사용량 예약)’, ‘Batch(비실시간, 예약처리)’가 있습니다.Amazon NovaBedrock에서, Amazon이 만든 GPT Model인 ‘Nova’를 사용할 수 있습니다.Amazon Nova 모델 종류. 한글도 지원합니다. 하지만, 한글에서의 성능은 검증이 좀 필요합니다.모델 소개 페이지Nova 모델 소개Amazon Bedrock GuardrailsGen AI를 쓸때, 민감한 정보가 ‘권한이 없는 유저에게 전달되는것’에 대한 우려가 있는데, 이를 위한 보안 기능입니다.Amazon Q Business사내 데이터를 연결하여, 빠르게 사내용 AI Agent를 만들게 해줍니다.한글 기반으로, 자체적으로 Query를 생성하여, 적절한 DB에 요청을 보내주기도 합니다.Amazon Q Business 정식 버전 출시 – 생성형 AI 기반 업무 생산성 향상 지원 기능 추가 | Amazon Web ServicesGen AI와 RAGGen AI의 치명적인 단점중 하나가, 환각(hallucination, 속이는것)입니다. 즉, 없는 데이터를 그럴듯하게 만들어내는 문제가 있습니다. 이를 해결하기 위한 Solution으로 사용되는것이 RAG(Retrieval, Augmented, Generation)입니다. RAG는 결국, GenAI가 참조할 수 있는 정보를 추가로 제공해주는 개념입니다.이 RAG를 제공하기 위해, Gen AI에서 접근 가능한 DB에, Gen AI가 이해할 수 있는 형태(Vectorized)의 데이터가 준비되어 있어야 합니다.이는, ‘데이터 저장공간’에 대한 준비 뿐만 아니라, ‘데이터 전처리’도 필요하다는 것을 의미합니다.사례, ‘ENVERUS’Foundation Model을 기반으로, Custom 목적의 데이터 수집을 어떤 Flow로 하는지 알 수 있었습니다.ENVERUS의 데이터 처리 Flow이를 Pipeline Architecture로 세분화 하여 표현하면, 아래와 같습니다.ENVERUS의 데이터 수집 Pipeline ArchitectureRAG를 구성하는 System Architecture데이터 저장 Workflow데이터를 수집하고, Gen AI가 이해할 수 있는 데이터(Embedding Vector)로 만드는 flow입니다.data 저장 workflow 데이터 수집 텍스트 추출Gen AI에서 ‘정보’로 판단할 데이터들을 추출 합니다. 데이터 청킹(쪼개기)추출된 데이터 전부를 Gen AI가 한번에 받아들일 순 없기때문에, 특정 기준(띄어쓰기 혹은 글자 수, 맥락 등의 기준)으로 조각냅니다. 임베딩(embedding)데이터 청크(chunk, 조각) 마다 ‘GenAI가 이해할 수 있는 데이터(Embedding vector, 임베딩 벡터)’로 만듭니다.Embedding vector가 만들어지는 과정 Embedding을 생성할 수 있는 모델 option들 데이터 조각(Chunk)을 받아서, 이를 일관된 기준에 따라, 수치화(Vectorize, embedding vector로 만듬)해야 합니다. 이 역할을 하는것이 임베딩 모델(embedding model)입니다. 임베딩 모델 바이너리 임베딩기존 임베딩 모델과 같이 0~1사이의 실수(float)값을 갖는게 아닌, ‘0, 1’ 단 2개의 값 만으로 embedding을 만듭니다. 이는 성능과 비용 개선 효과가 있지만, 정확도가 많이 차이납니다. 벡터 스토어생성된 데이터를 전문으로 다루는 벡터스토어에 저장합니다.이로서, Prompt에 대해서, ‘벡터 유사도(vector similarity)’를 활용하여, 관련된 데이터, 관련 없는 데이터 구분이 가능해집니다.Vector Store로 가능한 옵션들Gen AI와 MCP(Model Context Protocol)Gen AI를 좀 더 광범위 하게 활용하기 위해서, 여러 서비스(인터넷, DB등)를 연결시켜 사용하게 됩니다.이때, Agent와 다른 서비스들을 연결하여, Agent로 발전시키는 ‘핵심 Protocol 역할’을 하는것이 MCP입니다. RAG와 MCP는 다릅니다.RAG는 Gen AI의 데이터를 확장주는 ‘전략’이고, MCP는 Gen AI가 특정행위를 하게 만드는 프로토콜(protocol, 약속)입니다. 때문에, MCP는 표준화된 데이터 구조(JSON형식)도 포함하고 있습니다.Amazon Bedrock를 통해, MCP를 사용하는 사례를 보여주었는데, 특별한건 없었습니다.(스폰서 세션)우리에게 맞는 AI 서비스 Build하기이미 업계에서는, Base Model(Foundation model)을 기반으로 Custom을 통해 AI서비스를 만드는 전략이 대세가 되고 있습니다.여러 서비스들이 Base모델은 같은걸 사용하고 있고, 결국, 모델간 차이를 만드는건 ‘Data’임을 강조하고 있습니다. Gen AI는 결과물에 해당한다. 이는 빙산의 일각일 뿐이다.Gen AI라는 결과물 보다, 데이터 파운데이션(데이터 저장소와 같은)것이 더 중요하다.(빠르게 만들어낸 결과물만 보지 말라는 뜻)- From AWS Summit 2025데이터로 커스터마이징 하는 방법들을 아래와 같이 제시해주었습니다. RAG Gen AI에게 추가 데이터 Source를 제공해준다. 파인튜닝(fine-tuning) Base model에 데이터를 입력시켜, 파라미터를 업데이트(모델 자체를 일부 수정하는것) 지속적인 사전학습 파운데이션 모델을 직접 만든다 추천받은 자료들 Amazon Bedrock Knowledge base로 30분 만에 멀티모달 RAG 챗봇 구축하기 실전 가이드Amazon Bedrock Knowledge base로 30분 만에 멀티모달 RAG 챗봇 구축하기 실전 가이드 | Amazon Web Services번외. Multi Modal와 AGI‘Multi Modal’은 text뿐만 아니라, 여러 데이터 Source를 통해 맥락(context)을 파악하는 model입니다.Multi modal 개념Multi modal 설명 from MetaLLM(ChatGPT와 같은)은 Text 기반의 Model이라면, Multi-modal은 여러 데이터 형태를 받을 수 있는 model을 말합니다.AGI(일반 인공지능, 인간을 대신할 수 있는 지능)에 도달하기 위해, Multi-modal을 고도화 하는방향으로 진행되고 있습니다.분석(Analytics), 데이터 수집도움이 되는 배경정보데이터의 중요성과 데이터 갯수의 급격한 증가로 인해, 기존에 사용하던 형태의 DB보다, 분석에 특화된 DB가 필요해집니다.OLAP와 OLTPDB 시스템을 데이터 처리 방식에 따라, 2가지의 대분류로 분류합니다. OLTP(online transaction processing) 기존에 사용하던 형태의 DB, Transaction을 다룸 OLAP(online analytical processing) 대량의 데이터 분석에 특화된 기능을 갖춘 구조, 기능을 갖고 있습니다OLTP와 OLAP 비교 - 데이터 처리 시스템 간의 차이점 - AWS 데이터 레이크(Data Lake)의 필요성데이터의 중요성이 높아지면서, 데이터 Schema를 정하는 과정 없이, 원본데이터 그대로(비정형 데이터)를 저장하고자 하는 요구가 생기게 됩니다. 데이터의 Schema를 정하는 과정에서, 데이터 수집전에 데이터의 형태를 정해버리면서, 데이터의 한계에 부딪히게 되었습니다. 데이터 레이크가 필요한 이유는 무엇입니까?데이터에서 비즈니스 가치를 성공적으로 창출하는 조직은 경쟁자를 크게 능가할 것입니다. 451 Research 설문조사에 따르면 설문조사에 참여한 기업 중 절반 이상이 현재 데이터 레이크를 구현했으며, 다른 22%는 36개월 이내에 데이터 레이크를 구축할 계획이라고 답했습니다. 데이터 레이크를 비롯한 최신 데이터 아키텍처를 구현하는 회사는 운영 효율성과 매출 증대에서 측정 가능한 이점을 입증했습니다. 이러한 리더들은 실시간 스트림, IoT 센서, 소셜 미디어 및 고객 상호 작용 데이터를 비롯한 다양한 데이터 소스에서 고급 분석, 인공 지능 및 대규모 언어 모델을 사용합니다. 이 포괄적인 데이터 전략을 통해 데이터 기반 의사 결정을 더 빠르게 내리고, 고객 경험을 개인화하고, 예측 유지 보수를 통해 운영을 최적화하고, 경쟁사보다 먼저 새로운 매출 기회를 식별할 수 있습니다.AWS document(https://aws.amazon.com/ko/what-is/data-lake/)데이터 레이크란? - 데이터 레이크 및 분석 소개 - AWSOpen Table Format의 필요성이런 ‘비정형 데이터’를 관리하는 시스템이 필요해지면서, 별도의 ‘표준’이 생기게 됩니다. 이를 ‘Open Table Format’을 통해, 데이터들에 대한 ‘관리 데이터’를 생성하고 갱신하고, 더 빠르 조회가 가능하게 해주는 시스템이 만들어집니다.‘Apache Iceberg’, ‘Apache Hudi’, ‘Delta Lake’가 이에 해당합니다.AWS에서의 Transactional Data Lake를 위한 오픈 테이블 형식(Open table format) 선택 가이드 | Amazon Web ServicesApache Iceberg 그중, 이 컨퍼런스에선 ‘Apache Iceberg’를 추천하고 있었습니다. ‘Apache Iceberg’는 transaction 지원. 데이터를 버젼관리 하면서, ‘time travel(특정 시간대를 기준으로 SQL을 할 수 있는 기능)’기능을 제공합니다.AWS에서는 S3를 Storage로 사용하고, AWS Athena를 통해 관리하게 됩니다.이는 실제로 Query를 실행할때만 비용을 지불하여 비용최적화 됩니다. 파일 저장 비용(S3)는 별도입니다.AWS 분석 서비스에서 Apache Iceberg 활용하기 | Amazon Web Services" }, { "title": "Terraform의 State와 Flow 이해하기", "url": "/posts/Understand-Terraform-state/", "categories": "DevOps, Terraform", "tags": "aws, terraform, iac", "date": "2025-04-05 23:12:00 +0900", "snippet": "들어가면서이 post에선, Terraform의 핵심인 ‘State’에 대해서 알아봅니다.본문Terraform의 State에 대한 이해Terraform은 리소스의 현재 상태를 저장하고 추적하기 위해 State 파일을 사용합니다.State 파일에는 Terraform이 관리하는 인프라 리소스의 실제 상태가 JSON 형태로 저장됩니다. Terraform은 ...", "content": "들어가면서이 post에선, Terraform의 핵심인 ‘State’에 대해서 알아봅니다.본문Terraform의 State에 대한 이해Terraform은 리소스의 현재 상태를 저장하고 추적하기 위해 State 파일을 사용합니다.State 파일에는 Terraform이 관리하는 인프라 리소스의 실제 상태가 JSON 형태로 저장됩니다. Terraform은 이 State 파일을 사용해서 무엇을 변경해야 할지 어떤 리소스가 현재 존재하는지 어떤 값(output 등)을 다른 리소스에 넘겨야 할지등을 결정합니다.즉, State는 Terraform의 ‘source of truth(진실의 원천)’이 됩니다. Terraform에서 State는 *.tfstate라는 확장자를 가진 파일로 관리됩니다.State의 설계 개념Hashicorp에서 State에 대해서 설명하는 영상에 따르면, State는 크게 다음과 같은 상황을 가정하여 접근하였다고 합니다. ‘Infra’를 최초로 배포하는 ‘Day 1’상황 이미 운영되고 있는 ‘Infra’에서 무언가를 추가하는 ‘Day 2+’상황이 상황을 대응하기 위해, Infra를 Code로 표현하는(여기선 TF Config파일) ‘Infrastructure as Code’ 방식으로 접근하게 되었습니다.덕분에, Infra의 현재상태를 알 수 있었고, Infra의 ‘현재 상태(Current State)’와 ‘도달하고자 하는 상태(Desired State)’라는 개념이 구현될 수 있었습니다.State의 저장소‘State’는 Infra의 ’현재 상태(Current State)’를 의미함로서, 중요하게 다루어져야 합니다.Terraform에서는 이 ‘State’파일을 다음과 같은 저장소에 저장할 수 있습니다. 로컬 파일 (terraform.tfstate)기본적으로 현재 작업 디렉터리에 저장됩니다. 원격 저장소 (Remote Backend)실무에서는 AWS S3, GCS, Terraform Cloud 같은 곳에 저장해서 공유하고 버전 관리합니다. Infra 업데이드에 대한 동시성 해결을 위해, Lock파일을 포함하여 Remote Backend를 사용하는것을 추천합니다. State LockingTerraform의 ‘State Locking’은 State파일에 대한 동시성(concurrency)을 제어하기 위해 ‘Lock’을 사용하는 것을 말합니다.State파일을 원격(remote) 저장소에 저장하게 되면, 동시에 여러 작업자가 수정을 시도할 수 있는데, 이는 State불일치 상황인 ‘State Drift’를 초래할 수 있습니다.이를 방지하기 위해 오직 1개의 요청만 처리되도록, ‘locking’을 사용합니다.‘State Locking’은 다음과 같은 과정으로 작동됩니다.Terraform은 plan, apply 같은 명령을 실행할 때, Lock 요청을 보낸다. Lock 생성 작업 완료 후 Unlock이때, Lock을 못 걸게되면(수정 권한을 못 얻게 되면), 다음과 같은 메세지가 뜹니다.Error locking state: Error acquiring the state lock이 ‘Lock’은 설정한 Terraform Backend에 생성되며, Terraform v1.11 이후에는, AWS S3만을 이용해서 Lock기능을 사용할 수 있다고 합니다. 기존에는 DynamoDB를 활용해야 했다고 합니다.Terraform State와 Infra가 불일치한 상태(drift)일때 대처하는 법Terraform에서, ‘Infra의 현재상태를 나타내는 State와 실제 Infra가 불일치하는것’을 State Drift 라고 합니다.즉, *.tfstate 파일에 저장된 내용과 실제 Infra상태가 다른 상태를 의미합니다.이 ‘drift’상태에서 Terraform의 각 CLI명령어에서는 다음과 같은 모습을 보입니다. terraform planPlan을 실행하면, Terraform이 실제 인프라 상태(Real State)를 가져와서 State와 비교합니다.차이가 감지되면, Plan 결과에서 다음과 같이 표시합니다. # aws_instance.example will be updated in-place ~ resource \"aws_instance\" \"example\" { instance_type = \"t2.micro\" -&gt; \"t2.small\" } terraform applyPlan 결과를 보고 apply를 하면, 원래 코드에 맞춰서 인프라를 자동으로 되돌립니다.즉, Terraform은 “코드(.tf)에 정의된 상태”를 진실로 보고, 실제 인프라를 강제로 맞추려고 합니다이 Drift를 해결하기 위해, 다음 기능을 사용할 수 있습니다. terraform refresh (v0.15 이하)현재 인프라 상태를 State에 반영합니다. terraform plan -detailed-exitcodeDrift가 발생하면 Plan 명령어가 다른 exit code(2)를 반환합니다. CI에서 유용하게 사용합니다. terraform state rmState에서 리소스를 강제로 삭제합니다 $ terraform state rm 'packet_device.worker' terraform import기존의 리소스를 Terraform 관리 대상으로 끌어옵니다.최근에는 Terraform Cloud나 Drift Detection 기능을 통해 자동으로 감지할 수 있습니다.Terraform State 관련 CLI 명령어$ terraform show # State 파일 안의 내용 보기$ terraform state list # 현재 관리 중인 리소스 목록$ terraform state show &lt;resource&gt; # 특정 리소스 세부 정보 보기$ terraform state mv # 리소스 이름 이동/변경$ terraform state rm # 리소스를 State에서 삭제 (실제 인프라는 안 지워짐)Terraform을 이용한 workflow기본 workflow기본 Terraform Work FlowTerraform 작업의 일반적인 흐름은 다음과 같습니다. 코드 작성 (Write)*.tf 파일에 인프라 코드 작성 초기화 (Initialize)terraform init필요한 Provider 플러그인 설치 (AWS, GCP 등)백엔드 설정 (State 저장 위치 준비) 검토 (Plan)terraform plan현재 인프라 상태와 코드 비교변경사항 미리 보기“무엇이 추가/수정/삭제될지” 안전하게 검토 적용 (Apply)terraform apply실제로 리소스 생성, 수정, 삭제Plan 결과를 보고 적용할지 물어봄 (yes 입력) 검증 및 관리 (Manage)만든 리소스를 terraform show, terraform state list 로 확인리소스 수정 필요하면 코드 수정 후 다시 Plan → Apply 반복 정리 (Destroy)terraform destroy모든 인프라 리소스 삭제 (필요 시) 팀단위의 Terraform Work Flow(Best Practice)팀 단위로는, GitOps를 이용해서 ‘Git + PR리뷰 + Atlantis + Terraform Remote Backend’ stack을 사용합니다. Git 브랜치 생성 .tf 파일 코드 작성 및 로컬 테스트terraform initterraform plan PR(Pull Request) 생성GitHub, GitLab 등에 PR 올리고, terraform plan 결과도 공유합니다(CI를 통해 자동화하는 경우 많습니다 ). Atlantis가 terraform plan 수행Atlantis가 PR 이벤트를 감지해서, 자동으로 terraform plan 명령을 실행하고, 결과를 PR 코멘트에 붙여줍니다. 리뷰 및 승인다른 팀원이 코드/플랜 결과를 검토하고 승인 승인 후 PR Comment를 통해 atlantis apply 를 실행합니다.‘atlantis’는 PR 코멘트를 감지해서, terraform apply를 실행합니다. Atlantis가 terraform apply 수행합니다.실행 결과가, 다음 예시와 같은 PR Comment로 저장됩니다. ### Atlantis Apply Apply complete! Resources: 1 added, 0 changed, 0 destroyed. PR MergeReferences What is Terraform? What is Terraform | Terraform | HashiCorp Developer Introduction to HashiCorp Terraform with Armon Dadgar Terraform State Locking State: Locking | Terraform | HashiCorp Developer" }, { "title": "Serverless Architecture with Terraform - REST API", "url": "/posts/Serverless-Architecture-with-Terraform-REST-API/", "categories": "DevOps, Terraform", "tags": "aws, terraform, serverless, iac, lambda", "date": "2025-04-01 11:33:00 +0900", "snippet": "들어가면서회사의 비지니스적인 상황에 따라서, Serverless Architecture(AWS Lambda + API gateway)를 사용할때가 있습니다. Serverless 환경에서 개발할때에 여러 어려움중에 하나가, 개발 Cycle에 대한 불편함인데,Local에서 HTTP server를 구동하는게 어려워서, AWS Lambda에 배포하여 테스트...", "content": "들어가면서회사의 비지니스적인 상황에 따라서, Serverless Architecture(AWS Lambda + API gateway)를 사용할때가 있습니다. Serverless 환경에서 개발할때에 여러 어려움중에 하나가, 개발 Cycle에 대한 불편함인데,Local에서 HTTP server를 구동하는게 어려워서, AWS Lambda에 배포하여 테스트하는 경우가 많다는 겁니다.역시, 이런 어려움은 저만 겪는게 아니라, 이미 Solution이 있습니다. 바로 ‘Serverless Framework’입니다.Serverless Framework의 아쉬운점‘Serverless Framework’는 ‘Serverless-Offline’모듈을 통해, Local환경에서 Lambda기반의 HTTP Server를 가상화해서 활용할 수 있습니다.또한, ‘Serverless Framework‘로 자동화된 배포까지 구현할 수 있습니다.즉, ‘개발환경’과 ‘배포환경’을 하나의 Framework로 다룰 수 있습니다.‘Serverless Framework’는 AWS의 CloudFormation 기반으로 작동합니다. 때문에, Cloudformation기반으로 인프라의 State를 자동으로 맞춰주고, Rollback까지 손쉽게 진행할 수 있습니다.하지만, 많은 회사에서 이미 ‘Terraform’을 IaC(Infrastructure as Code)도구로 사용할텐데, 별도의 IaC Stack을 추가하는건 피하게 됩니다.때문에, ‘Terraform을 이용해서, Serverless framework와 같이 구현할 수 있지 않을까?’라는 생각으로 이 포스팅을 진행하게 되었습니다.본문먼저 작업을 완료한 Repository를 공유합니다. 이 포스팅과 함께 내용을 따라오시면, 이해에 도움이 되실겁니다.terraform-aws-examples/examples/lambda-base/apigateway at main · KanghoonYi/terraform-aws-examples(Terraform 기반의 serverless architecture example)접근 방법 및 과정Terraform자체는 IaC도구로서, Application Code를 handling하는 기능은 없습니다. 때문에, 다음과 같은 기능을 직접 구현해야 합니다. Source Code를 Lambda용으로 build (외부모듈 dependency를 포함하여 zip파일로 생성) Source Code기반으로 HTTP server를 Local환경에 가상화이를 통해, ‘AWS Lambda’를 생성하는 과정을 자동화 하게 됩니다.이후, API Gateway에 연결하기 위해 이 설정에 ‘API Gateway’에 대한 dependency를 설정하게 됩니다.이렇게 해서 완료된 환경을 AWS의 Guide에 따라, ‘AWS SAM’으로 Local에서 HTTP server로 가상화하여 구동하게 됩니다.Terraform의 Module기능 활용Terraform환경에서 Lambda를 쉽게 활용하기 위한 별도의 Module이 준비되어 있지만, 이 Module은 여전히 API Gateway와의 Dependency나 Execution Role과 같이 설정할 것들이 많습니다.이를 해소하고자, terraform-aws-modules/lambda/aws 모듈 기반의 Custom 모듈을 정의해서 사용합니다.terraform-aws-examples/examples/lambda-base/apigateway/tfModules/APIGatewayHandler at main · KanghoonYi/terraform-aws-examplesSource Code Buildhttps://github.com/KanghoonYi/terraform-aws-examples/blob/main/examples/lambda-base/apigateway/package.json# examples/lambda-base/apigateway/package.json{ \"name\": \"apigateway\", \"version\": \"1.0.0\", \"type\": \"module\", \"scripts\": { \"build:dev\": \"NODE_ENV=dev tsx esbuild.ts &amp;&amp; (cd .esbuild &amp;&amp; npm install --omit=dev)\", \"build:prod\": \"NODE_ENV=production tsx esbuild.ts &amp;&amp; (cd .esbuild &amp;&amp; npm install --omit=dev)\", \"local-run:dev\": \"sam local start-api --hook-name terraform --beta-features\" }, ...}https://github.com/KanghoonYi/terraform-aws-examples/blob/main/examples/lambda-base/apigateway/esbuild.ts// examples/lambda-base/apigateway/esbuild.tsimport { globSync } from 'glob';import esbuildModule from 'esbuild';import { packageJsonPlugin } from 'esbuild-plugin-package-json';import * as process from 'node:process';const entryFilePaths = globSync('src/functions/**/handler.ts', { // root: path.resolve(__dirname, 'src/functions'), nodir: true, dotRelative: true,});(async () =&gt; { const buildResult = await esbuildModule.build({ plugins: [packageJsonPlugin()], entryPoints: entryFilePaths, entryNames: '[dir]/[name]', outbase: './', outdir: '.esbuild', bundle: true, platform: 'node', tsconfig: './tsconfig.json', treeShaking: true, packages: 'external', sourcemap: true, minify: process.env.NODE_ENV === 'production', format: 'esm', }); return buildResult;})().catch((reason) =&gt; { console.error(reason); process.exit(1);});package.json 의 ‘script’를 이용하여, nodejs기반의 Lambda source code를 build하는 과정을 명시합니다.NODE_ENV=dev tsx esbuild.ts &amp;&amp; (cd .esbuild &amp;&amp; npm install --omit=dev) NODE_ENV=dev tsx esbuild.ts‘tsx’를 이용해서 source code를 build하는것을 말합니다. cd .esbuild &amp;&amp; npm install --omit=devbuild된 결과물을 기준으로, 외부 dependency를 다시 설치합니다. Souce Code build 과정을 Terraform에 표현하기여기서는 Terraform에서 Source code build 과정을 명시해주고, Local 구동이 가능하게 ‘AWS SAM Metdata’를 자동으로 생성하게 해줘야 합니다.https://github.com/KanghoonYi/terraform-aws-examples/blob/main/examples/lambda-base/apigateway/tfModules/APIGatewayHandler/main.tf# examples/lambda-base/apigateway/tfModules/APIGatewayHandler/main.tflocals { esbuild_src = \"./.esbuild\" function_name = \"${var.lambda_handler_name}-${var.stage}\" default_env = { STAGE = var.stage } combined_env = merge(local.default_env, var.environment_variables)}# Terraform 사용시 Source code를 새롭게 build하도록 설정resource \"terraform_data\" \"build_app\" { triggers_replace = { always_run = timestamp() } provisioner \"local-exec\" { command = \"rm -rf ${local.esbuild_src} &amp;&amp; export PATH=$PATH:${path.cwd}/node_modules/.bin &amp;&amp; npm run build:prod\" }}# AWS SAM을 위한 metadata 생성resource \"null_resource\" \"sam_metadata_aws_lambda_function\" { count = 1 provisioner \"local-exec\" { command = \"rm -rf ${local.esbuild_src} &amp;&amp; export PATH=$PATH:${path.cwd}/node_modules/.bin &amp;&amp; npm run build:prod\" } triggers = { # This is a way to let SAM CLI correlates between the Lambda function resource, and this metadata # resource resource_name = \"module.APIGatewayLambdaHandler.aws_lambda_function.this[0]\" resource_type = \"ZIP_LAMBDA_FUNCTION\" # The Lambda function source code. # original_source_code = jsonencode(var.source_path) original_source_code = local.esbuild_src # a property to let SAM CLI knows where to find the Lambda function source code if the provided # value for original_source_code attribute is map. # source_code_property = \"path\" # A property to let SAM CLI knows where to find the Lambda function built output built_output_path = \"${local.esbuild_src}/${module.APIGatewayLambdaHandler.local_filename == null ? \"\" : module.APIGatewayLambdaHandler.local_filename == null}\" } # SAM CLI can run terraform apply -target metadata resource, and this will apply the building # resources as well depends_on = [terraform_data.build_app, module.APIGatewayLambdaHandler]}Terraform에서 Source Code에 대한 Lambda 정의하기Terraform으로 명시한 ‘Source Code Build’ 결과물을 ZIP파일로 만들고, Lambda로 명시합니다.https://github.com/KanghoonYi/terraform-aws-examples/blob/main/examples/lambda-base/apigateway/tfModules/APIGatewayHandler/main.tfmodule \"APIGatewayLambdaHandler\" { source = \"terraform-aws-modules/lambda/aws\" architectures = [\"arm64\"] timeout = 15 function_name = local.function_name description = var.lambda_description handler = var.handler_src runtime = \"nodejs22.x\" memory_size = 256 publish = true create_function = true create_package = true create_role = false create_sam_metadata = false # 상위 과정에서 sam metadata를 수동으로 생성해야 하기 때문에, false로 세팅합니다 source_path = [ { path = local.esbuild_src commands = [ \"npm install --production\", \":zip\" # AWS Lambda에 적용하기 위해, source code를 zip파일로 압축합니다. ], patterns : [ \"node_modules/.+\", \"!node_modules/@aws-sdk/.*\", ] } ] store_on_s3 = true s3_bucket = var.source_code_bucket_name environment_variables = local.combined_env == null ? {} : local.combined_env lambda_role = var.lambda_role_arn event_source_mapping = {\t # SQS와 같이 Event mapping기능을 사용해야하는 경우에 여기에서 설정하게 됩니다. } tracing_mode = \"Active\" attach_network_policy = true attach_tracing_policy = true logging_log_group = \"/aws/lambda/${local.function_name}\" # Cloudwatch에서 사용할 Log Group이름을 설정합니다 cloudwatch_logs_retention_in_days = var.log_ttl_days depends_on = [ terraform_data.build_app, ]}# Proxy Resource (/{proxy+})resource \"aws_api_gateway_resource\" \"proxy\" { rest_api_id = var.agw_rest_api_id parent_id = var.agw_parent_resource_id path_part = var.agw_http_path}# Method (ANY)resource \"aws_api_gateway_method\" \"proxy_method\" { rest_api_id = var.agw_rest_api_id resource_id = aws_api_gateway_resource.proxy.id http_method = var.agw_http_method authorization = \"NONE\"}# Integration with Lambdaresource \"aws_api_gateway_integration\" \"lambda\" { rest_api_id = var.agw_rest_api_id resource_id = aws_api_gateway_resource.proxy.id http_method = aws_api_gateway_method.proxy_method.http_method integration_http_method = \"POST\" # API Gateway에서 Lambda를 실행할때 사용하는 method입니다. Lambda를 Invoke하는 것은 'POST'로만 가능합니다. type = \"AWS_PROXY\" uri = module.APIGatewayLambdaHandler.lambda_function_invoke_arn}# Lambda Permission for API Gateway# API Gateway에서 Lambda를 실행할 수 있는 권한을 명시합니다.resource \"aws_lambda_permission\" \"apigw\" { statement_id = \"AllowAPIGatewayInvoke\" action = \"lambda:InvokeFunction\" function_name = module.APIGatewayLambdaHandler.lambda_function_name principal = \"apigateway.amazonaws.com\" source_arn = \"${var.agw_execution_arn}/*/*\"}Local 에서 HTTP Server 실행 방법이제 AWS SAM을 이용해서, Terraform환경의 세팅을 기반으로 HTTP server를 실행해 보겠습니다. AWS SAM은 Docker기반으로 실행됩니다. 때문에, Docker Daemon이 실행중이어야 합니다. AWS SAM은 AWS CLI를 기반으로 작동하기 때문에, AWS Token관련 환경변수가 설정되어 있어야 합니다.# examples/lambda-base/apigateway/package.json{ \"name\": \"apigateway\", \"version\": \"1.0.0\", \"type\": \"module\", \"scripts\": { \"build:dev\": \"NODE_ENV=dev tsx esbuild.ts &amp;&amp; (cd .esbuild &amp;&amp; npm install --omit=dev)\", \"build:prod\": \"NODE_ENV=production tsx esbuild.ts &amp;&amp; (cd .esbuild &amp;&amp; npm install --omit=dev)\", \"local-run:dev\": \"sam local start-api --hook-name terraform --beta-features\" }, \"devDependencies\": { ... }}위의 package.json 내용중, 아래와 같은 실행 명령어를 사용합니다.$ NODE_ENV=dev tsx esbuild.ts &amp;&amp; (cd .esbuild &amp;&amp; npm install --omit=dev) ## 혹은 npm run buidl:dev$ sam local start-api --hook-name terraform --beta-features 그러면, 아래와 같은 메세지가 뜹니다.Experimental features are enabled for this session. Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/. Mounting APIGatewayGetHello-dev at http://127.0.0.1:3000/hello [GET] You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. If you used sam build before running local commands, you will need to re-run sam build for the changes to be picked up. You only need to restart SAM CLI if you update your AWS SAM template 2025-04-03 20:57:59 WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on http://127.0.0.1:30002025-04-03 20:57:59 Press CTRL+C to quit여기서 http://127.0.0.1:3000/hello [GET] 부분을 확인할 수 있습니다.이제 이 URL을 통해 HTTP 요청을 보내서 Application을 테스트해 볼 수 있습니다.Source Code 배포 방법Source Code를 배포할 때는, terraform을 적용할때와 같습니다.$ terraform apply -var-file=./vars/dev.tfvars한계이런식으로 Terraform환경에서도 Serverless Architecture를 구현할 수 있지만, Local구동시에도 Docker와 같은 각종 의존성이 생기는 문제가 있습니다.또한 이로 인해(Docker기반으로 Local서버가 실행되어서), Debugger를 이용해서 Application을 debuging하는 환경은 아직 세팅되어 있지 않습니다.References Terraform Module 구조 Standard Module Structure | Terraform | HashiCorp Developer Terraform에서 사용하는 AWS Spec 확인하기 Terraform Registry terraform-aws-module과 AWS SAM 연동 Terraform Registry Better together: AWS SAM CLI and HashiCorp Terraform | Amazon Web Services Terraform관련 AWS 공식 문서 AWS SAM CLI Terraform support - AWS Serverless Application Model AWS SAM을 이용해서 local에서 test하기 Using the AWS SAM CLI with Terraform for local debugging and testing - AWS Serverless Application Model" }, { "title": "What is “AWS Nitro System”?", "url": "/posts/What-is-AWS-Nitro-System/", "categories": "AWS, Architecture", "tags": "aws, server, machine, vm", "date": "2025-03-30 21:12:00 +0900", "snippet": "들어가면서Public Cloud Provider로 AWS를 주로 사용하게 되는데, AWS의 모든 서비스의 Base가 되는것이 EC2 Instance(이하 EC2)입니다.이 EC2는 2018년 부터, ‘Nitro System’이라는 하드웨어(네트워크 카드와 같은)와 소프트웨어(펌웨어)에 걸친 가상화시스템(virtualization system)으로 B...", "content": "들어가면서Public Cloud Provider로 AWS를 주로 사용하게 되는데, AWS의 모든 서비스의 Base가 되는것이 EC2 Instance(이하 EC2)입니다.이 EC2는 2018년 부터, ‘Nitro System’이라는 하드웨어(네트워크 카드와 같은)와 소프트웨어(펌웨어)에 걸친 가상화시스템(virtualization system)으로 Base로 바뀌게 됩니다.본문Why ‘Nitro System’?기존 Hypervisor의 한계기존의 서버들은 ‘Xen’기반의 하이퍼바이저(Hypervisor)를 사용하고 있었습니다.‘Xen’ Project Architecture(from: https://wiki.xenproject.org/)‘Xen’은 “Dom0(Domain 0)”이라는 특수한 가상 머신을 띄어서, Host의 장치들과 통신합니다.이는 “Dom0”이, ‘다른 VM(Virtual Machine)들에 대한 관리자 역할’ 및 ‘네트워크, 디스크 I/O(Input/Output)를 중개 역할’하는것을 의미합니다. 때문에, 이 구조는 다음과 같은 한계를 같습니다. “Dom0”운영을 위한 CPU 및 메모리 할당을 해야함. I/O 성능의 Bottleneck “Dom0”의 중개를 통해 Host의 자원을 사용하게 되어, 병목을 유발하게 됩니다. 예를 들면, EBS 통해 고속 I/O가 필요할 때, “Dom0”의 처리 능력에 의해 속도가 제한될 수 있습니다. Nitro System의 강점Nitro System virtualization architecture(from “The Security Design of the AWS Nitro System”) “Nitro System”은 기존의 “Dom0”이 수행하던 역할(I/O중개 역할)을 별도의 하드웨어 Card로 대체 하여(Dom0을 제거), CPU오버헤드를 최소화 했고, (하드웨어 가속기를 통해 소프트웨어로 처리되던 일을 하드웨어로 처리하도록 했다는 얘기) KVM(Kernel-based Virtual Machine)을 기반으로 하는 가벼운 VMM(Virtual Machine Manager, Hypervisor)를 사용하여, Hypervisor는 단순하게 가상머신을 관리만 해주도록 기능을 단순화(축소)하였습니다.이렇게, AWS는 아예 하드웨어 수준에서 재설계한 Nitro System을 개발하였고,이 단순화된 Nitro System 덕분에 EC2 인스턴스는 더 빠르고, 더 안전하며, 더 효율적으로 진화할 수 있게 되었습니다. 이 모든것은 VM환경에서 Bare Metal Server(Hypervisor없이 물리 서버를 직접 사용하여, 모든 하드웨어 자원을 100%로 활용할 수 있는 환경)와 가장 유사한 환경을 만들기 위한 노력입니다.Nitro System의 3개의 key Component Purpose-built Nitro Cards “하이퍼바이저의 부하를 하드웨어가 떠맡는다” → 즉, CPU 리소스를 100% 고객에게 제공 가능하게 됩니다. Nitro Network Card 네트워크 I/O를 처리하기 위한 별도의 하드웨어 카드입니다. 이 카드 덕분에, EC2 인스턴스의 네트워크 트래픽을 CPU가 아닌 하드웨어에서 직접 처리할 수 있습니다. Nitro EBS Card (Storage Card) EBS(Elastic Block Store) I/O를 처리하는 하드웨어 카드입니다. 고성능, 고일관성을 제공합니다. 이 카드 덕분에, 데이터는 항상 하드웨어 수준에서 암호화될 수 있게 됩니다. The Nitro Security Chip 인스턴스의 보안 상태를 감시하고, 하드웨어 기반 신뢰 부팅(trusted boot) 제공하는 별도의 하드웨어 입니다. (AWS 직원 포함 누구도 인스턴스 내부 데이터에 접근할 수 없도록 설계되어 있다고 합니다) The Nitro Hypervisor AWS가 개발한 경량화된 KVM 기반 하이퍼바이저로, 가상 머신을 실행시키는 역할을 합니다. 빠르게 OS를 시작할 수 있고, 단순한 구조로 높은 안정성을 갖추고 있습니다. 불필요한 기능들(Xen의 Dom0과 같은)이 제거되어, 매우 단순하고 가볍습니다. I/O처리는 별도의 하드웨어 카드들이 담당하기 때문에, ‘Nitro Hypervisor’는 CPU/메모리 가상화만 담당하면 됩니다. 덕분에 Hypervisor에 따른 오버헤드가 거의 없으며, 단순한 구조 덕분에, 공격 표면(attack surface)이 작아지는 효과가 있습니다. Nitro System의 detail한 부분이 부분에선, ‘AWS Nitro System’에서 조사하다가 인상 깊었던 부분에 대해서 다루려고 합니다.Amazon ENA(Elastic Network Adapter) Express기존의 TCP기반 network에선, 다음 이미지와 같이 packet을 보내는 ‘single path’를 사용했습니다.기존의 TCP flow Hashing때문에, 이 Single Path를 관리하기 위한 ‘congestion control(혼잡 제어)’, ‘패킷 재전송’, ‘connection Re-establishing’에 오버헤드가 걸리게 됩니다.이를 해결하기 위해, SRD(Scalable Reliable Datagram)기반의 ENA Express가 도입되게 됩니다.SRD 기반의 ENA ExpressSRD에선 Packet을 여러 경로(multi-path)를 통해 전달하게 됩니다. 이는, TCP의 높은 신뢰성을 유지하면서, 고속처리를 가능하게 해줍니다.‘Nitro System’에선 이 SRD에 대한 처리를 별도의 Network card가 담당하며, Packet의 순서를 맞춰서 분해하고, 재조립하는 과정을 수행합니다.이 ‘ENA Express’는 TCP와 UDP와 같은 모든 환경에서 사용할 수 있습니다.ENA Express의 개선 효과마무리 이런 AWS의 혁신적인 시스템은, C5, M5, R5, T3, T4g, P3dn, Inf1, Graviton2 기반 인스턴스 Bare Metal 인스턴스 (e.g., c5.metal, i3.metal)등에 적용되어 있습니다.References AWS Nitro System 소개 AWS Nitro System Deep dive into the AWS Nitro System AWS re:Invent 2023 - Deep dive into the AWS Nitro System (CMP306) AWS Nitro System Whitepaper The Security Design of the AWS Nitro System - The Security Design of the AWS Nitro System Xen(AWS의 Legacy Virtual Machine) Xen Project KVM(Kernel-based Virtual Machine) KVM AWS re:Invent 2018: Powering Next-Gen EC2 Instances: Deep Dive into the Nitro System (CMP303-R1) AWS re:Invent 2018: Powering Next-Gen EC2 Instances: Deep Dive into the Nitro System (CMP303-R1) ENA Express 소개 Amazon ENA Express – EC2에서 향상된 네트워크 지연 시간 및 흐름당 성능 Amazon Web Services AWS 고성능 컴퓨팅 네트워크, 2부: AWS가 제공하는 고성능 네트워크 프로토콜, SRD(Scalable Reliable Datagram) Amazon Web Services SRD A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC Paper: A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC" }, { "title": "스타트업을 위한 MySQL 튜닝 방법", "url": "/posts/MySQL-tunning-for-startup/", "categories": "DB, MySQL", "tags": "programming, db, mysql", "date": "2025-03-18 22:55:00 +0900", "snippet": "들어가면서근래에, 자주쓰던 NoSQL계열(mongodb, dynamodb, redis)이 아닌, RDB를 쓸일이 있었습니다.RDB를 production level로 올리고 나니, 개발환경과는 다르게, 몇가지 parameter를 세팅해야 ‘기대하는 기능’을 수행할 수 있었습니다.이 내용을 ‘기록해야겠다’는 생각이 들었습니다.InnoDB의 특성 이해하기 ...", "content": "들어가면서근래에, 자주쓰던 NoSQL계열(mongodb, dynamodb, redis)이 아닌, RDB를 쓸일이 있었습니다.RDB를 production level로 올리고 나니, 개발환경과는 다르게, 몇가지 parameter를 세팅해야 ‘기대하는 기능’을 수행할 수 있었습니다.이 내용을 ‘기록해야겠다’는 생각이 들었습니다.InnoDB의 특성 이해하기 MySQL에서 일반적인 목적(General Purpose)로 많이 사용하는 Storage Engine인 ‘innoDB’를 기준으로 합니다. ‘InnoDB’는 트랜잭션을 실행할 때 즉시 데이터를 디스크에 반영하지 않고, 먼저 로그(redo log)에 기록합니다. ‘트랜잭션 수행 과정’은 다음과 같습니다. 변경된 데이터는 Buffer Pool (메모리)에 먼저 저장됩니다. 트랜잭션을 COMMIT하면 Redo Log에 기록됩니다 (innodb_log_file). 이후, Lazy Flushing을 통해 실제 데이터를 디스크의 데이터 파일(.ibd)에 기록. 이 과정 때문에, COMMIT이 되었더라도 데이터 파일에는 즉시 반영되지 않을 수 있습니다.때문에, insert한 데이터를 바로 read했을때, 조회가 안되는 ‘phantom read’ 현상이 발생할 수 있습니다.본문TCP Connection 재사용성 높이기 max_connections 값 조정MySQL에 최대로 동시접속할 수 있는 Connection 갯수를 조절합니다.너무 낮으면, 새로운 TCP 커넥션을 만드는 일이 잦아지고, 너무 높으면 불필요한 리소스(TCP를 유지하는데에)를 계쏙 쓰게 됩니다.MySQL :: MySQL 8.4 Reference Manual :: 7.1.8 Server System Variables wait_timeout 과 interactive_timeout 값 조정MySQL은 사용되지 않는 TCP connection을 자동으로 종료하는데, 이때 idle시간(사용하지 않는 시간)을 조정하여, Connection 재사용률을 조절합니다. wait_timeout : 일반 Client용 세팅값 interactive_timeout : CLI 또는 DB 관리도구(GUI Client)용 세팅값MySQL :: MySQL 8.4 Reference Manual :: 7.1.8 Server System Variables skip_name_resolve 활성화MySQL은 Client가 접속할 때, DNS조회를 수행하는데, IP기반으로 연결한다면 불필요 합니다.MySQL :: MySQL 8.4 Reference Manual :: 7.1.8 Server System VariablesTransaction 반영 속도 높이기 innodb_flush_log_at_trx_commit = 2 로 세팅해서, transaction이 빨리 반영하도록 합니다.2로 설정하면, ‘강한 일관성’을 약간 포기하고(crash상황에서 transaction을 유실할 수 있습니다), transaction종료 후, log를 memory에 보관하고 사용합니다.(default인 1인 경우는 즉시 disk에 기록합니다.) Controls the balance between strict ACID(https://dev.mysql.com/doc/refman/8.4/en/glossary.html#glos_acid)compliance forcommitoperations and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value but then you can lose transactions in a crash. MySQL :: MySQL 8.4 Reference Manual :: 17.14 InnoDB Startup Options and System Variables innodb_log_file_size 값 조정innodb_log_file_size 는 innodb의 ‘Redo log’의 크기를 결정합니다.이 값을 더 크게 조정하면, transaction을 disk에 쓰기전에, ‘Redo log’에 저장되는 양이 많아지고, 메모리에 더 많은 내용을 저장하게 됩니다.이 결과로, ‘Checkpoint’생성(memory에 저장된 변경사항을 disk에 쓰는 작업) 빈도가 줄어듭니다. MySQL :: MySQL 8.4 Reference Manual :: 17.14 InnoDB Startup Options and System Variables 이 값을 반영하려면, DB재시작이 필요합니다. innodb_purge_threads 값 조정이 값은, innoDB의 MVCC(Multi-Version Concurrency Control, 하나의 record를 versioning하여 동시성을 제공하는 개념) 기능에서 따라오는 GC(Garbage Collection)성능을 개선하는 것과 관련된 parameter입니다.‘Undo Log(이전 버젼의 데이터를 저장하는 곳)’를 정리하는 Thread 수를 결정하며, 높을수록 빨리 정리됩니다.‘Undo Log’가 정리되지 않으면, ‘Buffer pool’에 데이터가 쌓이게 되어 transaction반영 속도를 느리게하는 원인이 됩니다. Transaction이 완료되면, 더 이상 필요 없는 ‘Undo Log’를 삭제하는 작업(purge)이 필요합니다. MySQL :: MySQL 8.4 Reference Manual :: 17.14 InnoDB Startup Options and System Variables 이 값을 반영하려면, DB재시작이 필요합니다. References Best practices for configuring parameters for Amazon RDS for MySQL Best practices for configuring parameters for Amazon RDS for MySQL, part 1: Parameters related to performance (Amazon Web Services) Designing Data Intensive Application(book) Designing Data-Intensive Applications (DDIA) — an O’Reilly book by Martin Kleppmann (The Wild Boar Book) MySQL의 storage engine인 ‘innodb’ introduction ‘Key Advantages of InnoDB’부분을 봐두면 좋습니다. MySQL :: MySQL 8.4 Reference Manual :: 17.1 Introduction to InnoDB MySQL innodb Parameters MySQL :: MySQL 8.4 Reference Manual :: 17.14 InnoDB Startup Options and System Variables MySQL Server System Parameters MySQL :: MySQL 8.4 Reference Manual :: 7.1.8 Server System Variables ‘Phantom Rows’ MySQL :: MySQL 8.4 Reference Manual :: 17.7.4 Phantom Rows" }, { "title": "What is gRPC?", "url": "/posts/what-is-grpc/", "categories": "Programming, networking", "tags": "programming, networking, grpc, restapi, protobuf, http/2", "date": "2024-11-21 19:03:00 +0900", "snippet": "들어가면서…gRPC는 REST 방식과 함께, 서버의 기능들을 외부에서 사용할 수 있도록하는 Interface중 하나입니다.API의 의미API는 2개의 플랫폼 사이에서 통신하는 Interface에 관한 얘기입니다. 대표적으로 Client와 Server간의 통신을 정의하는 REST API가 있습니다.그러나, Server의 특정 기능을 실행시키는 다른 방법...", "content": "들어가면서…gRPC는 REST 방식과 함께, 서버의 기능들을 외부에서 사용할 수 있도록하는 Interface중 하나입니다.API의 의미API는 2개의 플랫폼 사이에서 통신하는 Interface에 관한 얘기입니다. 대표적으로 Client와 Server간의 통신을 정의하는 REST API가 있습니다.그러나, Server의 특정 기능을 실행시키는 다른 방법이 있는데, 바로 RPC(Remote Procedure Call)입니다.RPC의 어려움이 RPC는 서버의 function을 네트워크를 통해 직접 실행하는 방법으로, REST API에 비해 아래와 같이 적용하기 어려운 점이 있습니다. 표준화된 규격 부족REST API는 HTTP기반으로 모든 언어에서 기본적으로 지원하고 있고, 표준화된 부분이 많아, design을 쉽게 따라할 수 있습니다. 반면에 RPC는 표준화가 부족하고 특정 기술에 종속되어 interoperability(상호 운영성)에 어려움이 있습니다. 복잡한 ArchitectureREST API가 Resource-oriented(리소스 중심) 설계로 직관적으로 동작을 이해할 수 있지만, RPC는 원격 메소드 호출마다 개별적으로 정의해야해서, 더 복잡해지게 됩니다. 디버깅의 어려움REST API는 human-readable한 JSON데이터를 주로 사용하는 반면에, RPC는 Binary포맷을 사용해, 디버깅에 어려움이 있습니다. 그럼에도, RPC는 REST API보다 더 나은 성능을 보여줍니다. URL Endpoint가 아닌(REST API인 경우), Server 함수를 직접 호출함으로서, 실행과정의 Depth를 줄여줍니다. Persistent Connection(지속적 연결)를 유지하여, connection을 계속해서 재사용 합니다.gRPC의 등장gRPC는 이런 RPC의 문제들을 해결하며, 개발 편의성과 분산처리를 고려하고, HTTP/2에 대한 호환성을 바탕으로 개발된 RPC프레임워크입니다.특히 gRPC는 Protocol Buffers(protobuf)라는 Binary데이터를 사용합니다.gRPC와 REST API 비교 특징 REST API gRPC 프로토콜 HTTP/1.1, HTTP/2 HTTP/2 데이터 형식 JSON, XML Protobuf (바이너리) 성능(상대적인 비교) 느림 빠름 통신 방식 동기식 스트리밍 지원(양방향 포함) 읽기 쉬움 사람이 읽고 쓰기 쉬움 사람이 읽기 어려움 사용 사례 간단한 웹 서비스 마이크로서비스, 실시간 애플리케이션 연결 관리 무상태, Keep-Alive 필요 지속적 연결(Persistent Connection) 스트리밍 지원 제한적 양방향 스트리밍 지원 네트워크 효율성 낮음 높음 처리 속도 느림 빠름 REST방식과 gRPC방식의 차이 from https://refine.dev/blog/grpc-vs-rest/#step-3-1데이터 serializing 방법인 Protocol Buffers(protobuf)Protocol Buffers(이하 protobuf)는 데이터를 Serializing(직렬화)하는 방법중에 하나입니다.또다른 Serializing 방법으로, JSON이 있습니다.JSON의 문제점JSON은 사람이 이해하기 쉬워서 디버깅이 쉽고, 기존의 XML에 비해 매우 단순한 구조를 갖고 있습니다.하지만, 아래와 같은 몇가지 문제점이 있습니다.효율성 문제 텍스트 기반 JSON은 사람이 읽기 쉽게 설계된 텍스트 기반 형식입니다. 이는 가독성을 높이지만, 데이터 크기가 커질 수 있습니다. 바이너리 형식(예: Protobuf, Avro)에 비해 데이터 크기가 더 크고 전송 시간이 더 오래 걸립니다. 중복된 키 JSON에서 키-값 쌍은 키 이름이 반복적으로 나타나므로 데이터 크기를 증가시킵니다. 예를 들어, 큰 배열에서 동일한 키가 반복됩니다. 압축 비효율성 JSON은 압축 알고리즘을 적용하면 개선되지만, 바이너리 형식만큼 효율적이지 않습니다. 데이터 타입의 한계 제한된 데이터 타입 JSON은 다음과 같은 데이터 타입만 지원합니다: 숫자(Number), 문자열(String), 불리언(Boolean), 객체(Object), 배열(Array), 널(null). 복잡한 데이터 타입(예: 날짜, 정수형/부동소수점 구분, 바이너리 데이터 등)을 표현하는 데 한계가 있습니다. 예: 날짜는 문자열로 표현되며, 이는 표준화되지 않아 해석이 달라질 수 있습니다. 숫자 표현 문제 JSON의 숫자는 모두 floating point(부동소수점)으로 표현됩니다. 이로 인해 정밀도 손실이 발생할 수 있습니다(예: 매우 큰 숫자나 작은 소수). JSON 분석하는 과정의 성능 문제 Parsing 속도문제 JSON은 TEXT 기반이므로 파싱 속도가 느립니다. 데이터를 읽고 사용하는 데 추가적인 처리 시간이 필요합니다. Schema 부재 명시적 Schema가 없습니다. JSON은 데이터의 구조를 명시적으로 정의하지 않습니다. 이는 데이터의 유연성을 제공하지만, 데이터 구조를 사전에 알지 못하면 처리하기 어려울 수 있습니다. 스키마가 없기 때문에 데이터 검증이 추가적으로 필요하며, 데이터 무결성을 보장하기 어렵습니다. 예: JSON 스키마를 따로 정의하거나 애플리케이션 수준에서 구조를 강제해야 함. 버전 관리가 어렵습니다 JSON은 버전 관리를 지원하지 않으므로, 데이터 구조가 변경될 경우 클라이언트와 서버 간 호환성을 유지하기 어려울 수 있습니다. 대규모 데이터 처리 비효율성 스트리밍 지원 부족 JSON은 전체 데이터를 메모리에 로드한 후 파싱합니다. 대규모 데이터를 처리하는 데 비효율적입니다. 반면, Protobuf와 같은 바이너리 형식은 스트리밍 처리에 더 적합합니다. 중첩된 데이터의 비효율성 중첩된 구조가 많아지면 JSON의 크기가 커지고, 읽기와 처리 성능도 저하됩니다. 표준화 부족 날짜 및 시간 형식 JSON은 날짜와 시간을 지원하지 않으므로, ISO 8601 형식 또는 Unix 타임스탬프 등 다양한 표현 방식이 사용됩니다. 이는 데이터 호환성과 일관성에 문제를 야기할 수 있습니다. 다양한 해석 JSON 파서마다 세부 동작이 다를 수 있습니다(예: 숫자 범위 처리, 특수 문자 처리). 그래서 Protobuf는 뭐가 나은데?protobuf는 Interface를 정의하는 언어(IDL)로서, 메세지 교환 포맷을 결정하는 역할을 합니다. gRPC can use protocol buffers as both its Interface Definition Language (IDL) and as its underlying message interchange format.from What is gRPC?syntax = \"proto3\";message HelloRequest { string name = 1;}message HelloReply { string message = 1;}service Greeter { rpc SayHello(HelloRequest) returns (HelloReply);}Binary로 이루어진 데이터입니다. 데이터 크기 최소화. 필드 번호(Field Numbers) 기반의 메시지 구조 이는 Field에 순서가 있음을 의미하며, 이를 기반으로 상위(forward compatibility) 및 하위 호환성(backward compatibility)을 제공합니다 Protobuf 메시지의 각 필드는 고유한 번호를 가지며, 이 번호는 메시지를 serialization(직렬화)/deserialization(역직렬화)하는 데 사용됩니다. 필드 이름은 코드에서 사용되지만, 실제 데이터 전송에서는 필드 번호만 필요합니다. 새로운 필드를 추가하거나 기존 필드를 제거하더라도, 필드 번호가 유지되면 메시지의 호환성을 유지할 수 있습니다. 압축 효율이 높습니다.더 확장된 데이터 타입을 제공합니다 정수형(int32, int64), 부동소수점(float, double) 구분 가능.Schema가 존재합니다 .proto 파일을 통해 데이터 구조를 명확히 정의하여 사용합니다. 데이터 검증 및 구조 변경이 편리합니다.gRPC의 기능들Client load balancing(proxy less)기존에는, Client에서는 proxy역할을 해주는 1개의 Load balancer(Infra단에 있는, 이하 LB)에 접속하여, LB에서 적당한 Server로 Routing해주곤 했습니다.gRPC를 이용하면, 이 depth또한 제거할 수 있습니다.(proxy less)Client에서 직접 접속할 Server들의 목록을 관리하여, balancing을 직접 수행하게 합니다.Server의 목록은 다음과 같은 방법으로 관리합니다. Static Addresses: 서버 주소를 클라이언트 코드나 설정 파일에 하드코딩합니다. Service Discovery: DNS, Kubernetes 등에서 dynamic하게(동적으로) 서버 리스트를 가져옵니다.gRPC 사례Service Mesh인 Istio와 함께 사용하기Istio는 마이크로서비스 간의 네트워크 통신을 관리하는 서비스 메쉬로 다음과 같은 기능을 제공합니다. 트래픽 관리: 라우팅, 로드 밸런싱, A/B 테스트, Canary 배포 지원. 서비스 간 보안: TLS 암호화 및 인증 관리. 모니터링 및 로깅: 트래픽, 오류, 성능 데이터를 수집하여 관찰성(Observability) 제공. 정책 관리: 서비스 간 통신 규칙 설정 (예: Rate Limiting, Access Control).Istio와 gRPC의 관계gRPC 트래픽 관리 Istio는 gRPC 트래픽도 HTTP/2로 처리할 수 있어, gRPC 요청을 라우팅하거나 로드 밸런싱할 때 별도의 추가 설정 없이 기본적으로 지원합니다. 서비스 간 트래픽을 제어하기 위해 Istio의 VirtualService 리소스를 활용할 수 있습니다: apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: grpc-service spec: hosts: - grpc-service.default.svc.cluster.local http: - match: - port: 50051 # gRPC의 protocol port headers: content-type: exact: \"application/grpc\" route: - destination: host: grpc-service port: number: 50051 보안 통신 (mTLS) gRPC는 HTTP/2를 사용하므로 TLS를 통해 보안 통신을 설정할 수 있습니다. Istio는 추가적으로 서비스 간 mTLS (Mutual TLS)를 제공하여 gRPC 트래픽의 암호화와 인증을 중앙에서 관리합니다.Observability(관찰성) Istio는 Envoy Proxy를 사용하여 gRPC 호출의 메트릭, 로깅, 분산 추적(Distributed Tracing)을 제공합니다. gRPC 서비스에서 발생하는 성공/실패 요청, 응답 시간, 오류 코드를 Istio의 Telemetry 도구로 모니터링할 수 있습니다.Fault Injection 및 재시도 Istio는 gRPC 서비스에 대해 재시도(Retry), 타임아웃(Timeout), Fault Injection(장애 테스트) 같은 네트워크 레벨의 제어 기능을 제공합니다. apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: grpc-service spec: hosts: - grpc-service.default.svc.cluster.local http: - retries: attempts: 3 perTryTimeout: 2s gRPC Load Balancing gRPC는 클라이언트 라이브러리에서도 로드 밸런싱 기능을 제공하지만, Istio를 사용하면 보다 정교한 트래픽 분배와 서비스 디스커버리를 수행할 수 있습니다.gRPC 리플렉션 및 디버깅 Istio를 사용하면 gRPC 서비스의 디버깅 및 관리를 위해 리플렉션(reflection) 기능과 Istio의 라우팅 규칙을 조합하여 개발 및 운영 환경을 쉽게 설정할 수 있습니다.References gRPC 홈페이지 gRPC gRPC의 지난 10년과 미래 Ten Years of gRPC | Jung-Yu (Gina) Yeh &amp; Richard Belleville, Google gRPC Core Concept Core concepts, architecture and lifecycle gRPC와 REST의 차이점 gRPC와 REST 비교 - 애플리케이션 설계의 차이 - AWS gRPC VS REST gRPC vs REST - A Brief Comparison | Refine protobuf(Protocol Buffers) Protocol Buffers" }, { "title": "LeetCode 75-45 1466. Reorder Routes to Make All Paths Lead to the City Zero", "url": "/posts/python-Reorder-Routes-to-Make-All-Paths-Lead-to-the-City-Zero/", "categories": "Programming, python, leetcode75", "tags": "programming, python, leetcode", "date": "2024-11-13 15:31:00 +0900", "snippet": "문제 요약도시 사이의 connection 정보가 input으로 주어집니다.n개의 도시에 대한 connection(edge정보)이 주어지는데, 이는 방향성(directional)이 있습니다.이중 수도인 capital(0번 도시)로 향하지 않는 방향성을 capital로 향하도록 수정하려고 합니다.최종 return값은 수정해야 하는 edge의 갯수입니다.E...", "content": "문제 요약도시 사이의 connection 정보가 input으로 주어집니다.n개의 도시에 대한 connection(edge정보)이 주어지는데, 이는 방향성(directional)이 있습니다.이중 수도인 capital(0번 도시)로 향하지 않는 방향성을 capital로 향하도록 수정하려고 합니다.최종 return값은 수정해야 하는 edge의 갯수입니다.Example 1문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n)$입니다. 여기서 n은 connection의 길이, 즉 city의 갯수입니다. DFS의 복잡도는 edge와 node의 갯수와 관련이 있지만, 이 문제에서는 모두 city의 갯수와 일치하기 때문에, $O(n)$으로 표기하였습니다. 공간복잡도(space complexity) 는 $O(n)$입니다. connections의 길이에 따라 별도의 Set[(int, int)]과 Dict[int, list[int]]를 생성합니다. 복잡도를 계산하기 매우 어렵게 느껴서, 부정확합니다. edge_set: 먼저 connections를 추후에 비교연산을 효율적으로 수행하기 위해, Set[(출발 도시, 도착 도시)] 형태로 변형시킵니다. neighbor_nodes: 추후 connections을 iteration하며, 이웃한 도시를 저장할때 사용합니다. visit_history: 이미 방문한 도시를 기록하여, 중복연상을 방지합니다. 이제 0번 도시를 기준으로 Tree라고 생각합니다. Tree는 root에서 leaf로 뻗어나가면서 연산이 진행되기 때문에, 비교할때 반대방향으로 하는것만 주의합니다. dfs로 neighbor_nodes를 순회하며, 문제의 요구사항이 모든 edge가 0번도시(Tree의 Root node)를 향해야 한다고 했기 때문에, edge방향성 비교를 할때 반대로 하도록 주의해야합니다. DFS를 하며 edge가 기대(expect, 0번도시를 향하지 않는 경우)와 다른 경우를 찾습니다. 찾은 count를 return합니다.from typing import Listclass Solution: def minReorder(self, n: int, connections: List[List[int]]) -&gt; int: edge_set: set[(int, int)] = set((connection[0], connection[1]) for connection in connections) neighbor_nodes: dict[int, list[int]] = { city: [] for city in range(n) } visit_history = set() count_changes = 0 for connection in connections: neighbor_nodes[connection[0]].append(connection[1]) neighbor_nodes[connection[1]].append(connection[0]) def dfs(neighbor_nodes: dict[int, list[int]], index: int) -&gt; None: nonlocal edge_set, count_changes, visit_history visit_history.add(index) neighbors = neighbor_nodes[index] for neighbor_index in neighbors: if neighbor_index in visit_history: continue if (neighbor_index, index) not in edge_set: count_changes += 1 dfs(neighbor_nodes, neighbor_index) return dfs(neighbor_nodes, 0) return count_changessolution = Solution()assert solution.minReorder(6, [[0,1],[1,3],[2,3],[4,0],[4,5]]) == 3assert solution.minReorder(5, [[1,0],[1,2],[3,2],[3,4]]) == 2결과Leetcode 제출 결과후기혼자서 풀때는 풀지 못했고, Leetcode Study모임에서 풀어오신분에게 영감을 받아 풀 수 있었습니다.확실히 Tree와 Graph로 넘어오면서 한층 더 난이도가 높게 느껴집니다.References1466. Reorder Routes to Make All Paths Lead to the City Zero - LeetCode" }, { "title": "NIPA ML 프로젝트 ‘Synchro-you’ 공유", "url": "/posts/NIPA-&-ML-project-Synchro-you/", "categories": "ML, MachineLearningBootcamp", "tags": "ML, Vector, DeepLearning, Similarity, Product, NIPA, TeamProject", "date": "2024-11-12 20:44:00 +0900", "snippet": "들어가기전에..2024 Google MLB가 끝나고, NIPA와 Google MLB와 연계된 실무 프로젝트(이하 NIPA)를 이어가게 되었습니다.NIPA에서 6명이 팀을 이루어 ML을 이용한 ‘Synchro-you’라는 제품을 만들었습니다.Synchro-you 프로젝트 소개최근 유행하고 있는 각종 챌린지 영상에서 영감을 받아, ‘내가 얼마나 잘 따라하...", "content": "들어가기전에..2024 Google MLB가 끝나고, NIPA와 Google MLB와 연계된 실무 프로젝트(이하 NIPA)를 이어가게 되었습니다.NIPA에서 6명이 팀을 이루어 ML을 이용한 ‘Synchro-you’라는 제품을 만들었습니다.Synchro-you 프로젝트 소개최근 유행하고 있는 각종 챌린지 영상에서 영감을 받아, ‘내가 얼마나 잘 따라하는지’를 확인하는 서비스를 만들게 됐습니다.AI를 활용한 실시간 댄스 점수 확인 서비스(How Accurate is My Dance Sync?) AI가 동장을 정밀하게 인식하고, 기준이 되는 동작과의 유사도를 점수로 제공합니다. 즉각적인 피드백을 받음으로서, 사용자들은 자신의 퍼포먼스를 개선할 수 있습니다. Game처럼 플레이할 수 있습니다.예상하는 문제들.. 촬영 각도에 따라 좌표 차이가 발생할 수 있는 문제.3차원 공간에서 동작이 일치해도, 2차원인 영상으로 변환되기 때문에, 촬영 각도에 따라 좌표 차이가 발생할 수 있습니다. 같은 길이(time기준)의 영상이여도, fps(Frame per seconds)의 차이로 총 데이터의 길이가 다를 수 있다.즉, 단순히 두 영상의 frame과 frame을 비교한다는 생각으로 접근할 순 없다. 구현 전략Rule-based 방법과 Deep Learning을 이용한 방법 비교저희는 ‘동작이 일치한다’는 개념에 대해서 고민하기 시작했습니다.동작을 어떻게 수치화(feature로 만들것인지) 고민하기 시작했습니다.그래서 결론 지은것이, 동작(movement)의 맥락(Context)이 필요하다는 것이었습니다Rule-based를 이용한 방법‘Rule-based’는 특정 규칙을 정의하여 비교하는 방법입니다. 우리 제품같은 경우, ‘frame별로 특정 포인트가 일치하면 일치 하는지’ 정도로 표현할 수 있습니다.이 방법을 사용하면, 움직임의 frame을 일치시켜야 하는 문제가 있습니다.frame과 frame을 비교하는 예시 - from https://en.wikipedia.org/wiki/Dynamic_time_warpingDeep Learning을 이용한 방법(채택된 방법)DTW라는, 시계열(time series)기반의 2개의 연속된 데이터(sequence)간 유사도(similarity)를 측정할 수 있는 알고리즘이 있다고 합니다.여기서 영감을 받아, 영상속 사용자의 pose를 인식하여, 주요 관절의 각도(angle)을 비교하는 방향으로 구현하게 되었습니다. 이렇게 하면, 카메라 촬영 각도에 따른 문제(불일치 문제)를 해결할 수 있습니다. 학습이나 추론에 사용하는 데이터량을 줄일 수 있습니다.데이터 선정AI hub의 ‘K-pop 안무영상’을 활용하였습니다.이 데이터셋은 한가지 춤에 대해서 여러 각도로 찍은 영상을 포함하고 있습니다. 이를 이용해, Positive인 경우와 Negative경우의 학습 데이터를 쉽게 구성할 수 있습니다. 데이터셋 안에 이미 관절 좌표가 있지만, 저희가 쓸 라이브러리 데이터에서 추출한게 아니기 때문에, 데이터 일관성을 위해 사용하지 않았습니다.데이터 전처리mediapipe의 pose landmark정보와 사용한 각도(angle) Landmark feature 영상 데이터 → Media pipe (라이브러리) 적용 → 프레임 별 Landmark 추출 → 스켈레톤 벡터 계산 → 신체 부위 각도 계산 Feature selection 33개의 landmark를 각도(angle)로 바꾸어, 최종 Input값은 신체각도 12개가 되었습니다.(위 이미지에서 빨간색으로 표시된 부분) Sequential data 특정 frame 단위(여기선 30)로 각도값들을 결합시켜서(concat), sequential데이터로 만들어 inference를 실행하게 됩니다.모델 학습 방법, Supervised Contrastive LearningAnchor를 기준으로, Positive Pair와 Negative Pair을 지정하여 학습시킵니다.Positive Pair: 같은 춤 내의 서로 다른 방향에서 촬영된 두 영상Negative Pair: 서로 다른 춤을 춘 두 영상모델 구조※Embedder: LSTM + Layer Normalization. ‘Synchro-you’ model architecture input된 데이터를 일관성을 유지하기 위해, 같은 가중치를 사용하여 ‘Embedding vector’를 계산 합니다. 2개의 ‘Embedding vector’간 ‘Supervised Constrastive Learning Loss(SCL loss)’를 계산합니다. $\\frac{\\text{positive}}{\\text{positive} + \\text{negative}}$으로 모델을 평가하게 만들어, Postive간의 유사도는 높이고(↑), negative의 유사도는 낮추도록(↓) 하였습니다. Binary classification을 통해, positive(1)과 negative(0)사이의 값이 만들어지도록 합니다.모델 배포 전략소규모 프로젝트 팀이기 때문에, 자동화된 배포전략을 구축하긴 어려웠습니다. TorchScript사용 모델 빌드팀과 서비스 빌드팀을 decouple하기 위해서, Pytorch 프레임쿼크의 TorchScript를 사용하였습니다. 모델 학습(Learning)환경에서 학습이 완료되면, 해당 모델의 가중치(weight)와 모델 구조를 TorchScript로 변환하여 저장하였습니다. 추후, 서비스 빌드팀에서 TorchScript를 받아, 다양한 환경에서 API서버를 빌드할 수 있게되었습니다.개인정보 보호 전략사용자가 자신이 찍힌 모습을 Server로 전송하는건 좀 더 민감한 문제가 됩니다.때문에, 영상을 서버로 전송하지 않고, Web App(Client)에서 영상의 Pose 및 각 관절 좌표를 인식하여 서버로 보냅니다.curl --location 'http://localhost:4000/predict' \\--header 'Content-Type: application/json' \\--data '{ \"seq\": 0, \"user_pose_data\": [[ 0.503532886505127, 0.25677618384361267, -0.2561524510383606, 0.5126909613609314, 0.245061457157135, -0.2039661705493927, ... 0.4993351995944977, 0.8586224317550659, -0.2884582281112671 ] ]}'Service ArchitectureClientnextjs와 reactjs를 이용해서 static으로 bulid하여 배포하였습니다.mediapipe를 통해 Client에서 영상의 좌표를 뽑아 서버에게 inference를 요청합니다.mediapipe가 WASM(WebAssembly, 웹 어셈블리)를 지원하여, Web app에서도 손쉽게 사용할 수 있습니다.ServerFastAPI와 pytorch를 이용해서 inference를 간단하게 구성하였습니다.InfraAWS의 ECS환경에서 CPU inference를 수행합니다.시연몇개의 챌린지를 예시로, Positive인 경우와 Negative인 경우의 점수 차이를 비교해 보았습니다.‘삐끼삐기 챌린지’긍정(Positive) 사례삐끼삐기 Positive 사례부정(Negative) 사례삐끼삐기 Negative 사례‘마라탕후루 챌린지’긍정(Positive) 사례마라탕후루 Positive 사례부정(Negative) 사례마라탕후루 Negative 사례영상(Demonstration) Your browser does not support the video tag. Here is a link to the video file instead. 회고팀장으로 역할하면서 여러 사람들과 ML제품을 zero-base에서 만들어간 것은 처음이었습니다.모두 진지하게 임해주셔서, 참 보람찬 프로젝트였습니다.남은 과제 gRPC를 이용한 real-time inference 제공처음 설계시에는 real-time inference를 위한 환경까지 구축하는것이었지만, 프로젝트 시간 관계상, 그리고 이미 서버의 inference time이 500ms로 준수한 성능을 내주어서, 나중으로 미루었습니다. 영상 시작점이 제각각인 경우에 대한 대응 미비같은 챌린지 영상이어도, 움직임이 시작되는 시점이 달라 정확한 inference가 이루어지지 않는 문제가 남아있습니다. Client의 버벅임 문제Client에서 inference가 WASM이라 병렬로 이루어지긴 하지만, 해당 결과를 영상에 좌표를 매 frame마다 그리는게 원인으로 보이며, web worker를 통해 해결할 수 있지 않을까… 싶습니다. References DTW(Dynamic time warping) Dynamic time warping TorchScript TorchScript — PyTorch 2.5 documentation" }, { "title": "HashMap Implementation with Kotlin", "url": "/posts/HashMap-Implementation-with-Kotlin/", "categories": "Programming, kotlin", "tags": "Computer Science, programming, Data Structure, kotlin", "date": "2024-10-27 03:22:00 +0900", "snippet": "OverviewHashmap은 Software의 탐색 속도를 개선하는데 활용하게 됩니다.이때, Hashmap이 어떻게 구현(implementation)되어 있는지 확인해보려 합니다.Source Codeimport java.io.Serializableimport kotlin.collections.*object EmptyIterator : ListIter...", "content": "OverviewHashmap은 Software의 탐색 속도를 개선하는데 활용하게 됩니다.이때, Hashmap이 어떻게 구현(implementation)되어 있는지 확인해보려 합니다.Source Codeimport java.io.Serializableimport kotlin.collections.*object EmptyIterator : ListIterator&lt;Nothing&gt; { override fun hasNext(): Boolean = false override fun hasPrevious(): Boolean = false override fun nextIndex(): Int = 0 override fun previousIndex(): Int = -1 override fun next(): Nothing = throw NoSuchElementException() override fun previous(): Nothing = throw NoSuchElementException()}object EmptySet : Set&lt;Nothing&gt;, Serializable { private const val serialVersionUID: Long = 3406603774387020532 override fun equals(other: Any?): Boolean = other is Set&lt;*&gt; &amp;&amp; other.isEmpty() override fun hashCode(): Int = 0 override fun toString(): String = \"[]\" override val size: Int get() = 0 override fun isEmpty(): Boolean = true override fun contains(element: Nothing): Boolean = false override fun containsAll(elements: Collection&lt;Nothing&gt;): Boolean = elements.isEmpty() override fun iterator(): Iterator&lt;Nothing&gt; = EmptyIterator private fun readResolve(): Any = EmptySet}class HashMapImpl&lt;K, V&gt; constructor() { private val defaultCapacity = 16 private var loadFactor = 0.75f private var size = 0 private var sizeThreshold = (defaultCapacity * loadFactor).toInt() val entries: Set&lt;Map.Entry&lt;K, V&gt;&gt; get() = EmptySet private class Bucket&lt;K, V&gt; constructor(val key: K?, val value: V?) { var next: Bucket&lt;K, V&gt;? = null; } private var _buckets: Array&lt;Bucket&lt;K, V&gt;?&gt;? = null; private var buckets: Array&lt;Bucket&lt;K, V&gt;?&gt; get() { if (_buckets == null) { _buckets = kotlin.arrayOfNulls&lt;Bucket&lt;K, V&gt;&gt;(defaultCapacity); } return _buckets ?: throw AssertionError(\"Set to null by another thread\"); } set(value) { _buckets = value } private var _values: Collection&lt;V&gt;? = null val values: Collection&lt;V&gt; get() { if (_values == null) { _values = object : AbstractCollection&lt;V&gt;() { override operator fun contains(element: @UnsafeVariance V): Boolean = containsValue(element) override operator fun iterator(): Iterator&lt;V&gt; { val entryIterator = entries.iterator() return object : Iterator&lt;V&gt; { override fun hasNext(): Boolean = entryIterator.hasNext() override fun next(): V = entryIterator.next().value } } override val size: Int get() = this@HashMapImpl.size } } return _values!! } private var _keys: AbstractSet&lt;K&gt;? = null; val keys: AbstractSet&lt;K&gt; get() { if (_keys == null) { _keys = object : AbstractSet&lt;K&gt;() { override operator fun contains(element: K): Boolean = containsKey(element) override operator fun iterator(): Iterator&lt;K&gt; { val entryIterator = entries.iterator() return object : Iterator&lt;K&gt; { override fun hasNext(): Boolean = entryIterator.hasNext() override fun next(): K = entryIterator.next().key } } override val size: Int get() = this@HashMapImpl.size } } return _keys!! } fun containsKey(key: K): Boolean { return implFindEntry(key) != null } fun put(key: K, value: V): V { val index = getIndex(key); val bucket = Bucket(key, value); if (index in buckets.indices) { // depth 1단계에 bucket이 이미 존재한다면, linked list로 연결. buckets[index]?.next = bucket; } else { buckets[index] = bucket; } size += 1 if (size &gt;= sizeThreshold) { resize() } return value; } fun get(key: K): V? { val index = getIndex(key); if (index !in buckets.indices) { return null; } var bucketHead = buckets[index] ?: return null; while (bucketHead.next != null &amp;&amp; bucketHead.key != key) { bucketHead = bucketHead.next!!; } return bucketHead.value; } private fun getIndex(key: K): Int { return Math.floorMod(key.hashCode(), buckets.size) } private fun resize() { val tempBuckets = buckets; buckets = Array(buckets.size * 2) { null }; tempBuckets.forEachIndexed { idx, bucket -&gt; buckets[idx] = bucket } } private fun implFindEntry(key: K): Map.Entry&lt;K, V&gt;? = entries.firstOrNull { it.key == key } private fun containsValue(value: V): Boolean = entries.any { it.value == value } fun size() = size}fun main() {\t\t// test를 위한 code val hashMap = HashMapImpl&lt;String, String&gt;(); hashMap.put(\"firstKey\", \"firstValue\"); hashMap.put(\"secondKey\", \"secondValue\") assert(hashMap.keys == hashSetOf(arrayOf(\"firstKey\", \"secondKey\"))); assert(hashMap.get(\"secondKey\") == \"secondValue\");}해설Hashmap안에, 실제로 값(key, value 모두)이 저장되는건 Array의 element로 저장됩니다.처음 HashMap을 생성할때, 특정 값(defaultCapacity)에 해당하는 길이(size)만큼의 Array를 생성해서 사용합니다.BucketHashmap내부에선 key와 value를 Bucket 이라는 instance를 생성하여 HashMap의 array에 추가합니다.만약 값을 추가할 때(put 할 때), ‘hash collision’이 발생하면 Bucket을 ‘Linked List’형태로 추가하게 됩니다.HashMap 내부 데이터 구조. From https://oshyshkov.com/2021/07/07/java-hashmap-implementation/RehashingHashMap에 값이 많이 추가될 수록, LinkedList의 길이가 커지며, HashMap 성능인 $O(1)$의 이점을 누리기가 어려워집니다. 그래서 Rehashing 이라는 작업이 이루어 집니다.Rehashing은 특정 threshold(임계점)을 기준으로, 이보다 더 많은 element가 쌓이면 이루어집니다.내부 Array를 더 큰 사이즈(보통 2배 크기)로 다시 생성하여, 기존 Array의 값을 모두 copy합니다.loadFactorHashMap에서 loadFactor는 해시 맵의 성능과 효율성을 관리하는 중요한 요소입니다. loadFactor는 해시 맵이 얼마나 채워지면 리해싱(rehashing)을 수행할지를 결정하는 기준입니다.loadFactor는 해시 맵이 얼마나 채워졌는지를 rate(비율) 값으로 나타내며, 기본적으로는 0.75로 설정됩니다. 즉, 현재 크기 대비 허용 가능한 최대 항목 수를 정의합니다.\\[\\text{loadFactor}=(\\text{저장된 항목 수})/(\\text{현재 설정된 해시 맵의 최대 버킷 수})\\]Q&amp;A왜 loadFactor = 0.75가 기본값일까? 성능과 메모리의 균형을 갖춘 값입니다. 0.75는 일반적으로 충돌을 최소화하면서도 메모리 사용을 적절히 유지하는 값으로 알려져 있다고 합니다. 너무 낮은 loadFactor를 설정하면 리해싱이 자주 발생해서 성능이 떨어질 수 있고, 너무 높은 loadFactor를 설정하면 충돌이 자주 발생하여 탐색 속도가 느려질 수 있습니다.왜 ArrayList가 아니라 Array를 사용하는가?어차피 Rehashing과정이 있다면, 이를 Dynamic Array인 ‘ArrayList’를 사용하면 편하지 않을까요? ArrayList는 동적(dynamic)으로 크기를 조정하면서 빈 공간을 유지할 수 있습니다. 이는 메모리 낭비를 초래할 수 있습니다. 반면에, Array는 고정된 크기의 메모리를 사용하므로, HashMap에서는 메모리 할당을 보다 효율적으로 관리할 수 있습니다. HashMap에서도 ArrayList와 같이 메모리 재할당이 이루어지긴 합니다.하지만, ArrayList에서는 기존의 순서대로 Array를 복사하는 반면, HashMap에서는 Bucket이 들어가야 하는 index 위치도 바꿔줍니다(key값을 기준으로 Hashing을 다시 진행하기 때문에).이는, 기존의 Linked List 형태로 있던 데이터를 고르게 분포시키는 효과가 있습니다.ConclusionHashMap의 간단한 동작 과정을 정리하며 마무리 합니다. 해시 계산: 입력된 키에 대해 해시값을 계산합니다. 배열 인덱스 계산: 해시값을 배열의 인덱스로 변환하여, 어느 위치에 저장할지 결정합니다. 버킷 저장: 배열의 해당 인덱스에 데이터를 저장하고, 충돌이 발생하면 연결 리스트 또는 트리로 관리합니다. 리해싱: 배열이 꽉 차면 리해싱을 통해 배열 크기를 두 배로 늘리고 데이터를 재배치합니다.References Kotlin의 HashMap Spec확인했던 곳 HashMap - Kotlin Programming Language Kotlin Source Code https://github.com/JetBrains/kotlin/blob/09273fdd828836a1c45dcbb877b2aa48ef9da80a/libraries/stdlib/src/kotlin/collections/AbstractMap.kt 또 다른 Kotlin구현 example https://github.com/darian-catalin-cucer/HashMap" }, { "title": "Hash Table, Hash Map, Hash Set", "url": "/posts/HashTable-HashMap-HashSet/", "categories": "Programming, Computer Basics", "tags": "Computer Science, programming, Data Structure, algorithm", "date": "2024-10-17 16:13:00 +0900", "snippet": "OverviewHash를 이용한 자료구조는 Time Complexity를 줄이는데 아주 중요한 역할을 합니다.여기서는, Hash를 이용한 Data Structure를 소개합니다.또한, Hash의 Time Complexity는 $O(1)$(insert, delete, search)로 알려져 있는데, 어떻게 이게 가능한지 알아보려 합니다. 단순하게 생각...", "content": "OverviewHash를 이용한 자료구조는 Time Complexity를 줄이는데 아주 중요한 역할을 합니다.여기서는, Hash를 이용한 Data Structure를 소개합니다.또한, Hash의 Time Complexity는 $O(1)$(insert, delete, search)로 알려져 있는데, 어떻게 이게 가능한지 알아보려 합니다. 단순하게 생각하면, key의 hash값이 일치하는지 확인하는것도 $O(n)$이 될것 같은데, $O(1)$로 소개되는, 관련된 내용도 다룹니다.Hash와 관련된 이유Hash를 이용한 Data structure은 table, map, set등 다양합니다.이들의 성능은 모두 ‘hash function’과 깊게 연관되어 있습니다. A huge amount of the performance of a hash table depends on the quality of your hash functions.어떤 ‘hash function’을 쓰느냐에 따라, 세부적인 성능이 달라 지지만, 그 내부에 있는 idea는 같습니다.Key값을 Hash처리하는 이유.Key값으로 보통 String데이터가 주어지는데, 이를 그대로 사용하지 않고, hash function을 사용하는 이유가 뭘까요?이는 높은 성능을 내기위한 목적과, 일관된 성능을 제공하기 위함입니다. 높은 성능? 만약, key로 주어진 String을 그대로 이용한다면, String을 구성하는 각 character를 모두 비교해야 합니다. 이는 100% 일치하는지 확인하는 방법이긴 하지만, 문자열 길이에 비례하는 $O(n)$의 Time Complexity를 갖게 됩니다. Hash Data structure에 사용되는 Hash function들은 String을 받으면, Integer값을 return합니다. 이 값은, HashMap이 갖고 있는 데이터의 위치를 나타내는 index가 됩니다. 이는 key값을 비교할때, ‘Integer 비교(comparison)’가 가능하게 하며, 더 나은 ‘비교 성능($O(1)$)’과 더 적은 메모리를 사용하게 합니다. 대표적인 Hash함수로 ‘siphash’가 있습니다. 현재 rust와 cpp에서 hashmap을 구성할때 사용하고 있습니다. 일관된 성능? Hash함수는 데이터 분배에 영향을 미칩니다. 다량의 데이터가 Hash table에 들어가야 한다면, hash결과에 따라, 데이터가 균일하게 bucket(key값에 대응하는 데이터들이 보관되는 곳)에 배치되도록 합니다. 이는 검색, 삽입, 삭제 속도를 일정하게 유지(key에 따른 성능뿐만 아니라 전체적인 성능도)해주는 역할을 합니다.Hash를 이용한 자료구조에서, Hash함수는 다음을 반드시 만족해야 합니다.k1 == k2 -&gt; hash(k1) == hash(k2)Hash Collision 문제Hash는 input값을 encoding하기 때문에, 서로 다른 값인데, 같은 hash값을 가질 수 있습니다. Hash의 결과값은 범위가 한정되어 있는 반면에, 입력값은 훨씬 다양하기 때문에 발생하게 됩니다.이를 수식으로 표현하면 다음과 같습니다.k1 != k2 -&gt; hash(k1) == hash(k2) Collision이 발생하는 경우, ‘Chaining’이라는 방법으로 중복된 hash값 끼리 묶어서 저장(같은 bucket에 저장). ‘Open Addressing’이라는 방법으로 빈 bucket에 데이터를 저장.이런 방법이 사용됩니다. 이 Collision의 발생을 줄이려면, 값의 범위가 더 큰 hash function을 사용하거나, 값의 분포가 더 균등하게 발생하는 hash function을 사용허간, Hashmap의 사이즈를 결정하는 요소들을 수정합니다.Hash를 이용한 Data StructureHashTable 혹은 HashMapJava hashmap from https://oshyshkov.com/2021/07/07/java-hashmap-implementation/HashMap은 주어진 key값을 Hash function을 통해 integer key로 변환합니다.이 integer key를 기반으로 탐색, 삽입을 수행합니다. HashTable과 HashMap은 구현 세부사항에 약간의 차이는 있지만, Data Structure관점에선 동일하게 취급합니다. Java에서는 HashTable과 HashMap이 별도로 있습니다.HashTable은 Synchronous(동기)로서 일관된 데이터를 보장하지만 성능은 떨어집니다(병렬 처리 어려움).HashMap은 Ansynchronous(비동기)로서 더 나은 성능을 발휘하지만, 데이터 일관성은 떨어집니다. null값을 저장할 수 있습니다. Rust에서 HashMap은 SIMD lookup을 지원합니다.A hash map implemented with quadratic probing and SIMD lookup.HashSet데이터의 중복을 제거하는데 많이 사용하는 hashSet입니다.hash값을 기준으로, unique한 데이터를 저장합니다.Java와 rust에선, Hashmap을 기반으로 구현되어 있습니다(value가 공백인 형식, key값만 이용.). A hash set implemented as a HashMap where the value is (). from rust doc 다양한 Hash Function과 Pros &amp; ConsHash를 사용한 Data Structure에서, Hash함수를 바꿈으로서 성능을 개선할 수 있습니다. SipHash 1-3 ‘medium sized key’에서 좋은 성능을 내며, ‘HashDos’공격에 대한 대비가 되어 있는 function입니다. 기본적으로 host에서 생성된 random seed를 사용하여 보안성을 갖춥니다. 암호화를 갖춘 hash function들 중에서는 빠른 특징이 있습니다.(murmur보다는 느립니다) While its performance is very competitive for medium sized keys, other hashing algorithms will outperform it for small keys such as integers as well as large keys such as long strings, though those algorithms will typically not protect against attacks such as HashDoS. MurmurHash 낮은 ‘collision’을 자랑하고, 데이터 분포를 고르게 만들 수 있습니다(다량의 데이터에 대한 hashmap성능을 최적화 할때 좋음). key값이 커도, 빠르게 hash처리할 수 있습니다(성능이 좋습니다). seed값을 사용하지 않아, 보안에 취약한 모습을 보입니다.(공격자가 의도적으로 collision을 발생시킬 수 있음) SHA-256 SHA-256은 보안성을 갖춘 해시 함수로, 주로 암호화와 관련된 환경에서 많이 사용됩니다. 보안이 중요한 해시 작업에 적합합니다. 계산 방법이 매우 복잡하지만, 충돌가능성이 매우 낮습니다. 하지만, 성능이 느려서 HashMap에 사용하기에는 적합하지 않을 수 있습니다. MD5 이전에는 많이 사용했지만, 충돌이 발생하는 경우를 쉽게 찾을 수 있어(빈번히 발생할 수 있어) 사용되지 않습니다.Hash관련 취약점HashDoS Attack공격자가 의도적으로 Hash Collision을 발생시키는 공격입니다.의도적으로 Hash Collision을 발생시켜, 특정 Bucket에 데이터가 몰리게하고, 이 데이터를 선형탐색(linear probe)하도록 하여 컴퓨터 자원을 차지하게 만듭니다.이에 대응하여(해결하기 위하여), SipHash가 등장했습니다.ConclusionHash Data Structure은 Hash Function과 깊게 관련되어 있으며, 그 목적과 용도에 맞게 내부 hash function을 변경하여 사용할 수 있습니다.$O(1)$이라는 Time Complexity가 가능한것도, Hash Function이 내놓는 결과가 Integer이기 때문에, 가능한 결과입니다.다만, Collision이 많이 발생하는 Hashmap인 경우 성능이 감소할 수 있지만, 데이터 분포가 잘 되는 Hash function을 사용한다면 문제를 최소화 할 수 있습니다.References HashMap의 구현체중 하나인 SwissTable에 대한 소개 영상 CppCon 2017: Matt Kulukundis “Designing a Fast, Efficient, Cache-friendly Hash Table, Step by Step” Rust의 hashmap HashMap in std::collections - Rust HashTable은 프로그래머의 기본기라고 소개하는 영상 Hash Table은 프로그래머의 기본기 Java hashmap Implementation Java HashMap implementation Basics of Hash Tables Basics of Hash Tables Tutorials &amp; Notes Hash collision을 이용한 HashDos 공격 Collision attack Collisions for Hash Functions MD4, MD5, HAVAL-128 and RIPEMD Collisions for Hash Functions MD4, MD5, HAVAL-128 and RIPEMD SwissTable abseil / Swiss Tables and absl::Hash" }, { "title": "Imperative programming(명령형 프로그래밍)과 Declarative programming(선언형 프로그래밍)", "url": "/posts/Imperative-programming-and-Declarative-programming/", "categories": "Programming, Computer Basics", "tags": "Computer Science, programming", "date": "2024-10-11 23:01:00 +0900", "snippet": "Computer Engineering을 할때, 개념적인 부분에 대한 이해 없이도 뭔가를 만들어낼 수 있습니다.하지만 이는 곧 한계에 부딪힙니다.‘Imperative programming’과 ‘Declarative programming’는 개념으로 들어서는 ‘아하..’하면서 이해는 되지만, 매번 체득되지 않는.. 그런것이었습니다.도서 ‘Designing...", "content": "Computer Engineering을 할때, 개념적인 부분에 대한 이해 없이도 뭔가를 만들어낼 수 있습니다.하지만 이는 곧 한계에 부딪힙니다.‘Imperative programming’과 ‘Declarative programming’는 개념으로 들어서는 ‘아하..’하면서 이해는 되지만, 매번 체득되지 않는.. 그런것이었습니다.도서 ‘Designing Data-Intensive Application’를 읽고, 이에 대해 깨닳은 바가 있어. 이를 공유하고자 합니다.What is it?Imperative programming과 Declarative programming은 Computer에게 ‘명령을 내리는 방법’에 대한 얘기입니다.‘Imperative’와 ‘declarative’는 서로 추구하는바가 다르고, 용도가 다릅니다.Imperative programming(명령형 프로그래밍)‘Imperative’는 결과를 얻기 위한 과정을 단계별로 직접 작성하는 방법입니다. An imperative language tells the computer to perform certain operations in a certain order. You can imagine stepping through the code line by line, evaluating conditions, updating variables, and deciding whether to go around the loop one more time.우리가 사용하는 java, c, python, javascript,… 등등이 이에 해당합니다.예를 들면,아래와 같이, ‘animals’ array에서 shark를 찾아내기 위해, 각 단계를 programming합니다.function getSharks() {\tvar sharks = [];\tfor (var i = 0; i &lt; animals.length; i++) {\t\tif (animals[i].family === \"Sharks\") {\t\t\tsharks.push(animals[i]);\t\t}\t}\treturn sharks;} 흔히, 컴퓨터가 작동하는 방식과 같다고 얘기합니다.Declarative programming(선언형 프로그래밍)‘Declarative’는 이미 약속된 interface로 원하는 결과물을 선언(Declarative)하는 방식입니다. you just specify the pattern of the data you want—what conditions the results must meet, and how you want the data to be transformed (e.g., sorted, grouped, and aggregated)—but not how to achieve that goal.대표적으로 SQL, HTML, XSL 등이 이에 해당합니다.예를 들면,아래와 같이 SQL로 기대하는 결과물을 작성할 수 있습니다.SELECT * FROM animals WHERE family = 'Sharks';혹은 HTML과 CSS 예시가 있습니다&lt;ul&gt;\t&lt;li class=\"selected\"&gt;\t\t&lt;p&gt;Sharks&lt;/p&gt;\t\t&lt;ul&gt;\t\t\t&lt;li&gt;Great White Shark&lt;/li&gt;\t\t\t&lt;li&gt;Tiger Shark&lt;/li&gt;\t\t\t&lt;li&gt;Hammerhead Shark&lt;/li&gt;\t\t&lt;/ul&gt;\t&lt;/li&gt;\t&lt;li&gt;\t\t&lt;p&gt;Whales&lt;/p&gt;\t\t&lt;ul&gt;\t\t\t&lt;li&gt;Blue Whale&lt;/li&gt;\t\t\t&lt;li&gt;Humpback Whale&lt;/li&gt;\t\t\t&lt;li&gt;Fin Whale&lt;/li&gt;\t\t&lt;/ul&gt;\t&lt;/li&gt;&lt;/ul&gt;li.selected &gt; p { background-color: blue; }여기서 CSS code는 특정 DOM element(HTML을 구성하는 기본요소)의 ‘background-color’를 지정하고 있습니다.‘background-color’를 바꾸는 과정에 대한 구체적인 지시 없이, 시스템이 이를 이해하여 결과물을 만듭니다.‘Imperative programming’과 ‘Declarative programming’ 비교 Imperative programming는 computer에게 보다 구체적으로 명령을 내릴 수 있습니다. 대부분의 programming language가 이에 해당하며, 작성된 language에 dependency(의존성)를 갖습니다. 그래서, 각 programming language별로 별도로 programming해야 합니다. Declarative programming는 결과물에 이르는 ‘세부 과정(implementation details)’을 숨기고, 시스템이 이를 해석해서 작동합니다. 특정 programming language에 dependency가 걸리지 않으며, ‘Declarative Language’를 이해하도록 language상관없이 구현할 수 있습니다. Language에 대해 ‘보편적인’ Optimizer를 통해 성능을 끌어올릴 수 있습니다. 이는 ‘imperative’에서 각 로직에 맞게 최적화 해줘야하는것과는 달리, 보편적인 optimizer를 통해 성능 최적화를 할 수 있습니다. it also hides implementation details of the database engine, which makes it possible for the database system to introduce performance improvements without requiring any changes to queries. ‘parallel execution(병렬 실행)’구현이 상대적으로 쉽습니다(구현이 상대적으로 자유롭기 때문에). 이는 ‘실행에 순서가 있어’ 병렬실행 ‘구현’이 어려운 ‘Imperative’와 두드러지는 차이점입니다. declarative languages often lend themselves to parallel execution. References Imperative Programming Imperative programming" }, { "title": "LeetCode 75-23 Equal Row and Column Pairs by python", "url": "/posts/python-Equal_row_and_column_pairs/", "categories": "Programming, python, leetcode75", "tags": "programming, python, leetcode", "date": "2024-10-09 19:00:00 +0900", "snippet": "문제 요약interger로 이루어진 2차원 배열이 주어지는데, 여기서 row 전체와 column전체가 일치하는 경우의 수를 구하는 문제입니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n^2)$입니다. 여기서 n은 gridmatrix의 1차원(row혹은 column) 크기 입니다. 이는 grid를 전체 순회...", "content": "문제 요약interger로 이루어진 2차원 배열이 주어지는데, 여기서 row 전체와 column전체가 일치하는 경우의 수를 구하는 문제입니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n^2)$입니다. 여기서 n은 gridmatrix의 1차원(row혹은 column) 크기 입니다. 이는 grid를 전체 순회해야하기 때문입니다(column의 배열을 별도로 만들어야 하기 때문.).공간복잡도(space complexity) 는 $O(n)$입니다. grid의 row를 순회하며 frequency hashmap을 생성하기 때문입니다.먼저 row를 순회하며, interger pattern을 파악하고, frequency를 기록합니다.이때, row의 pattern을 hashmap의 key값으로 사용합니다.이후 column을 순회하며 hashmap에 이미 있는 pattern인지 확인한 후 기록합니다.column에 대한 key를 만들기 위해, 별도의 buffer array를 사용하여, 매번 array가 생성되는것을 방지하였습니다.rust code와 다른점은, hashmap의 key를 string으로만 설정할 수 있어, ‘type casting(형 변환, 타입 변환)’을 추가하였습니다.from typing import Listdef equalPairs(grid: List[List[int]]) -&gt; int: n = len(grid) frequency_dict = dict() for row_idx in range(0, n): row_str = ','.join(map(str, grid[row_idx])) if row_str in frequency_dict: frequency_dict[row_str] += 1 else: frequency_dict[row_str] = 1 count = 0 buffers = [''] * n for col_idx in range(0, n): for row_idx in range(0, n): buffers[row_idx] = str(grid[row_idx][col_idx]) key = ','.join(buffers) if key in frequency_dict: count += frequency_dict[key] * 1 return countassert equalPairs([[3,2,1],[1,7,6],[2,7,7]]) == 1assert equalPairs([[3,1,2,2],[1,4,4,5],[2,4,2,2],[2,4,2,2]]) == 3결과Leetcode 제출 결과ReferencesEqual Row and Column Pairs - LeetCode" }, { "title": "LeetCode 75-23 Equal Row and Column Pairs by Rust", "url": "/posts/rust-Equal_row_and_column_pairs/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-10-09 18:24:00 +0900", "snippet": "문제 요약interger로 이루어진 2차원 배열이 주어지는데, 여기서 row 전체와 column전체가 일치하는 경우의 수를 구하는 문제입니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n^2)$입니다. 여기서 n은 gridmatrix의 1차원(row혹은 column) 크기 입니다. 이는 grid를 전체 순회...", "content": "문제 요약interger로 이루어진 2차원 배열이 주어지는데, 여기서 row 전체와 column전체가 일치하는 경우의 수를 구하는 문제입니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n^2)$입니다. 여기서 n은 gridmatrix의 1차원(row혹은 column) 크기 입니다. 이는 grid를 전체 순회해야하기 때문입니다(column의 배열을 별도로 만들어야 하기 때문.).공간복잡도(space complexity) 는 $O(n)$입니다. grid의 row를 순회하며 frequency hashmap을 생성하기 때문입니다.먼저 row를 순회하며, interger pattern을 파악하고, frequency를 기록합니다.이때, row의 pattern을 hashmap의 key값으로 사용합니다.이후 column을 순회하며 hashmap에 이미 있는 pattern인지 확인한 후 기록합니다.column에 대한 key를 만들기 위해, 별도의 buffer array를 사용하여, 매번 array가 생성되는것을 방지하였습니다.use std::collections::HashMap;fn equal_pairs(grid: Vec&lt;Vec&lt;i32&gt;&gt;) -&gt; i32 { let n = grid.len(); let mut frequency_table = HashMap::new(); for row in &amp;grid { frequency_table.entry(row).and_modify(|counter| *counter += 1).or_insert(1); } let mut count = 0; // let mut buffer_vec: Vec&lt;i32&gt; = Vec::with_capacity(n); let mut buffer_vec: Vec&lt;i32&gt; = vec![0; n]; buffer_vec.fill(0); for col_idx in 0..n { for row_idx in 0..n { buffer_vec[row_idx] = grid[row_idx][col_idx]; } let row_count = *frequency_table.get(&amp;buffer_vec).unwrap_or(&amp;0); if row_count &gt;= 1 { count += row_count * 1; } } return count;}fn main() { assert_eq!(equal_pairs(Vec::from([Vec::from([3,2,1]), Vec::from([1,7,6]), Vec::from([2,7,7])])), 1); assert_eq!(equal_pairs(Vec::from([Vec::from([3,1,2,2]), Vec::from([1,4,4,5]), Vec::from([2,4,2,2]), Vec::from([2,4,2,2])])), 3);}결과Leetcode 제출 결과ReferencesEqual Row and Column Pairs - LeetCode" }, { "title": "How does work 'update' on MongoDB internally?", "url": "/posts/How-does-work-update-on-MongoDB-internally/", "categories": "Books, Designing Data-Intensive Applications", "tags": "Architecture, MongoDB, DocumentDB", "date": "2024-10-06 02:29:00 +0900", "snippet": "책 ‘Designing Data-Intensive Applications’을 보면서, 궁금한것들이 많아집니다.이 책이 2017년 초판이 인쇄 되었는데, 제가 쓰는 기술들에 실제로 적용 되는지 궁금해집니다.이 글은 그중 하나인, ‘Mongodb(Document db)에서 update는 disk의 rewrite를 발생시키는가?’에 대한 글 입니다.Over...", "content": "책 ‘Designing Data-Intensive Applications’을 보면서, 궁금한것들이 많아집니다.이 책이 2017년 초판이 인쇄 되었는데, 제가 쓰는 기술들에 실제로 적용 되는지 궁금해집니다.이 글은 그중 하나인, ‘Mongodb(Document db)에서 update는 disk의 rewrite를 발생시키는가?’에 대한 글 입니다.OverviewMongodb와 같은 Document DB는 별도의 schema가 없습니다(정확하게는, write할때 schema가 없습니다). 이는 ‘Relational Database(이하 RDB)’와 같이 정해진 schema가 있는 형태와 가장 큰 차이점 입니다때문에, Document를 update할 때, 고려해야할 문제가 한가지 더 생깁니다.‘Update한 document의 size가 original보다 크다면?’기존에 차지하던 disk공간에 update된 document를 넣을 순 없습니다. 크기가 다르니까요.여기서는 ‘Mongodb의 경우 어떻게 처리되는지’ 확인한 내용을 정리하여 다루고자 합니다.Default StorageEngine인 Wiredtiger 이해하기WiredTiger는 Mongodb의 default storage engine입니다.Mongodb에서 ‘write’는 ‘Checkpoint’와 ‘Journaling’을 기반으로 작동합니다.또한, Disk상에서의 데이터는 ‘Block’으로 다루어 집니다.Block‘Block’은 ‘Wiredtiger’에서 disk에 있는 데이터를 다루는 단위입니다. block is a chunk of data that is stored on the disk and operated on as a single unit.‘Wiredtiger’기반의 모든 file들은 이 ‘Block’으로 구성되어 있습니다.WiredTiger Block Layout(https://source.wiredtiger.com/develop/arch-block.html#block_what)Snapshot and CheckpointWiredTiger는 특정 record에 대한 write작업 시작시, 데이터에 대한 스냅샷(snapshot)을 만듭니다.이 ‘snapshot’에 대하여 write작업을 적용하고, Disk에 쓰기작업까지 모두 완료되면, 이 변경사항을 기반으로하는 ‘Checkpoint’를 생성합니다.WiredTiger: Snapshot(WiredTiger에서 snapshot을 어떤 개념으로 사용하고 있는지 알 수 있는 페이지입니다.)Journaling‘Journaling’은 ‘기록을 남기는것’을 의미합니다.DB에서의 ‘Journaling’은 Data에 변화(delta값)를 기록하는것을 말합니다.‘Checkpoint’사이의 데이터 변화를 기록합니다.이때 장애가 발생하면, 이 ‘journal record’를 기준으로 복구합니다. 이러한 system을 ‘Journaling file system’이라고 합니다Journaling Process에 따라 작동합니다.Client가 DB에 대한 ‘write’를 발생시키면, 가장 최신의 ‘Checkpoint’를 기준으로 data의 ‘snapshot(in-memory)’을 생성합니다. 해당 작업에 대한 ‘journal record’를 생성합니다. 이후 추가되는 write작업이 ‘journal record’에 buffering 됩니다(쌓입니다). 이후 특정 record에 대한 update가 발생하면, snapshot에 반영합니다.이를 ‘journal record’에 기록합니다. 특정 주기나 특정 조건을 만족하면, 이 ‘journal record’를 Disk에 씁니다(’journal commit’).이 data는 장애시 복구를 위해 사용합니다. 특정 주기나 조건을 만족하면, snapshot을 Disk에 작성합니다. snapshot이 반영된 새로운 ‘Checkpoint’를 생성합니다. 완료.이런 과정을 반복하면서, Disk에 여러 ‘Checkpoint’가 생기게 됩니다.Compaction‘Checkpoint’는 여러 disk block을 참조합니다. Each checkpoint references a certain set of disk blocks for a table.‘Checkpoint’가 쌓이면서, disk block또한 늘어나게 됩니다. 이에 따라, 이제 사용하지 않는 old block을 정리하고, table을 탐색하는데 사용하는, B-Tree를 새롭게 구축하게 됩니다.이 과정을 ‘Compaction’이라고 합니다.Conclusion(결론)Data를 Update하는 경우, 항상 disk에 Document전체 데이터가 새로 써집니다. 기존의 block을 copy하여, update를 적용한 새로운 Block을 생성합니다.이렇게 생성된 새로운 Block은 Table을 구성하는 ‘B-Tree’에 있던 기존의 Block Reference를 대체하게 됩니다.이 ‘B-Tree’를 기반으로 새로운 ‘Checkpoint’를 생성하게 됩니다.References WiredTiger Storage Engine WiredTiger Storage Engine Mongodb에서의 Write Operation Write Operation Performance Journaling and the WiredTiger Storage Engine Journaling journal Glossary [Issue] Add interface allowing partial updates to existing values Issue Amazon DocumentDB vs MongoDB 의 내부 아키텍쳐 와 장단점 비교 Amazon DocumentDB vs MongoDB 의 내부 아키텍쳐 와 장단점 비교 WiredTiger의 Block WiredTiger: Block Manager WiredTiger의 Transaction using Snapshots WiredTiger: Snapshot WiredTiger의 B-Tree WiredTiger: B-Trees WiredTiger의 Compaction WiredTiger: Compaction" }, { "title": "Ensemble Learning- XGBoost", "url": "/posts/XGboost/", "categories": "ML, EnsembleLearning", "tags": "ML, EnsembleLearning, XGBoost", "date": "2024-09-30 14:53:00 +0900", "snippet": "What is XGBoost?Decision Tree의 발전 역사 Decision Trees Bagging(Bootstrap aggregating)sample 데이터를 랜덤으로 선택하여 여러개의 ‘bootstrap’으로 묶고, 이 ‘bootstrap’으로 모델을 학습시키는 방법. sample 데이터의 다양성을 확보하는것.규칙적으로 sampl...", "content": "What is XGBoost?Decision Tree의 발전 역사 Decision Trees Bagging(Bootstrap aggregating)sample 데이터를 랜덤으로 선택하여 여러개의 ‘bootstrap’으로 묶고, 이 ‘bootstrap’으로 모델을 학습시키는 방법. sample 데이터의 다양성을 확보하는것.규칙적으로 sample 데이터의 일부를 제외하는 ‘K-fold’와는 다릅니다. Random ForestBagging을 기초로 하지만, Bagging에서 feature를 random하게 subset으로 만들어서 활용합니다. Adaptive Boosting여러 모델을 만들어 내는데, 각 모델을 sequential하게 만들어집니다. 이 과정에서, 계속 틀리는(못 푸는) sample데이터에 가중치를 줘서, 뒤로 갈 수록 해당 sample에 대해 강화된 모델을 만들도록 하는 방법입니다. Gradient Boosting모델이 만들어낸 ‘residual(잔차, $\\hat{y}$과 $y$의 차이)’에 대해서 sequential하게 학습하여 여러 모델을 만들어내는 방법.‘residual’에 대해서 학습하기 때문에, ‘overfit(과적합)’가능성이 높습니다. 때문에, 여러 ‘regularization’을 이용하게 됩니다. XGBoost“‘Gradient Boosting’을 좀 더 빠르게, 대용량 데이터에 대해서 대응되게 돌릴 수 있을까?”를 기본으로 만들어진 ‘Gradient Boosting’의 최적화(optimized) 모델입니다.XGBoost는 ‘Gradient Boosting Algorithm’입니다.기존과 다르게, Split Finding Algorithm(병렬처리에 대한 얘기)을 사용합니다.XGBoost에서 하고자 하는것.Decision Tree에서의 문제 개선.Decision Tree같은 경우, ‘Basic exact greedy algorithm’로서 반드시 최적의 결과물(optimal point)를 찾아냅니다.(모든 가능성을 확인하기 때문에) 하지만, 데이터 자체가 메모리에 모두 들어가지 못하는 경우, 알고리즘을 사용하지 못하는 문제가 있으며, 모든 가능성을 탐색해야 하므로, 분산(distributed)환경에서 처리가 불가합니다.그래서, ‘Approximate algorithm’을 사용합니다.Split Finding Algorithm‘decision tree’에선, 모든 example 사이에 ‘split point(분할 point. left node, right node로 분할하는 point)’를 두고, 어떤게 최적의 ‘split point’인지 찾아 냅니다. 즉 전수조사를 합니다.‘XGBoost’에선 이를 개선하여, bucket(example을 특정 갯수만큼 묶은것)을 적용합니다.bucket안에서, bucket 내부의 데이터만 가지고, ‘split point’를 찾습니다. split point를 찾기 전에, 모든 examples을 ascending으로 정렬해야 합니다.여기서, bucket 내부의 ‘split point’를 찾는 과정이 bucket단위로 이루어지기 때문에, bucket단위로 병렬처리가 가능합니다. 이런 bucket방식은 ‘approximation’ 접근이기 때문에(모든 경우에수에 대해서 고려하는게 아니기 때문에) 정확한 기준(split point)이 아닐 수 있습니다.split 단위split의 기준을 global 혹은 local로 설정할수도 있습니다. Per Tree(Global variant)root node에서부터 leaf node에 이르기까지, bucket의 size가 동일합니다.(어쩔 수 없이 잘려야 하는경우는 제외) Per Split(Local variant)root의 bucket 갯수를 leaf node에 이르기까지 유지합니다.root node에서 depth가 증가하면서, ‘percentile’에 따라 다시 bucket을 구분합니다.depth가 깊어질수록, 하나의 bucket에 들어가는 example의 수가 줄어듭니다. epsilon과 local, global variant의 비교local와 global중 어느게 더 좋은지는 판단하기 어렵다.다만, global을 사용하는 경우, epsilon을 작게 가져가야 한다.Sparsity-Aware Split Finding현실세계의 데이터에는 변수가 많습니다. 특정 데이터(feature value)가 비어 있는 경우.(이런 데이터를 ‘missing data’라고 합니다.) 혹은, 0이 너무 자주 출현하는 data인 경우.(one-hot vector 같은)이와 같은 경우, decision tree에서 어느 방향으로 가야할지 모호합니다.이를 해결하기 위해, tree를 정의할때 기본 방향(default direction)을 설정하고 이 방향으로 보내도록 합니다. 위 이미지의 파랑색: ‘missing value’를 모두 오른쪽으로 몰고 정렬합니다. 이를 기준으로 ‘best split point’를 찾습니다. 붉은색: ‘missing value’를 모두 왼쪽으로 몰고 정렬합니다. 이를 기준으로 ‘best split point’를 찾습니다.이 결과, 우측의 특정 feature에 대한 예시같은 경우, 왼쪽 정렬이 가장 나은 결과임을 알 수 있고, 이 ‘왼쪽’이 default direction이 됩니다. 이렇게, X1 example은 age feature에 대해서는 default direction에 따라 ‘왼쪽’으로 X2 example은 gender feature에 대해서 missing value 상태이므로, default direction에 따라 ‘오른쪽’으로 이동합니다.우측의 그래프는 ‘Sparsity aware algorithm’을 적용하는것이 상당한 성능 개선이 있음을 보여줍니다.System Design for Efficient Computing이 Decision Tree Learning을 하는데 가장 time-consuming한것이 뭘까?각 feature의 데이터들을 정렬하는것입니다.이는 최적의 알고리즘(Quick sort)을 사용하더라도, $O(n\\log{n})$의 비용이 들며, example의 size에 비례합니다.하지만, 이 ‘정렬’은 데이터 전처리(pre-processing)과정의 문제이기 때문에, 최초 1회 시행되면 되고, 실제 learning에는 영향을 미치지 않습니다.CPU의 hardware cache사이즈에 따라, block사이즈를 조절하여 성능개선 효과를 얻을 수 있습니다.References 고려대학교 일반대학원 산업경영공학과 강의 04-7: Ensemble Learning - XGBoost (앙상블 기법 - XGBoost) 강의 자료 origin paper XGBoost: A Scalable Tree Boosting System" }, { "title": "Ensemble Learning Overview", "url": "/posts/Ensemble-Learning/", "categories": "ML, EnsembleLearning", "tags": "ML, EnsembleLearning", "date": "2024-09-28 18:23:00 +0900", "snippet": "What is Ensemble Learning?다양한 알고리듬(algorithm)을 결합하여, 개별 알고리듬보다 더 우수한 성능을 만들어내는 방법입니다.이 자체가 알고리듬은 아니고, 섞는(ensemble) 방법에 관한 얘기입니다.Background(탄생 배경)No Free Lunch Theorem(공짜 점심 같은건 없다.) Every algorith...", "content": "What is Ensemble Learning?다양한 알고리듬(algorithm)을 결합하여, 개별 알고리듬보다 더 우수한 성능을 만들어내는 방법입니다.이 자체가 알고리듬은 아니고, 섞는(ensemble) 방법에 관한 얘기입니다.Background(탄생 배경)No Free Lunch Theorem(공짜 점심 같은건 없다.) Every algorithm scored best or next-to-best on at least two of the six data sets.‘No Free Lunch Theorem’는 ‘success(성공)으로 가는 지름길은 없다, 쉬운길은 없다.’는 의미를 담고 있는 수학계에서 전해지는 말입니다.‘Ensemble Learning’에선, ‘모든 문제에 대해서 최고의 결과물을 내는 하나의 알고리듬은 없다’를 의미합니다.때문에, 상황에 맞춰서 알고리듬을 선택하는것이 중요했습니다.여러 알고리듬이 있지만, 모든 dataset에서 최고의 성적을 기록하는 알고리듬은 없습니다.Motivation지금까지는, 상황에 맞는 알고리듬을 선택하는데 집중해 왔지만, 이 논문에선 아래와 같은 얘기를 하고 있습니다. 여러 알고리듬을 섞은(ensemble)한 결과가, 단일 알고리듬중 최고의 결과보다 더 좋다. 이는 실험적으로 발견된 현상이라고 합니다.Ensemble 기법인 average, vote..등 상관 없이 단일 모델보다 더 개선된 모습을 보여줍니다.실험 결과 및 Conclusion(결론)실험 결과, ‘Ensemble Learning’에 해당하는, ‘RF(Random Forest)’와 ‘BST-DT’와 같이 ensemble한 형태가 단일 모델 보다 모든 dataset에 대해 더 나은 결과를 내었습니다.References 고려대학교 일반대학원 산업경영공학과 강의 04-1: Ensemble Learning - Overview Ensemble Methods in Data Mining paper Ensemble Methods in Data Mining Ensemble Methods in Data Mining Paper: Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? ‘Ensemble Learning’을 엄청나게 많은 ‘classifier’에 대해 적용해서 이론을 확인하는 논문. jmlr.org" }, { "title": "LeetCode 75-14 Maximum Average Subarray 1", "url": "/posts/Maximum-Average-Subarray-1/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-26 17:14:00 +0900", "snippet": "문제 요약integer array인 nums안에서, k에 해당하는 길이만큼의 요소를 뽑습니다. 이때, 이 요소들의 평균값의 최대를 return합니다.즉, k길이의 subarray에 대한 average의 최댓값을 구합니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n)$입니다. 이는 nums를 2회 순회하지만...", "content": "문제 요약integer array인 nums안에서, k에 해당하는 길이만큼의 요소를 뽑습니다. 이때, 이 요소들의 평균값의 최대를 return합니다.즉, k길이의 subarray에 대한 average의 최댓값을 구합니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n)$입니다. 이는 nums를 2회 순회하지만, 각 순회가 독립적으로 이루어지 때문입니다.공간복잡도(space complexity) 는 $O(n)$입니다. 주어진 nums를 순회하며, 각 element의 합을 accumulation(축적, 누적)하여 별도의 array에 저장하기 때문입니다.평균은 각 요소의 합에 비례합니다. 때문에, subarray 요소의 합이 최대가 되면, 평균(average) 값이 최대가 됩니다.또한, ‘sum’의 값으로 데이터를 저장하고 있는게, 조금이나마 메모리 사용에 이점이 있습니다(실수인 f64는 64bit의 저장공간을 사용. 반면에 정수인 i32는 32bit의 저장공간을 사용.).subarray의 합을 구하기 위해, array를 1회 순회하며 각 단계마다 총합을 누적하여 별도의 array에 저장합니다.이는 아래의 수식을 통해 subarray의 합을 구하는데 사용됩니다.nums의 index n부터 m 까지의 subarray의 합은, 다음과 같은 수식으로 표현됩니다.\\[\\sum_{i=n}^{m}\\text{num}_i = \\sum_{i=0}^{m}\\text{num}_i - \\sum_{i=0}^{n-1}\\text{num}_i\\]이를 이용하여, subarrary의 합을 구하는 것을 단순화하고 최댓값을 찾습니다.fn find_max_average(nums: Vec&lt;i32&gt;, k: i32) -&gt; f64 { if nums.len() &lt;= 0 { panic!(\"num\\'s length must greater than 0.\") } else if nums.len() &lt;= 1 { return nums[0] as f64; } let mut sum_array: Vec&lt;i32&gt; = Vec::with_capacity(nums.len()); let mut sum_accu = 0; for element in &amp;nums { sum_accu += element; sum_array.push(sum_accu) } let mut max_sum: i32 = i32::MIN; for start_pointer in 0..=nums.len() - k as usize { let end_pointer = start_pointer + k as usize - 1; let start_sum = sum_array[start_pointer]; let end_sum = sum_array[end_pointer]; max_sum = std::cmp::max(end_sum - start_sum + nums[start_pointer], max_sum); } return max_sum as f64 / k as f64;}fn main() { assert_eq!(find_max_average(Vec::from([1,12,-5,-6,50,3]), 4), 12.75000); assert_eq!(find_max_average(Vec::from([5]), 1), 5.00000); assert_eq!(find_max_average(Vec::from([4,0,4,3,3]), 5), 2.80000); assert_eq!(find_max_average(Vec::from([8860,-853,6534,4477,-4589,8646,-6155,-5577,-1656,-5779,-2619,-8604,-1358,-8009,4983,7063,3104,-1560,4080,2763,5616,-2375,2848,1394,-7173,-5225,-8244,-809,8025,-4072,-4391,-9579,1407,6700,2421,-6685,5481,-1732,-8892,-6645,3077,3287,-4149,8701,-4393,-9070,-1777,2237,-3253,-506,-4931,-7366,-8132,5406,-6300,-275,-1908,67,3569,1433,-7262,-437,8303,4498,-379,3054,-6285,4203,6908,4433,3077,2288,9733,-8067,3007,9725,9669,1362,-2561,-4225,5442,-9006,-429,160,-9234,-4444,3586,-5711,-9506,-79,-4418,-4348,-5891]), 93), -594.58065);}결과Leetcode 제출 결과ReferencesMaximum Average Subarray 1 - LeetCode" }, { "title": "LeetCode 75-13 Max Number of K-Sum Pairs", "url": "/posts/Max-Number-of-K-Sum-Pairs/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-25 15:39:00 +0900", "snippet": "문제 요약integer array인 nums안에서, 그 합이 k를 만족하는 2개의 요소를 구합니다. 이때, k를 만족하는 요소는 중복으로 사용될 수 없습니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n\\log{k})$입니다. 이는 nums를 정렬(sort)하는 과정이 있기 때문입니다.공간복잡도(space...", "content": "문제 요약integer array인 nums안에서, 그 합이 k를 만족하는 2개의 요소를 구합니다. 이때, k를 만족하는 요소는 중복으로 사용될 수 없습니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n\\log{k})$입니다. 이는 nums를 정렬(sort)하는 과정이 있기 때문입니다.공간복잡도(space complexity) 는 $O(n)$입니다. 주어진 nums가 ‘immutable(불변)’ 상태임으로, nums를 copy하는 저장공간이 추가로 필요합니다.왼쪽(nums 시작점)에서 출발하는 pointer와 오른쪽(nums 끝)에서 출발하는 pointer, 총 2개의 pointer를 이용합니다.여기서, pointer를 변경하는 부분이 중요한데, 이 pointer변경을 단순하게 하기 위해서, ‘nums’를 오름차순으로 정렬(sort)해야 합니다. 정렬된 ‘nums’는 다음과 같은 조건을 만족합니다. left_pointer는 커질수록 element값도 커집니다. right_pointer는 작아질수록 element의 값도 작아집니다. 이에 따라, left_pointer와 right_pointer가 가리키는 element의 합(sum)을 k와 비교해서 pointer를 변경합니다. sum이 k보다 크면, 값을 작게 만들어야 합니다. 때문에, right_pointer를 왼쪽으로 이동합니다. sum이 k보다 작으면, 값을 크게 만들어야 합니다. 이에따라, left_pointer를 오른쪽으로 이동합니다.fn max_operations_v1(nums: Vec&lt;i32&gt;, k: i32) -&gt; i32 { let mut sorted_nums: Vec&lt;i32&gt; = nums.clone(); sorted_nums.sort(); let mut left_pointer: usize = 0; let mut right_pointer: usize = sorted_nums.len() - 1; let mut count = 0; while right_pointer &gt; left_pointer { let left_element = sorted_nums[left_pointer]; let right_element = sorted_nums[right_pointer]; let sum = left_element + right_element; if sum == k { count += 1; left_pointer += 1; right_pointer -= 1; } else if sum &lt; k { left_pointer += 1; } else { right_pointer -= 1; } } return count;}fn main() { assert_eq!(max_operations_v1(Vec::from([1,2,3,4]), 5), 2); assert_eq!(max_operations_v1(Vec::from([3,1,3,4,3]), 6), 1);}결과Leetcode 제출 결과두번째 시도Complexity시간복잡도(time complexity) 는 $O(n\\log{n})$입니다.공간복잡도(space complexity) 는 $O(n)$입니다.nums의 sort 알고리즘을 ipnsort를 사용하도록 바꾸었으며, nums의 길이가 0인 경우 함수가 빨리 종료되도록 변경하였습니다 ipnsort는 ‘quick sort’의 average case와 ‘heap sort’의 (빠른) worst case를 합친, 즉 장점을 합친 알고리즘이라고 합니다.rust 공식문서에서 ‘sort_unstable’ 참고fn max_operations_v2(nums: Vec&lt;i32&gt;, k: i32) -&gt; i32 { let mut sorted_nums: Vec&lt;i32&gt; = nums.into_iter().filter(|val| val &lt; &amp;k).collect::&lt;Vec&lt;i32&gt;&gt;(); if (sorted_nums.len()==0){ return 0 } sorted_nums.sort_unstable(); let mut left_pointer: usize = 0; let mut right_pointer: usize = sorted_nums.len() - 1; let mut count = 0; while right_pointer &gt; left_pointer { let left_element = sorted_nums[left_pointer]; let right_element = sorted_nums[right_pointer]; let sum = left_element + right_element; if sum == k { count += 1; left_pointer += 1; right_pointer -= 1; } else if sum &lt; k { left_pointer += 1; } else { right_pointer -= 1; } } return count;}fn main() { assert_eq!(max_operations_v2(Vec::from([1,2,3,4]), 5), 2); assert_eq!(max_operations_v2(Vec::from([3,1,3,4,3]), 6), 1);}결과Leetcode 제출 결과ReferencesMax Number of K-Sum Pairs - LeetCode" }, { "title": "LeetCode 75-12 Container With Most Water", "url": "/posts/Container-With-Most-Water/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-24 20:12:00 +0900", "snippet": "문제 요약integer array인 height안에서, 2개의 height를 골라 ‘물을 담을 수 있는 면적(2개의 height중 작은것 기준으로 측정해야함을 의미함.)’을 측정하여 가장 큰 면적을 return합니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n)$입니다. height array를 1회 순회...", "content": "문제 요약integer array인 height안에서, 2개의 height를 골라 ‘물을 담을 수 있는 면적(2개의 height중 작은것 기준으로 측정해야함을 의미함.)’을 측정하여 가장 큰 면적을 return합니다.문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n)$입니다. height array를 1회 순회합니다.공간복잡도(space complexity) 는 $O(1)$입니다. height의 크기와 비례해서 커지는 space는 없습니다.왼쪽(height 시작점)에서 출발하는 pointer와 오른쪽(height 끝)에서 출발하는 pointer, 총 2개의 pointer를 이용합니다.여기서, pointer를 변경하는 부분이 중요한데, 이때, 문제의 요구사항인 ‘maximum amount of water a container can store’를 기억해야 합니다. 즉, height가 클수록 좋습니다(면적이 최대가 되어야 하기 때문에).이에 따라, height가 작은 쪽의 pointer를 이동시킵니다.fn max_area(height: Vec&lt;i32&gt;) -&gt; i32 { let mut left_pointer: usize = 0; let mut right_pointer: usize = height.len() - 1; let mut max_area: i32 = 0; while right_pointer - left_pointer &gt; 0 { let area_width: i32 = (right_pointer - left_pointer) as i32; let area_height: i32 = std::cmp::min(height[left_pointer], height[right_pointer]); let area = area_width * area_height; max_area = std::cmp::max(area, max_area); if height[left_pointer] &lt;= height[right_pointer] { left_pointer += 1; } else { right_pointer -= 1; } } return max_area;}fn main() { assert_eq!(max_area(Vec::from([1,8,6,2,5,4,8,3,7])), 49); assert_eq!(max_area(Vec::from([1, 1])), 1); assert_eq!(max_area(Vec::from([2,3,10,5,7,8,9])), 36);}결과Leetcode 제출 결과ReferencesContainer With Most Water - LeetCode" }, { "title": "LeetCode 75-11 Is Subsequence", "url": "/posts/is_subsequence/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-22 14:48:00 +0900", "snippet": "문제 요약function의 parameter인 s와 t는 string을 받습니다.s는 t의 subsequence인지 확인해야 합니다.이 문제에서의 ‘subsequence’의 의미는 다음과 같습니다. a new string that is formed from the original string by deleting some (can be none) o...", "content": "문제 요약function의 parameter인 s와 t는 string을 받습니다.s는 t의 subsequence인지 확인해야 합니다.이 문제에서의 ‘subsequence’의 의미는 다음과 같습니다. a new string that is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. subsequence는 원문(original)에서 특정 문자를 삭제하면 만들어질 수 있는걸 말합니다. 즉 순서가 지켜져야 합니다. 예를 들면, “ace” is a subsequence of “abcde” while “aec” is not문제 풀이첫번째 시도Complexity시간복잡도(time complexity) 는 $O(n)$입니다.공간복잡도(space complexity) 는 $O(1)$입니다.s와 t를 각각 바라보는 cursor를 총 2개 운영하였으며, 조건을 만족하면, cursor의 위치가 바뀝니다.이때, t를 순회하는 동안, s를 모두 순회했는지 확인하여, 문제가 요구하는 결과값을 bool로 return합니다.fn is_subsequence(s: String, t: String) -&gt; bool { let mut s_cursor: usize = 0; let mut t_cursor: usize = 0; while s_cursor &lt; s.len() &amp;&amp; t_cursor &lt; t.len() { let s_char = &amp;s[s_cursor..s_cursor+1]; let t_char = &amp;t[t_cursor..t_cursor+1]; if s_char == t_char { s_cursor += 1; } t_cursor += 1; } if s_cursor &gt;= s.len() { return true; } return false;}fn main() { assert_eq!(is_subsequence(String::from(\"abc\"), String::from(\"ahbgdc\")), true); assert_eq!(is_subsequence(String::from(\"axc\"), String::from(\"ahbgdc\")), false); assert_eq!(is_subsequence(String::from(\"aaaaaa\"), String::from(\"bbaaaa\")), false);}결과ReferencesIs Subsequence - LeetCode" }, { "title": "LeetCode 75-10 Move Zeroes", "url": "/posts/move_zeroes/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-21 18:55:00 +0900", "snippet": "문제 요약주어진 nums의 ‘0’을 모두 우측으로 몰고, ‘0’이 아닌 값은 원래 있던 순서대로 두어야 합니다.문제 풀이첫번째 시도단순하게, ‘0’을 매우 큰 값으로 취급하여, array를 정렬하였습니다.Complexity시간복잡도(time complexity) 는 $O(n\\log{n})$입니다. ‘rust’의 array sort 기능을 이용해서 구...", "content": "문제 요약주어진 nums의 ‘0’을 모두 우측으로 몰고, ‘0’이 아닌 값은 원래 있던 순서대로 두어야 합니다.문제 풀이첫번째 시도단순하게, ‘0’을 매우 큰 값으로 취급하여, array를 정렬하였습니다.Complexity시간복잡도(time complexity) 는 $O(n\\log{n})$입니다. ‘rust’의 array sort 기능을 이용해서 구현했고, rust의 sort algorithm은 driftsort 이기 때문에, 이와 같은 time complexity를 갖습니다.공간복잡도(space complexity) 는 $O(1)$입니다.use std::cmp::Ordering;fn move_zeroes(nums: &amp;mut Vec&lt;i32&gt;) -&gt; &amp;Vec&lt;i32&gt; { nums.sort_by(|a, b| { if *a == 0 { return Ordering::Greater; } else if *b == 0 { return Ordering::Less; } return Ordering::Equal; }); return nums;}fn main() { assert_eq!(*move_zeroes(&amp;mut Vec::from([0,1,0,3,12])), [1,3,12,0,0]); assert_eq!(*move_zeroes(&amp;mut Vec::from([0])), [0]);}결과두번째 시도array를 역방향(마지막 부터 순회) 순회하여, ‘0’을 만나면, remove하고, 다시 push합니다.이전보다 메모리 사용량은 개선되었지만, 여전히 ‘time complexity’ 결과가 안 좋게 나옵니다.Complexity시간복잡도(time complexity) 는 $O(n^{2})$입니다. ‘rust’의 remove는 array의 element를 제거하고, 그 뒤를 잇는 나머지 element를 shifting(메모리 위치를 이동시킴)해야 합니다.때문에, nums순회 이외에, 그 길이 만큼 추가로 순회해야 합니다.공간복잡도(space complexity) 는 $O(1)$입니다.fn move_zeroes(nums: &amp;mut Vec&lt;i32&gt;) -&gt; &amp;Vec&lt;i32&gt; { let mut idx: usize = nums.len() - 1; while idx &gt;= 0 { let element = nums[idx]; if element == 0 { nums.remove(idx); nums.push(element); } if idx == 0 { break; } idx -= 1; } return nums;}fn main() { assert_eq!(*move_zeroes(&amp;mut Vec::from([0,1,0,3,12])), [1,3,12,0,0]); assert_eq!(*move_zeroes(&amp;mut Vec::from([0])), [0]); assert_eq!(*move_zeroes(&amp;mut Vec::from([0, 0, 1, 2, 3])), [1,2,3,0,0]);}결과세번째 시도‘0’의 위치를 기억하는 cursor 변수를 별도로 사용합니다(아래 코드에선, zero_pos).이후 nums를 순회하면서, ‘0’이 아닌 값을 만날때, 현재의 zero_pos에 위치한 값과 swap합니다.Complexity시간복잡도(time complexity) 는 $O(n)$입니다.nums를 1회 순회하기 때문입니다.공간복잡도(space complexity) 는 $O(1)$입니다.fn move_zeroes(nums: &amp;mut Vec&lt;i32&gt;) -&gt; &amp;Vec&lt;i32&gt; { let mut zero_pos: usize; match nums.iter().position(|num| *num == 0) { // 0인 값이 없으면, 함수 종료 Some(value) =&gt; { zero_pos = value; } _ =&gt; { // default 실행 return nums; } } if zero_pos &gt;= nums.len() - 1 { return nums; } for idx in zero_pos+1..nums.len() { let element = nums[idx]; if element != 0 { nums.swap(idx, zero_pos); zero_pos += 1; } } return nums;}fn main() { assert_eq!(*move_zeroes(&amp;mut Vec::from([0,1,0,3,12])), [1,3,12,0,0]); assert_eq!(*move_zeroes(&amp;mut Vec::from([0])), [0]); assert_eq!(*move_zeroes(&amp;mut Vec::from([0, 0, 1, 2, 3])), [1,2,3,0,0]); assert_eq!(*move_zeroes(&amp;mut Vec::from([2,1])), [2,1]);}결과ReferencesMove Zeroes - LeetCode" }, { "title": "LeetCode 75-9 String Compression", "url": "/posts/String-Compression/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-20 18:05:00 +0900", "snippet": "문제 요약주어진 chars(char array)를 압축(compression)합니다.이 chars는 여러 char의 group(반복되는)으로 이루어져 있으며, 이 반복되는 char를 압축하는 문제입니다.이때, If the group’s length is 1, append the character to s. group의 길이가 1이면,...", "content": "문제 요약주어진 chars(char array)를 압축(compression)합니다.이 chars는 여러 char의 group(반복되는)으로 이루어져 있으며, 이 반복되는 char를 압축하는 문제입니다.이때, If the group’s length is 1, append the character to s. group의 길이가 1이면, 그냥 char ‘s’ 하나만 붙입니다(숫자 없이. 1을 붙이지 않는다는 의미). Otherwise, append the character followed by the group’s length. 그렇지 않으면, char ‘s’ 뒤에 반복되는 숫자를 붙입니다. You must write an algorithm that uses only constant extra space 반드시 상수항(constant) 만큼의 공간(space)를 추가로 사용해야 합니다. Note that group lengths that are 10 or longer will be split into multiple characters in chars 반복되는 횟수가 10회 이상인경우, 이를 chars에 반영할때, 각 자리수를 별개의 char로 만들어서 넣어야 합니다. 문제 풀이첫번째 시도순회 방향의 문제처음에는 chars를 1회 ‘정방향’ 순회하면서 문제를 풀어낼 수 있다고 생각했습니다.이때의 문제는, ‘extra space’에 대한 제약사항 때문에 입력받은 chars를 직접 수정해야하는데, ‘정방향’으로 순회하며 수정하면, array가 계속 변경되어, 탐색이 어려워지는 문제가 있습니다.그래서 생각한게, ‘역방향’ 으로 순회하면 array의 뒷 부분만 변경되기 때문에, index를 활용한 순회가 좀 더 쉬워진다고 생각했습니다.chars를 변경하는 시점의 문제저는 char의 repeat을 count하고, 이를 chars에 반영하는 ‘시점’을 어떻게 해야할지 고민했습니다.문제의 요구사항을 참고하여, 역방향 탐색을 하면서, 바로 왼쪽(index가 더 작은쪽)의 char가 현재의 것과 다르면, 지금까지 세고 있던 count를 chars array에 반영하도록 하였습니다.반복횟수가 10회 이상인 경우의 문제아래 code의 convert_repeat_count_to_chars()가 이에 대해 작성한 solution입니다.반복을 count한 값이 10이상이면, 각 1의 자리부터 각 자릿수를 분리하여, 집어 넣는 방식입니다.Complexity시간복잡도(time complexity) 는 $O(n)$입니다.chars를 1회 순회(iteration의 의미)합니다. convert_repeat_count_to_chars가 각 iteration마다 실행되지만, 이는 count의 값의 자릿수에 의존적인것이라 constant하다고 여겼습니다공간복잡도(space complexity) 는 $O(1)$입니다.chars를 직접 변경하면서 iteration의 결과를 저장하고 있고, 때문에, chars에 비례하여 추가로 사용하고 있는 공간이 없어서, $O(1)$이라고 계산했습니다.fn compress(chars: &amp;mut Vec&lt;char&gt;) -&gt; i32 { if chars.len() &lt;= 1 { return 1; } let mut repeat_char: char = char::default(); let mut repeat_count: u32 = 0; let mut idx: i32 = chars.len() as i32 - 1; while idx &gt;= 0 { let char = chars[idx as usize]; let before = if idx &gt; 0 { chars[idx as usize - 1] } else { char::default() }; if repeat_char == char { repeat_count += 1; } else { repeat_char = char; repeat_count = 1; } if before != char { let splice_start_idx = idx as usize; chars.splice(splice_start_idx..std::cmp::min(splice_start_idx + repeat_count as usize, chars.len()), convert_repeat_count_to_chars(repeat_char, repeat_count)); } idx -= 1; } return chars.len() as i32;}fn convert_repeat_count_to_chars(repeat_char: char, count: u32) -&gt; Vec&lt;char&gt; { let mut mut_count = count; let mut chars = vec![repeat_char]; if count &lt;= 1 { return chars; } while (mut_count &gt; 0) { chars.insert(1, std::char::from_digit(mut_count % 10, 10).unwrap()); mut_count = mut_count / 10; } return chars;}fn main() { assert_eq!(compress(&amp;mut Vec::from(['a', 'a','b', 'b', 'c', 'c', 'c'])), 6); assert_eq!(compress(&amp;mut Vec::from(['a'])), 1); assert_eq!(compress(&amp;mut Vec::from(['a','b','b','b','b','b','b','b','b','b','b','b','b'])), 4)}결과ReferencesString Compression - LeetCode" }, { "title": "Chapter 1- Reliable, Scalable, and Maintainable Applications", "url": "/posts/Reliable,-Scalable-and-Maintainable-Applications/", "categories": "Books, Designing Data-Intensive Applications, PART 1- Foundation of Data Systems", "tags": "Computer Science, Architecture", "date": "2024-09-16 23:51:00 +0900", "snippet": "이 1장에서는 책 전반에서 사용하는 전문 용어(terminology)와 접근 방식(approach)을 소개합니다.신뢰성(reliability), 확장성(scalability), 유지보수성(maintainability) 같은 단어의 실제 의미와 목표를 당설하기 위해 어떻게 해야하는지 알아봅니다.Reliability(신뢰성)소프트웨어에 대한 Reliabi...", "content": "이 1장에서는 책 전반에서 사용하는 전문 용어(terminology)와 접근 방식(approach)을 소개합니다.신뢰성(reliability), 확장성(scalability), 유지보수성(maintainability) 같은 단어의 실제 의미와 목표를 당설하기 위해 어떻게 해야하는지 알아봅니다.Reliability(신뢰성)소프트웨어에 대한 Reliability에 대한 기대치는 다음과 같습니다. The application performs the function that the user expected.어플리케이션은 사용자가 기대한 기능을 수행한다. It can tolerate the user making mistakes or using the software in unexpectedways.시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다. Its performance is good enough for the required use case, under the expectedload and data volume.시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다. The system prevents any unauthorized access and abuse.시스템은 허가되지 않은 접근과 오남용을 방지한다. Fault-tolerant(내결함성) or Resilient(탄력성)Fault(결함)은 ‘잘못 될 수 있는 일’을 말합니다. 이 fault를 대처할 수 있는 시스템을 ‘Fault-tolerant(내결함성)’ 또는 ‘Resilient(탄력성)’을 지녔다고 말합니다.‘Fault-tolerant’가 모든 결함을 견딜 수 있는 시스템을 의미하긴 하지만 실현가능하진 않습니다.(자연재해 같은 통제 불가능한 경우가 있습니다) 그래서 ‘특정 유형’에 대해서만 내성이 있는것으로 여깁니다.Fault와 Failure‘Fault’와 ‘Failure’는 동일하지 않습니다. ‘Fault’는 시스템의 한 구성요소(Composite)으로 여겨지지만, ‘Failure’는 사용자에게 서비스를 제공하지 못하고 시스템이 멈춘것을 의미합니다.Fault-tolerant를 증가시키는 방법고의적으로 결함을 유도함으로써 Fault-tolerant(내결함성) 시스템을 지속적으로 훈련합니다.Netflix의 Chaos Monkey가 이런 접근 방식을 사용하고 있습니다. Netfilx의 Chaos Monkey는 (Blog글에 따르면), AWS의 Production환경의 instance를 무작위로 마비시킨다고 합니다. 이를 통해 엔지니어들은 더 나은 ‘자동 복구’를 구축하도록 노력한다고 합니다.보통은 결함을 예방하는것을 넘어서서 Fault-tolerant(내결함성)을 원하지만, 해결책이 없는경우와 같이 예방하는것이 더 나은 경우가 있습니다. 대표적으로 ‘Security(보안)’ 문제 입니다.Hardware Faults하드웨어 장치로부터 기인한 ‘fault’ 유형입니다. 무작의적이고 서로 독립적인 특징이 있습니다.최근까지 ‘single machine’(1개의 장비로 운영되는 시스템)의 전체 장애는 매우 드물었기 때문에, 하드웨어를 중복으로 준비하는걸로 충분히 대응할 수 있었습니다.하지만, 데이터의 양과 어플리케이션에 대한 의존성이 늘어나면서 더 많은 수의 장비를 사용하게 되었고, 하드웨어 결함율도 비례하여 증가하였습니다. 또한 AWS와 같은 Public Cloud에서 instance가 별도의 경고없이 사용중지되는 경우도 있습니다.따라서, 소프트웨어도 ‘fault-tolerant’를 사용하는 시스템으로 옮겨가고 있습니다.Software ErrorsSoftware에서 기이한 ‘fault’ 유형입니다. 예상하기 어렵고, 노드간에 상관간계가 있어, ‘hardware faults’보다 더 자주 system error를 발생시킵니다.Software Errors의 예시는 다음과 같습니다. A software bug that causes every instance of an application server to crash whengiven a particular bad input. For example, consider the leap second on June 30,2012, that caused many applications to hang simultaneously due to a bug in theLinux kernel.It’s Time to Ditch the Leap Second: The Devastating Effect of Adding Just 1 SecondLeap second hits Qantas air bookings, while Reddit and Mozilla stutter A runaway process that uses up some shared resource—CPU time, memory, diskspace, or network bandwidth. A service that the system depends on that slows down, becomes unresponsive, orstarts returning corrupted responses. Cascading failures, where a small fault in one component triggers a fault inanother component, which in turn triggers further faults [10].이런 bug는 특정 상황에 마주하기 전까지 오랫동안 나타나지 않을 수 있습니다.‘Software Errors’에는 신속한 해결책이 없으며, 아래의 것들이 해결에 도움을 줍니다. carefully thinking about assumptions and interactions in thesystem thorough testing process isolation allowing processes to crash and restart measuring, monitoring, and analyzing system behavior in productionHuman Errors사람에 기인한 ‘fault’유형입니다. 사람이 최선의 의도를 갖고 있다해도, 미덥지 않다고 알려져 있습니다. 대규모 인터넷 서비스에 대한 연구에 따르면, 운영자의 설정 오류가 중단의 주요 원인이라고 합니다.그럼에도 시스템의 ‘Reliability’를 어떻게 챙길까요? Design systems in a way that minimizes opportunities for error. For example,well-designed abstractions, APIs, and admin interfaces make it easy to do “theright thing” and discourage “the wrong thing.” However, if the interfaces are toorestrictive people will work around them, negating their benefit, so this is a trickybalance to get right.잘 디자인된 ‘abstraction(추상화)’는 ‘정상적인 일’은 쉽게하고, ‘잘못된 일’은 어렵게 합니다. Decouple the places where people make the most mistakes from the places wherethey can cause failures. In particular, provide fully featured non-productionsandbox environments where people can explore and experiment safely, usingreal data, without affecting real users.사람이 실수할 수 있는 부분을 decouple(분리)시킵니다. 실제 데이터를 활용한 ‘sandbox’환경을 제공해야 합니다. Test thoroughly at all levels, from unit tests to whole-system integration tests andmanual tests. Automated testing is widely used, well understood, and espe‐cially valuable for covering corner cases that rarely arise in normal operation. Allow quick and easy recovery from human errors, to minimize the impact in thecase of a failure. For example, make it fast to roll back configuration changes, rollout new code gradually (so that any unexpected bugs affect only a small subset ofusers), and provide tools to recompute data (in case it turns out that the old com‐putation was incorrect). Set up detailed and clear monitoring, such as performance metrics and errorrates. In other engineering disciplines this is referred to as telemetry. (Once arocket has left the ground, telemetry is essential for tracking what is happening,and for understanding failures [14].) Monitoring can show us early warning sig‐nals and allow us to check whether any assumptions or constraints are being vio‐lated. When a problem occurs, metrics can be invaluable in diagnosing the issue. Implement good management practices and training—a complex and importantaspect, and beyond the scope of this book.Scalability(확장성)‘Scalability’는 증가한 부하에 대처하는 ‘시스템 능력’을 설명하는데 사용됩니다.Describing Load(부하 기술하기)시스템의 현재 ‘Load(부하)’를 간결한게 기술해야합니다. 이는 부하 성장 질문(”부하가 두 배로 되면 어떻게 될까?”)을 논의할 수 있게 합니다.‘Load’를 설명하는 ‘Load parameter’는 시스템 구조에 따라 달라집니다. 이 구조에 따라,requests per second to a web server, the ratio of reads to writes in a database, the number of simultaneously active users in a chat room, the hit rate on a cache, or something else과 같은 값이 parameter가 될 수 있습니다.Twitter(트위터) 예시(from Timelines at Scale)트위터의 주요 동작은 다음과 같습니다. Tweet(트윗) 작성사용자는 새로운 메세지를 개시할 수 있습니다.(평균 초당 4.6k요청, 피크(peak)일 때 초당 12k 요청 이상) Timeline(홈 화면의 타임라인)사용자는 팔로우한 사람의 tweet을 볼 수 있습니다.(초당 300k요청) write 작업에 해당하는 ‘tweet작성’이 12k인건 다루기가 상당히 쉽습니다.하지만, 트위터의 확장성 문제는 ‘fan-out’에 있습니다. 이 ‘fan-out’은 1개의 tweet을 여러 팔로어들에게 노출해야하는 요구사항 때문에 생깁니다. ‘fan-out’은 입력(input)을 여러개의 출력(output)으로 확장하는것을 말합니다. 즉 1개의 신호를 여러 machine에서 처리할 수 있게 확장하는것을 말합니다. fan-out의 output(출력)은 다른 기능의 input(입력)이 됩니다.Twitter의 Timeline기능 예시이를 해결하기 위해서, 관계형 데이터베이스를 이용하고, query를 통해 구현하는 방법이 있습니다. tweet이 등록되면, 전역 tweet collection에 등록하고, 필요에 따라 다음 query예시와 같이 팔로우중인 유저의 tweet을 불러옵니다.SELECT tweets.*, users.*FROM tweets JOIN users ON tweets.sender_id = users.id JOIN follows ON follows.followee_id = users.idWHERE follows.follower_id = current_user 혹은, 각 사용자들의 Timeline의 cache를 만들고 유지하며, 이 cache에 새로운 tweet을 추가하는식으로 구현합니다. 이렇게하면, 각 유저의 Timeline에 대한 read요청은 이미 결과값이 계산되어(cache되어) 있기 때문에, 그 비용이 매우 저렴합니다.Tweeter Timeline 구조 from ‘Timelines at Scale’*Twitter’s data pipeline for delivering tweets to followers, with load parameters as of November 2012*위의 접근방식중 두번째(cache를 이용한 방법)의 불리한점은 ‘트윗 작성’이 많은 부가 작업(여러 cache에 write해야하는것)을 필요로하게 된다는점 입니다. 이는 follow숫자가 많은 경우 치명적이게 되는데, 그래서 이 ‘follower 숫자’가 핵심적인 ‘Load parameter’가 됩니다. 트위터는 5초 이내에 팔로워들에게 tweet을 전송하기위해 노력한다고 합니다.트위터에서는 최종적으로, 두번째 구현방식을 기반으로, 팔로워가 매우 많은 소수 유저의 경우 이 방식에서 제외시키고, 첫번째 접근방식과 유사하게 작동한다고 합니다.Describing Performance(성능 기술하기)일단 ‘System load’를 묘사하고나면, Load가 증가할 때, 어떤일이 일어나는지 다음 2가지 방법으로 살펴볼 수 있습니다. When you increase a load parameter and keep the system resources (CPU, mem‐ory, network bandwidth, etc.) unchanged, how is the performance of your systemaffected?Load parameter가 증가했을때, scale-up(cpu, 메모리, 네트워크 대여폭을 증가)하지 않고 유지하면 시스템 성능이 어떻게 변화할까? When you increase a load parameter, how much do you need to increase theresources if you want to keep performance unchanged?Load paramter가 증가했을때, 성능이 유지되길 원한다면, 자원을 얼마나 늘려야 할까? 이 질문들이 가능해지려면, ‘Performance number(성능 수치)’가 필요합니다.Hadoop 같은 ‘batch processing system(일괄 처리 시스템)’은 thoughput(처리량)을 중요하게 생각하는 반면, 온라인 시스템은 ‘resposne time(응답시간)’을 중요하게 생각합니다. ‘latency(지연시간)’와 ‘response time(응답시간)’의 차이’response time’은 Client의 관점에서 본 시간으로, 요청을 처리하는 실제 시간외에도 네트워크 지연, 큐 지연도 함하고 있습니다.반면, ‘latency’는 요청이 처리되길 기다리는 시간입니다.‘Response time(응답시간)’의 경우, 같은 요청이라도 매 요청마다 달라집니다. 이는 여려가지 요인이 있지만, 이 ‘Response time’을 사용할때는 그 값의 ‘distribution(분포)’로 봐야합니다.이 ‘distribution(분포)’를 볼때, ‘arithmetic mean(산술 평균)’본다 ‘percentile(백분위)’을 사용하는게 좋습니다. ‘percentile’의 종류는 다음과 같습니다. p50 특정 범위안에서 값을 정렬하고, 50%에 위치하는 값을 가리킵니다. 사용자가 보통 얼마나 오래 기다리는지 확인할때 사용합니다. p95, p99, p999 ‘higher percentiles(상위 백분위)’ 혹은 ‘tail latency(꼬리 지연 시간)’으로 불립니다. 특이값(outliers)이 얼마나 안 좋은지 볼때 사용합니다.이러한 ‘Percentile(백분위)’은 ‘Service level objective(SLO, 서비스 수준 목표)’와 ‘Service level agreement(SLA, 서비스 수준 협약서)’에서 자주 사용합니다.예를 들어, the service is considered to be up if it has a median response time of less than 200 ms and a 99th percentile under 1 s (if the response time is longer, it might as well be down), and the service may be required to be up at least 99.9% of the time.이런식으로 사용되곤 합니다.Tail latency amplification(꼬리 지연 증폭)end-user(최종 사용자)에게 요청 일부가 여러 백엔드 요청으로 이루어져 있다면, ‘p95, p99’와 같은 ‘higher percentiles’가 중요합니다. 각 요청을 병렬처리하고 있다고 하더라도, 모든 작업이 완료되는건 가장 느린 요청이 완료되어야 합니다. 이 때문에, 최종 ‘response time(응답 시간)’이 느려지는데, 이를 ‘tail latency amplification(꼬리 지연 증폭)’이라고 합니다.Approaches for Coping with Load(부하 대응 접근 방식)시스템의 성능을 측정하기 위한 ‘Load(부하)’와 ‘Metric(지표)’를 정했으니, 확장성을 논의할 수 있습니다.‘Load parameter(부하 변수)’가 어느정도 증가하더라도 성능을 좋게 유지하려면 어떻게 해야 할까요? Scaling up(용량 확장, 수직확장) 더 고사양의 장비로 이동합니다 Scaling out(규모 확장, 수평확장) 다수의 낮은 사양의 장비로 확장하여 Load(부하)를 분산시킵니다. 이런 Architecture를 ‘shared-nothing’ 이라고 합니다.Stateless(상태를 저장하지 않음, 각 장비가 독립적으로 기능을 수행) 서비스를 ‘scale out’하는건 간단하지만, ‘stateful’ 데이터 시스템을 분산 설치하는건 아주 많은 복잡도가 발생되는 일입니다.그래서, ‘High-availability(고가용성)’에 대한 요구사항이 생기기 전까지는 단일 노드에 데이터베이스를 유지하는것이 최근까지의 통념입니다.저자는 최근에는 분산 시스템을 위한 도구와 추상화가 좋아지면서, ‘Distributed data systems(분산 데이터 시스템)’이 기본으로 자리잡을 수 있다고 설명하고 있습니다.모든 상황에 맞는 확장 아키텍쳐(magic scaling source라 불리는)는 없습니다.특정 Application의 주요 동작과 잘 하는 동작이 무엇인지 대한 가정을 기반으로, 확장성을 갖춘 architecture를 설계합니다.Maintainability(유지보수성)소프트웨어의 비용은 대부분 초기 개발이 아니라, 이어지는 유지보수가 큰 부분을 차지한다고 합니다.많은 엔지니어들이 소위 ‘legacy’ 시스템을 유지보수하는것을 선호하지 않습니다. 이는 유지보수 과정의 많은 ‘고통’이 동반되기 때문입니다.다행이도, 소프트웨어 설계과정에서 이 ‘고통’을 고려하여 설계함으로서 최소화 할 수 있습니다. Operability(운용성) Make it easy for operations teams to keep the system running smoothly. 운영하기 쉽게 만들어라 Simplicity(단순성) Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system. (Note this is not the same as simplicity of the user interface.) 복잡도를 최대한 제거하여 새로운 엔지니어가 이해하기 쉽게 만들어라. Evolvability(발전성) Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity. 엔지니어가 시스템을 쉽게 변경할 수 있게 해야한다.Operability(운용성)‘좋은 운용성’이란 동일하게 반복되는 task를 쉽게 수행하게끔 만들어, 운영팀이 고부가가치 활동에 노력을 집중하게 한다는 의미입니다. 이를 위해, 아래와 같은 일을 할 수 있습니다. Providing visibility into the runtime behavior and internals of the system, withgood monitoring Providing good support for automation and integration with standard tools Avoiding dependency on individual machines (allowing machines to be takendown for maintenance while the system as a whole continues running uninter‐rupted) Providing good documentation and an easy-to-understand operational model(“If I do X, Y will happen”) Providing good default behavior, but also giving administrators the freedom tooverride defaults when needed Self-healing where appropriate, but also giving administrators manual controlover the system state when needed Exhibiting predictable behavior, minimizing surprisesSimplicity(단순성)프로젝트가 커짐에 따라서, 시스템은 매우 복잡하고 이해하기 어려워집니다. Complexity(복잡도)는 같은 시스템에서 작업하는 사람들의 진행을 느리게하고, 유지보수 비용을 증가시키는 원인이 됩니다.이 Complexity(복잡도)는 다양한 증상으로 나타납니다.‘explosion of the state space’, ‘tight coupling of modules’, ‘tangled dependencies’, ‘inconsistent naming and terminology’, ‘hacks aimed at solving performance problems’, ‘special-casing to work around issues elsewhere’등이 있습니다.때문에, Simplicity(단순성)이 시스템의 핵심 목표여야 합니다. 또한 Simplicity(단순성)이 기능을 줄인다는 의미는 아닙니다.Moseley와 Marks는 ‘Out of the Tar pit’에서 ‘accidental complexity(우발적 복잡도)’를 설명하며, 이를 end-user에게 보이는 문제에 있는게 아니라, ‘구현’단에서만 발생하는것으로 정의하고 있습니다.Evolvability(발전성)시스템의 요구사항이 바뀌지 않을 가능성은 매우 적습니다.이 책에서는 Agile기법이 적용되는 소프트웨어 보다는, 다양한 application이나 다른 특성을 가진 서비스로 구성된 ‘larger data system(대규모 데이터 시스템)’수준에서 민첩성을 높이는 방법을 찾습니다.이를 위해선, 시스템의 ‘Simplicity(단순성)’과 ‘Abstraction(추상화)’가 매우 중요합니다.References Netfilx의 Chaos Monkey와 관련된 Blog글 The Netflix Simian Army Why Do Internet Services Fail, and What Can Be Done About It? [Why Do Internet Services Fail, and What Can Be Done About It? USENIX](https://www.usenix.org/conference/usits-03/why-do-internet-services-fail-and-what-can-be-done-about-it) Tweeter의 ‘Timelines at scale’ Timelines at Scale" }, { "title": "1 Byte and The number of bits(1Byte와 bit수)", "url": "/posts/Byte-and-bit/", "categories": "Programming, Computer Basics", "tags": "Computer Science, programming", "date": "2024-09-14 19:31:00 +0900", "snippet": "1bit(비트)는 0과 1의 값을 가질 수 있는 데이터의 최소 단위입니다.이 bit를 특정갯수 만큼 묶어서, 1byte(바이트)라고 합니다.Byte의 의미최초의 byte는 ‘하나의 character 데이터를 나타내는데 필요한 bit수’를 의미했습니다. Byte denotes a group of bits used to encode a character...", "content": "1bit(비트)는 0과 1의 값을 가질 수 있는 데이터의 최소 단위입니다.이 bit를 특정갯수 만큼 묶어서, 1byte(바이트)라고 합니다.Byte의 의미최초의 byte는 ‘하나의 character 데이터를 나타내는데 필요한 bit수’를 의미했습니다. Byte denotes a group of bits used to encode a character, or the number of bits transmitted in parallel to and from input-output units.- from ‘PLANNING A COMPUTER SYSTEM’ 컴퓨터에선, character를 특정 정수(bit로 표현한 integer)로 mapping하여 사용합니다.초기 컴퓨터는 주로 text데이터를 처리했기 때문에, 자연스럽게 byte가 메모리 공간을 할당하는 최소 단위가 되었습니다(character를 저장해야하기 때문에). 이후 수많은 ‘computer architecture’에서 메모리의 공간을 나타내는 최소 단위로 사용되었습니다. 현재처럼 1byte = 8bit인 표준이 없을때는, Byte의 bit수는 Hardware에 따라 달라졌습니다.특히 초기 컴퓨터에선, 6bit character code를 사용하는 시스템이 있었습니다.표준이된 1 byte = 8 bits고정된 bit수로 문자를 인코딩하는 ASCII(American Standard Code for Information Interchange)가 표준으로 자리잡자, 자연스럽게 ‘1 byte = 8 bits’로 자리잡게 되었습니다.ASCII는 원래 7bit로 128개의 문자를 표현하였습니다. 여기에는 영어와 몇가지 기호가 포함되어 있었습니다.이를 8bit로 확장하여, 256개의 문자를 표현할 수 있게하여 ‘1 byte = 8 bits’로 사용하게 되었습니다.숫자(Number)와 Byte‘1 byte = 8 bits’가 표준으로 자리 잡으면서, 숫자를 저장하기 위한 공간도 byte단위로 표시하게 되었습니다.기본적인 단위인 byte의 bit수 8을 기준으로,8-bit, 16-bit, 32-bit, 64-bit, 128-bit 정수가 있습니다.각 bit수에 따라, 정수값 표현 범위는, 아래와 같습니다.\\[-(2^{n-1})\\sim(2^{n-1}-1)\\]예를 들어, 8-bit 정수의 경우, $-(2^7)\\sim2^{7}-1$ 로, $-128\\sim127$의 범위를 갖습니다.여기서, $n-1$로 bit수에서 1을 제외하는 이유는, 가장 왼쪽 bit를 부호(+, - 양수인지 음수인지 나태내는것)를 표기하는데 사용하기 때문입니다. 양수의 범위는, 0 0000000 에서 0 1111111 까지로 0에서 127까지 가리킵니다. 음수의 범위는, 1 0000000 에서 1 1111111까지, -128에서 -1까지를 나타냅니다. 양수 범위에서 -1을 하게 되는 이유는, 7-bit로 표현할 수 있는 숫자의 가지수는 128개(=$2^7$)이지만, 이는 갯수를 의미하고, 0을 포함시키면 표현 가능한 최대 양수값은 127이 됩니다.Weakly typed언어(javascript)에서의 Number약타입 언어(Weakly typed language)는 변수 선언시 별도의 type을 지정하지 않습니다. 때문에, 어떤 숫자를 저장하던, 이미 정해진 memory공간을 사용합니다.javascript를 예로 들면, number는 다른 프로그래밍 언어의 double과 같은 공간을 차지합니다.이 double은 소수점표현(분수값표현)이 가능합니다.javascript에선, number가 double로 다루어지기 때문에, 정수 32를 저장하더라도, 내부적으로 소수점을 포함한 값으로 저장합니다.Number - JavaScript | MDNReferences Wikipedia에서의 Byte정보 Byte Javascript의 Number Number - JavaScript | MDN Rust의 Integer Types Integer Types" }, { "title": "LeetCode 75-3 Kids With the Greatest Number of Candies", "url": "/posts/Kids-With-the-Greatest-Number-of-Candies/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-10 15:53:00 +0900", "snippet": "문제 요약candies라는 array에는 candy의 갯수(integer type)가 들어있습니다. candies[i]는 ith째 아이(kid)가 가지고 있는 candy의 갯수입니다.extraCandies는 여러분이 가지고 있는 여분의 candy를 의미하며,이를 각각의 아이에게’만’ 주었을 때, 각각의 아이가 가장 많은 수(Greatest number...", "content": "문제 요약candies라는 array에는 candy의 갯수(integer type)가 들어있습니다. candies[i]는 ith째 아이(kid)가 가지고 있는 candy의 갯수입니다.extraCandies는 여러분이 가지고 있는 여분의 candy를 의미하며,이를 각각의 아이에게’만’ 주었을 때, 각각의 아이가 가장 많은 수(Greatest number)의 candy를 갖게 되는지 일종의 시뮬레이션 결과를 반환하는 문제입니다.문제 풀이간단한 문제라 작성한 solution은 1개 입니다. ‘example1’로 예를 들어 설명하면, Input: candies = [2,3,5,1,3], extraCandies = 3 Output: [true,true,true,false,true]Input의 값을 고려하여 아래의 조건을 충족해야 합니다.\\[\\text{candies}+3\\geqq5 \\rightarrow candies \\geqq 2\\]즉, candies array에서, 각 요소(element)가 $candies \\geqq 2$를 만족하는지 확인하면 됩니다.impl Solution { pub fn kids_with_candies(candies: Vec&lt;i32&gt;, extra_candies: i32) -&gt; Vec&lt;bool&gt; { // let max_candy = candies.iter().max().unwrap(); let threshold: i32 = candies.iter().max().unwrap() - extra_candies; let results: Vec&lt;bool&gt; = candies.iter().map(|&amp;candy_count| { return candy_count &gt;= threshold; }).collect(); return results; }}기타 의견$candies \\geqq 2$를 확인할 때에, CPU의 SIMD를 활용하여 병렬처리할 수 있을듯 합니다.rust에선 std::simd모듈로 ‘experimental API’로서 제공하고 있습니다.ReferencesKids With the Greatest Number of Candies - LeetCode" }, { "title": "Big-O notation and complexity(big-O 표기법과 복잡도) Examples", "url": "/posts/big-O-and-Time-Complexity(Examples)/", "categories": "Books, Cracking The Coding Interview", "tags": "Computer Science, algorithm, 알고리즘, Time Complexity", "date": "2024-09-09 21:01:16 +0900", "snippet": "Overview몇가지 예시를 통해 big-O표기법에 대해서 이해합니다.예시여러개의 문자열을 각각 정렬하고 전체 list에 대해서도 정렬하는 경우총 N개의 문자열이 있다고 하자, 이때, 각각의 문자열을 알파벳 순으로 정렬하고, 전체 문자열 목록을 사전(Dictionary)순에 따라 정리하는 예시입니다. 이 경우, 문자열 1개를 정렬하는데, $O(N\\...", "content": "Overview몇가지 예시를 통해 big-O표기법에 대해서 이해합니다.예시여러개의 문자열을 각각 정렬하고 전체 list에 대해서도 정렬하는 경우총 N개의 문자열이 있다고 하자, 이때, 각각의 문자열을 알파벳 순으로 정렬하고, 전체 문자열 목록을 사전(Dictionary)순에 따라 정리하는 예시입니다. 이 경우, 문자열 1개를 정렬하는데, $O(N\\log{N})$의 비용이 들기때문에, 전체 문자열 N에 대해서 수행해야 하므로 총$O(N*N\\log{N})$. 이후 전체 문자열을 사전순으로 정렬해야 하므로, 추가로 $O(N\\log{N})$. 총 $O(N^2\\log{N} + N\\log{N})\\rightarrow O(N\\log{N})$ 이 된다고 생각할 수 있습니다.하지만 이건 틀렸습니다. ‘문자열 길이와 문자열 list의 길이는 다르다’는 걸 고려해야 합니다. 이를 수정하면, 문자열중 가장 긴 경우의 길이를 ‘s’라고 하고, array의 길이를 ‘a’라고 했을때, 1개의 문자열을 정렬하는데, $O(s\\log{s})$가 든다. a개의 문자열을 정렬해야 하므로, $O(a*s\\log{s})$가 된다. 이제, 전체 문자열 array를 정렬하기 위한 비용을 다루는데, 쉽게 $O(a\\log{a})$라고만 생각할 수 있다. 하지만, ‘사전순’으로 정렬해야 하기 때문에, 2개의 문자열을 비교하는 비용인 $O(s)$를 고려해야 한다. 이에 따라, 문자열 array를 정렬하는 비용은 $O(a*s\\log{a})$이다.이를 모두 정리하면,\\(O(a*s(\\log{a}+\\log{s}))\\)이다. 2개의 문자열을 비교하는 비용($O(s)$)을 잊지말자.재귀호출(recursive call)의 패턴분석다음은 이진탐색트리(Binary Search Tree, 이하 BST)의 모든 Node의 합을 구하는 코드입니다.여기서, 시간복잡도(Time complexity)는 어떻게 될까요?int sum(Node node) {\tif (node == null) {\t\treturn 0;\t}\treturn sum(node.left) + node.value + sum(node.right);}BST라는 이유로, $\\log{n}$으로 시작하는 무언가를 떠올릴 수 있지만, 이 경우,\\[O(N)\\]가 맞습니다.이를 2가지 방향으로 설명할 수 있습니다.코드가 실제로 의미하는바가 무엇인가?간단한 방법으로 코드의 실제 의미를 생각해보는 방법입니다. 이 코드는 각 노드를 방문한 후, 상수시간($O(1)$)의 작업을 수행합니다. 이에 따라, 각 Node를 방문하는 비용이 지배적이며, 결국 Node의 갯수와 연관이 있습니다.즉, Node의 갯수 N과 선형관계인 $O(N)$이 시간복잡도(Time complexity)가 됩니다.재귀호출의 패턴을 분석하자지난 post에서 ‘Recursive call’의 runtime을 $O(\\text{분기 갯수}^{깊이})$로 표현했습니다.각 Node의BST의 경우, 그 의미에 따라서 ‘분기 갯수 = 2’가 되고, $O(2^{깊이})$으로 정리됩니다.여기서, BST의 깊이는 $\\log_2{N}$이 므로, $O(2^{\\log_2{N}})$이 됩니다.$\\log_2{}$의 의미를 다시 기억하면, 다음과 같습니다.\\[2^{P}=Q \\rightarrow \\log_2{Q}=P\\]이에 따라, $O(2^{\\log_2{N}})$을 정리하면,\\[P=2^{\\log_2{N}} \\rightarrow \\log_2{P}=\\log_2{N} \\rightarrow P=N\\]즉,\\[O(N)\\]이 됩니다.Reference Book CRACKING the CODING INTERVIEW" }, { "title": "LeetCode 75-2 Greatest Common Divisor of Strings", "url": "/posts/Greatest-Common-Divisor-of-Strings/", "categories": "Programming, rust, leetcode75", "tags": "programming, rust, leetcode", "date": "2024-09-09 18:05:00 +0900", "snippet": "문제 요약두 문자열, str1과 str2에서 각각 반복되는 최대 길이의 문자열중 공통된 부분(’x’로 표기)을 찾습니다.문제 풀이첫번째 시도처음에는 문제를 이해하지 못하고(최대공약수 문제인지 모르고), 풀려고 했습니다.여러 접근법이 머리에 떠오릅니다. str1과 str2를 byte code로 바꾸어서, 각각 ‘-’연산을 하면 0이 아닌 지점을 통해 ...", "content": "문제 요약두 문자열, str1과 str2에서 각각 반복되는 최대 길이의 문자열중 공통된 부분(’x’로 표기)을 찾습니다.문제 풀이첫번째 시도처음에는 문제를 이해하지 못하고(최대공약수 문제인지 모르고), 풀려고 했습니다.여러 접근법이 머리에 떠오릅니다. str1과 str2를 byte code로 바꾸어서, 각각 ‘-’연산을 하면 0이 아닌 지점을 통해 뭔가를 파악할 수 있지 않을까? ‘easy’난이도 임에도, 한참 헤맸는데 문제를 잘 이해하는게 얼마나 중요한지 깨달았습니다.fn gcd_of_strings_v1(str1: String, str2: String) -&gt; String { let mut long_len: usize = std::cmp::max(str1.len(), str2.len()); let mut diff_cursor: usize = 0; for i in 1..=long_len { if i &gt; str1.len() || i &gt; str2.len() { diff_cursor = i - 1; break; } let sliced_str1 = &amp;str1[0..i]; let sliced_str2 = &amp;str2[0..i]; if sliced_str1 != sliced_str2 { diff_cursor = i; break; } } let x = (&amp;str1[0..diff_cursor]).to_string(); if x.len() == 1 { return String::from(\"\"); } if x.len() % 2 != 0 { return x; } let mut i: usize= x.len() / 2; while i &gt; 0 { let part = &amp;x[0..i]; let compare_part: &amp;str = &amp;x[i..i+part.len()]; if part == compare_part { return part.to_string(); } i = (i / 2) as usize; } return x;}fn main() { assert_eq!(gcd_of_strings_v1(\"ABCABC\".to_string(), \"ABC\".to_string()), \"ABC\"); assert_eq!(gcd_of_strings_v1(\"ABABAB\".to_string(), \"ABAB\".to_string()), \"AB\"); assert_eq!(gcd_of_strings_v1(\"LEET\".to_string(), \"CODE\".to_string()), \"\");}두번째 시도문제의 이름을 보니 ‘Great Common Divisor’가 보입니다. ‘최대공약수’를 의미하는듯 합니다.이를 문제에 활용할 수 있는 방법을 찾아봅니다.fn gcd_of_strings_v1(str1: String, str2: String) -&gt; String { let common_divisors = get_common_divisors(str1.len(), str2.len()); let mut gcd: usize = 0; for &amp;divisor in common_divisors.iter().rev() { let str1_part = &amp;str1[0..divisor as usize]; let str2_part = &amp;str2[0..divisor as usize]; if str1_part != str2_part { continue; } let str1_repeat = String::from(str1_part).repeat(str1.len() / (divisor as usize)); if str1_repeat != str1 { continue; } let str2_repeat = String::from(str2_part).repeat(str2.len() / (divisor as usize)); if str2_repeat != str2 { continue; } gcd = divisor as usize; break; } if gcd == 0 { return \"\".to_string(); } return String::from(&amp;str1[0..gcd]);}fn get_common_divisors(num_1: usize, num_2: usize) -&gt; Vec&lt;u32&gt; { let num1_divisors = get_divisors_of_num(num_1); let num2_divisors = get_divisors_of_num(num_2); let mut num1_idx: usize = 0; let mut num2_idx: usize = 0; let mut common_divisors: Vec&lt;u32&gt; = Vec::new(); while (num1_idx &lt; num1_divisors.len() &amp;&amp; num2_idx &lt; num2_divisors.len()) { let num1_candidate = num1_divisors[num1_idx]; let num2_candidate = num2_divisors[num2_idx]; if num1_candidate == num2_candidate { common_divisors.push(num1_candidate); num1_idx = std::cmp::min(num1_idx + 1, num1_divisors.len()); num2_idx = std::cmp::min(num2_idx + 1, num2_divisors.len()); continue; } if num1_candidate &lt; num2_candidate { num1_idx = std::cmp::min(num1_idx + 1, num1_divisors.len()); } else if num1_candidate &gt; num2_candidate { num2_idx = std::cmp::min(num2_idx + 1, num2_divisors.len()); } } return common_divisors;}fn get_divisors_of_num(target_num: usize) -&gt; Vec&lt;u32&gt; { let mut divisors: Vec&lt;u32&gt; = Vec::new(); for i in 1..=target_num { if target_num % i == 0 { divisors.push(i as u32); } } return divisors;}fn main() { assert_eq!(gcd_of_strings_v1(\"ABCABC\".to_string(), \"ABC\".to_string()), \"ABC\"); assert_eq!(gcd_of_strings_v1(\"ABABAB\".to_string(), \"ABAB\".to_string()), \"AB\"); assert_eq!(gcd_of_strings_v1(\"LEET\".to_string(), \"CODE\".to_string()), \"\"); assert_eq!(gcd_of_strings_v1(\"ABABABAB\".to_string(), \"ABAB\".to_string()), \"ABAB\");}Flow는 다음과 같습니다. get_common_divisors 함수(여기선 method)를 통해, 각 String의 길이(length)에 대한 ‘공약수 array’를 구합니다. ‘공약수(common divisor)’를 구하는 이유는, 우리가 찾아낼 String ‘x’는 str1과 str2에서 모두 반복되고 있습니다. 때문에, 문자열(String) x의 길이(length)는 str1과 str2모두에서 반복이 가능한 길이여야 합니다. 이 문자열 x의 길이로서 가능성 있는것이 ‘공약수(common divisor)’입니다. 이 ‘공약수(common divisor)’를 기준으로 String의 slice를 만들어, str1과 str2에서 반복이 발생하는 최대 값을 찾으면, 문자열 ‘x’를 구할 수 있습니다. 이 ‘공약수 array’를 큰 값부터 탐색합니다.(array를 reverse탐색합니다.) for &amp;divisor in common_divisors.iter().rev() { ... } 이 공약수(common divisor)를 기준으로 str1과 str2를 slice하고, 실제로 각각의 str1과 str2에서 반복이 발생하는지 확인합니다.여기선, String slice를 원문 String의 길이에 맞게 반복하고, 일치하는지 확인합니다. fn gcd_of_strings_v1(str1: String, str2: String) -&gt; String { let common_divisors = get_common_divisors(str1.len(), str2.len()); let mut gcd: usize = 0; for &amp;divisor in common_divisors.iter().rev() { ... let str1_repeat = String::from(str1_part).repeat(str1.len() / (divisor as usize)); if str1_repeat != str1 { continue; } let str2_repeat = String::from(str2_part).repeat(str2.len() / (divisor as usize)); if str2_repeat != str2 { continue; } ... } ... } 위의 모든 조건을 만족하는 divisor를 찾아내면, ‘Greatest Common Divisor’를 찾아낸 겁니다. 공약수(common divisor)중 큰것 부터 탐색했기 때문에, 가장 먼저 발견되는 값이 ‘최대공약수’입니다. 기타 함수 설명Function get_common_divisors()두개의 정수를 받아서, $O(n)$의 비용으로 ‘common divisor’ list를 반환합니다.fn get_common_divisors(num_1: usize, num_2: usize) -&gt; Vec&lt;u32&gt; { let num1_divisors = get_divisors_of_num(num_1); let num2_divisors = get_divisors_of_num(num_2); let mut num1_idx: usize = 0; let mut num2_idx: usize = 0; let mut common_divisors: Vec&lt;u32&gt; = Vec::new(); while (num1_idx &lt; num1_divisors.len() &amp;&amp; num2_idx &lt; num2_divisors.len()) { let num1_candidate = num1_divisors[num1_idx]; let num2_candidate = num2_divisors[num2_idx]; if num1_candidate == num2_candidate { common_divisors.push(num1_candidate); num1_idx = std::cmp::min(num1_idx + 1, num1_divisors.len()); num2_idx = std::cmp::min(num2_idx + 1, num2_divisors.len()); continue; }\t\t // num1_divisors와 num2_divisors는 정렬되어 있으므로, 아래와 같이 작은쪽의 index만 증가시킵니다. if num1_candidate &lt; num2_candidate { num1_idx = std::cmp::min(num1_idx + 1, num1_divisors.len()); } else if num1_candidate &gt; num2_candidate { num2_idx = std::cmp::min(num2_idx + 1, num2_divisors.len()); } } return common_divisors;}ReferencesGreatest Common Divisor of Strings - LeetCode" }, { "title": "Merge sort implementation with rust(rust로 병합정렬 구현)", "url": "/posts/Merge-sort-implementation-with-rust/", "categories": "Programming, rust", "tags": "programming, rust, sort, algorithm", "date": "2024-09-01 19:00:00 +0900", "snippet": "‘merge sort’는 ‘Divide and conquer(분할과 정복)’을 사용한 대표적인 알고리즘입니다.이 ‘Divide and conquer’는 문제를 작게 쪼개서 해결하고, 이를 다시 합침으로서 문제를 해결해 나가는걸 말합니다. ‘Divide and conquer’는 여러 문제해결의 기본이 되곤 하며, 앞으로 부딪히는 문제가 너무 크게 느껴...", "content": "‘merge sort’는 ‘Divide and conquer(분할과 정복)’을 사용한 대표적인 알고리즘입니다.이 ‘Divide and conquer’는 문제를 작게 쪼개서 해결하고, 이를 다시 합침으로서 문제를 해결해 나가는걸 말합니다. ‘Divide and conquer’는 여러 문제해결의 기본이 되곤 하며, 앞으로 부딪히는 문제가 너무 크게 느껴진다면, 이 전략으로 접근하면 도움이 됩니다.‘merge sort’에서는 총 2단계로 알고리즘의 단계를 구분합니다. 분할(divide) 단계 array를 반으로 나눕니다. 정복(conquer) 단계(merge 단계) 나뉘어진 2개의 array를 하나로 합칩니다. 이때, 정렬이 이루어집니다.(2개의 array를 각각 보면서 크기를 비교하여 합칩니다) Time complexity and space complexityTime complexity(시간복잡도) Best case Average Worst case $O(n\\log{n})$ $O(n\\log{n})$ $O(n\\log{n})$ 어떤 상황이든 $O(n\\log{n})$으로 안정적인 성능을 보여줍니다.여기서 n은 정렬 대상이 되는 array 전체를 1회 탐색해야하는것을 의미합니다.알고리즘의 단계로 보면,Merge sort analysis with Tree. 출처: https://mathcenter.oxford.emory.edu/site/cs171/mergeSortAnalysis/ 분할(divide) 단계 $O(\\log{n})$, array를 반으로 줄여나가면서 함수의 실행횟수가 $O(\\log{n})$이 됩니다. 위 tree 이미지에서 tree depth가 이에 해당합니다. 정복(conquer) 단계 $O(n)$, 이 과정에서 정렬이 이루어지므로, 대상이 되는 array의 요소들을 1회씩 읽어야 합니다 왜 $O(2^{n})$이 아닐까?이전 post에서는 recursive 호출에 대해 시간복잡도가 $O(2^{n})$이라고 했습니다.‘Merge sort’에서 ‘recursion’을 사용함에도, 이번에는 왜 $O(n\\log{n})$일까? Array를 divide(분할)하는 작업의 비용이 매우 낮습니다. ‘Recursive Algorithm’에서의 ‘Time Complexity(시간복잡도)’는 각 function들(tree의 node에 해당함)의 실행시간을 고려한 결과입니다. 하지만, ‘Merge sort’에선, 각 function에서 실행하는 분할(divide)과 합치는(merge)작업의 비용이 매우 적다고 여겨집니다. Array를 탐색하고 정렬하는 비용이 더 영향력있다. 그래서 function의 실행 횟수(Node의 총갯수)보다 array를 탐색하고 정렬하는 비용이 더 영향력이 있다고 판단하며, array를 탐색하고 정렬하는 횟수에 집중하여 $O(n\\log{n})$이라고 표현합니다. $\\log{n}$은 ‘탐색과 정렬’이 총 몇번 이루어지는지(tree의 depth)에 관한것이고, $n$은 각 단계별로 탐색해야할 array의 길이를 의미합니다.Space Complexity(공간복잡도)\\(O(n)\\)array를 분할하는 과정에서 추가 공간이 필요합니다.Merge sort with $O(1)$ space complexityarray의 element가 ‘양의 정수(unsigned integers)’일때, 수학적 trick을 이용해서 $O(1)$의 알고리즘을 만들 수 있다고 합니다.https://www.geeksforgeeks.org/merge-sort-with-o1-extra-space-merge-and-on-lg-n-time/Implementation첫번째 시도use std::ops::DerefMut;use rand::Rng;fn generate_rand_array(size: usize) -&gt; Vec&lt;u32&gt; { let mut rng = rand::thread_rng(); let mut rand_arr= Vec::with_capacity(size); for i in 0..size { rand_arr.push(rng.gen_range(1..100)); } return rand_arr}fn merge_sort(mut arr: Vec&lt;u32&gt;) -&gt; Vec&lt;u32&gt; { merge_sort_partition(arr.deref_mut()); return arr;}fn merge_sort_partition(arr: &amp;mut [u32]) -&gt; &amp;mut [u32] { if (arr.len() &lt;= 1) { return arr; } let mid = ((arr.len() / 2) as f64).floor() as usize; let (left_arr, right_arr) = arr.split_at_mut(mid); merge_sort_partition(left_arr); merge_sort_partition(right_arr); let sorted_left = left_arr.to_vec(); let sorted_right = right_arr.to_vec(); let mut left_idx = 0; let mut right_idx = 0; for idx in 0..arr.len() { let mut value; let left_val = if left_idx &lt; sorted_left.len() {sorted_left[left_idx % sorted_left.len()]} else { u32::MAX }; let right_val = if right_idx &lt; sorted_right.len() { sorted_right[right_idx % sorted_right.len()] } else { u32::MAX }; if (left_val &lt; right_val) { value = left_val; left_idx += 1; } else { value = right_val; right_idx += 1; } arr[idx] = value; } return arr}fn main() { // random value로 이루어진 array 생성 let mut arr = generate_rand_array(10); println!(\"{:?}\", arr); // [94, 10, 54, 27, 47, 4, 69, 6, 93, 87] 출력 let sorted_arr = merge_sort(arr); println!(\"{:?}\", sorted_arr); // [4, 6, 10, 27, 47, 54, 69, 87, 93, 94] 출력}평가알고리즘은 잘 작동하지만,chatGPT로부터 아래와 같은 feedback을 받았습니다. 불필요한 메모리 사용 제거할것 left_arr.to_vec() 이 부분이 기존 Vec의 slice에 대한 pointer를 기준으로 새로운 Vec를 생성함으로써, 불필요하게 저장공간을 사용하고 있다. deref_mut사용하지 말것. ‘Vec’의 Slice 기능을 사용할것. ‘deref’는 역참조(dereference, pointer가 가리키는 메모리에 접근하여 데이터를 가져오는것.) 값을 반환하는 method로서, 여기선 array의 주소를 function에 argument로 전달하기 위한 맥락으로 사용하였습니다. 이 ‘deref’를 명시적(explicit)으로 사용하는것은 rust에서 anti-pattern으로 여겨집니다. 메모리의 값을 읽는 것은 암시적으로 이루어져야 하는데, 명시적으로 하게되면 코드의 간결함이 떨어지게 됩니다. 이는 코드의 가독성과 안정성을 떨어트리는 요인이 됩니다. 두번째 시도use rand::Rng;fn generate_rand_array(size: usize) -&gt; Vec&lt;u32&gt; { let mut rng = rand::thread_rng(); let mut rand_arr= Vec::with_capacity(size); for _ in 0..size { rand_arr.push(rng.gen_range(1..100)); } return rand_arr}fn merge_sort(mut arr: Vec&lt;u32&gt;) -&gt; Vec&lt;u32&gt; { merge_sort_partition(&amp;mut arr); return arr;}fn merge_sort_partition(arr: &amp;mut [u32]) -&gt; &amp;mut [u32] { if arr.len() &lt;= 1 { return arr; } let mid = ((arr.len() / 2) as f64).floor() as usize; // divide merge_sort_partition(&amp;mut arr[..mid]); merge_sort_partition(&amp;mut arr[mid..]); let mut merged_arr = arr.to_vec(); // rust의 borrow정책때문에 필요. // merge merge(&amp;arr[..mid], &amp;arr[mid..], &amp;mut merged_arr); arr.copy_from_slice(&amp;merged_arr); return arr}fn merge(arr1: &amp; [u32], arr2: &amp; [u32], result_arr: &amp;mut [u32]) { let mut left_idx = 0; let mut right_idx = 0; for idx in 0..result_arr.len() { let value; let left_val = if left_idx &lt; arr1.len() {arr1[left_idx % arr1.len()]} else { u32::MAX }; let right_val = if right_idx &lt; arr2.len() { arr2[right_idx % arr2.len()] } else { u32::MAX }; if left_val &lt; right_val { value = left_val; left_idx += 1; } else { value = right_val; right_idx += 1; } result_arr[idx] = value; }}fn main() { // random value로 이루어진 array 생성 let arr = generate_rand_array(10); println!(\"{:?}\", arr); let sorted_arr = merge_sort(arr); println!(\"{:?}\", sorted_arr);}‘merge’단계를 별도의 function으로 분리하였습니다.이 과정에서 rust의 ‘second mutable borrow occurs here’에러가 발생하였습니다.‘second mutable borrow occurs here’rust에서는 References and Borrowing라는 변수 및 메모리 참조 정책을 사용합니다.borrow checker를 통해 compile단계에서의 메모리 참조 위험을 감지하고 알려줍니다.위 코드에서 merge_sort_partition(&amp;mut arr);의 &amp;mut arr은 변수 arr에 대한 수정 권한을 function에 넘겨주는것과 같습니다.이를 받는 function에선 해당 권한을 1회만 사용할 수 있습니다.때문에, ‘second mutable borrow occurs here’ error는 이를 2회 이상 사용하려 했다는 내용입니다. 이를 해소하기 위해, arr을 별도의 메모리로 copy(여기선 새로운 Vec를 생성했습니다)하고, 함수 마지막에 원본 arr에 정렬된 최종 데이터를 copy함으로써 이 정책을 우회했습니다.Reference Rust ‘References and Borrowing’ https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html" }, { "title": "Bubble sort implementation with rust(rust로 버블정렬 구현)", "url": "/posts/Bubble-sort-implementation-with-rust/", "categories": "Programming, rust", "tags": "programming, rust, sort, algorithm", "date": "2024-08-30 19:00:00 +0900", "snippet": "‘bubble sort’는 가장 기본적인 sort algorithm(정렬 알고리즘)입니다.Time complexity and space complexityTime complexity(시간복잡도)시간 복잡도는 다음과 같습니다   Best case Average Worst case ...", "content": "‘bubble sort’는 가장 기본적인 sort algorithm(정렬 알고리즘)입니다.Time complexity and space complexityTime complexity(시간복잡도)시간 복잡도는 다음과 같습니다   Best case Average Worst case comparisions(비교연산 횟수) $O(n^2)$ $O(n^2)$ $O(n^2)$ swaps(교환연산 횟수) $O(1)$ $O(n^2)$ $O(n^2)$ 구현하는 방법에 따라, Best case의 시간복잡도를 $O(n)$(comparision 횟수)와 $O(1)$(swap 횟수)로 만들 수 있습니다 ‘Best case‘는 array가 이미 정렬되어 있는 경우를 말합니다.Space complexity(공간복잡도)\\(O(1)\\)‘bubble sort’는 array(메모리에서의 연속된 공간을 의미)에서 2개의 값을 교환(swap)하기 때문에,target이 되는 array이외의 추가로 공간이 필요하지 않습니다ImplementationBest case가 $O(n^2)$인 구현use rand::Rng;fn generate_rand_array(size: usize) -&gt; Vec&lt;u32&gt; { let mut rng = rand::thread_rng(); let mut rand_arr= Vec::with_capacity(size); for i in 0..size { rand_arr.push(rng.gen_range(1..100)); } return rand_arr}fn bubble_sort(mut arr: Vec&lt;u32&gt;) -&gt; Vec&lt;u32&gt; { for i in 0..arr.len() { let working_idx = arr.len() - 1 - i; for j in 0..working_idx + 1 { if j &lt; working_idx &amp;&amp; arr[j] &gt; arr[j+1] { arr.swap(j, j + 1) } } } return arr;}fn main() { let arr = generate_rand_array(10); println!(\"{:?}\", arr); let sorted_arr = bubble_sort(arr); println!(\"{:?}\", sorted_arr); return;}Best case가 $O(n)$인 구현use rand::Rng;fn generate_rand_array(size: usize) -&gt; Vec&lt;u32&gt; { let mut rng = rand::thread_rng(); let mut rand_arr= Vec::with_capacity(size); for i in 0..size { rand_arr.push(rng.gen_range(1..100)); } return rand_arr}fn bubble_sort(mut arr: Vec&lt;u32&gt;) -&gt; Vec&lt;u32&gt; { let mut is_swapped: bool = false; for i in 0..arr.len() { let working_idx = arr.len() - 1 - i; for j in 0..working_idx + 1 { if j &lt; working_idx &amp;&amp; arr[j] &gt; arr[j+1] { arr.swap(j, j + 1); is_swapped = true; } } if is_swapped == false { // 1회 iteration후 swap이 일어났는지 판단합니다 return arr; } } return arr;}fn main() { // random value로 이루어진 array 생성 let arr = generate_rand_array(10); println!(\"{:?}\", arr); // 정렬 수행 let sorted_arr = bubble_sort(arr); println!(\"{:?}\", sorted_arr); // 이미 정렬된 array인 경우 let new_sorted_arr = bubble_sort(sorted_arr); println!(\"{:?}\", new_sorted_arr); return;}Reference 여러 sort algorithm의 과정을 animation으로 보기 Visualization and Comparison of Sorting Algorithms source code repository https://github.com/KanghoonYi/algorithm-study/blob/main/rust/src/bin/bubble_sort.rs" }, { "title": "Big-O notation and complexity(big-O 표기법과 복잡도)", "url": "/posts/big-O-and-Time-Complexity/", "categories": "Books, Cracking The Coding Interview", "tags": "Computer Science, algorithm, 알고리즘, Time Complexity", "date": "2024-08-29 19:00:00 +0900", "snippet": "디스크에 있는 파일을 다른 지역의 친구에게 보낸다고 해보자. 대부분의 사람이, 이메일이나 FTP와 같은 network방식을 떠올리게됩니다.하지만 파일의 크기가 1TB라면? 이럴때는 자동차를 타고 직접 전달하는게 빠르지 않을까?이렇게 어떤 행위(컴퓨터에선 알고리즘을 의미)에 대한 비용을 어떻게 비교할까요?여기선 컴퓨터에서 효율성을 비교하는 방법을 알아보...", "content": "디스크에 있는 파일을 다른 지역의 친구에게 보낸다고 해보자. 대부분의 사람이, 이메일이나 FTP와 같은 network방식을 떠올리게됩니다.하지만 파일의 크기가 1TB라면? 이럴때는 자동차를 타고 직접 전달하는게 빠르지 않을까?이렇게 어떤 행위(컴퓨터에선 알고리즘을 의미)에 대한 비용을 어떻게 비교할까요?여기선 컴퓨터에서 효율성을 비교하는 방법을 알아보겠습니다.big-O의 의미big-O 시간은 알고리즘의 효율성을 나타내는 지표(metric) 혹은 언어입니다. 이를 통해 알고리즘이 이전보다 빨라졌는지 느려졌는지 판단합니다.시간복잡도(time complexity)big-O표기법에 따른 그래프위의 그래프처럼 다양한 big-O표기가 있습니다.O(1)(상수의 시간복잡도)가 얼마냐에 따라, 특정 input범위에선, O(1)의 시간복잡도가 더 클 수 있습니다.big-O, big-θ, big-Ω big-O(big-O)(최악의 경우)엄밀히 말하면(그리고 학계에선) big-O는 시간의 상한을 나타냅니다. 또 다른 말로, ‘최악의 경우’를 나타냅니다.예를 들면, array의 모든 값을 std console(표준 출력, console창)에 출력한다면, O(N)으로 표현할 수 있지만, 이 외에 $O(N^2), O(N^3), O(2^N)$도 옳은 표현입니다. 하지만, 업계에선 ‘big-O’를 ‘big-θ’의 의미를 추가하여 사용하고 있습니다.이 부분이 ‘학계에선’이라고 표현한 이유입니다. big-Ω(big-Omega)(최선의 경우)학계에선 big-Ω는 최선(best)의 상황에 대한 표기입니다.예를 들어, array의 모든 값을 출력하는 상황은 Ω(N) 뿐만 아니라 $\\Omega{(\\log{N})}$ 혹은 $\\Omega{(1)}$로 표현할 수 있습니다. Q: 위의 array예시(모든 값을 출력하는)에서, $\\Omega(\\log{N})$, $\\Omega{(1)}$이 가능할까? A: 불가능합니다. 모든 array의 element(구성요소)들을 최소 1회 읽어야 합니다. big-θ(big-theta)(딱 맞게 표현하고 싶은 경우)big-O와 big-Ω 모두를 의미합니다.예를 들어, 어떤 알고리즘이 O(N)이면서 Ω(N)이라면, 이 알고리즘을 θ(N)으로 표기할 수 있습니다. 최선의 경우(big-Ω)는 별로 쓸만한 개념이 아닌 탓에 논의 대상이 되지 않는다. 아무 알고리즘에 대해서 최선에 해당되는 특수한 input을 넣는다면, O(1)에 동작하도록 만들 수 있다.대부분의 경우 최악의 경우(big-O)에 관심이 있기 때문에, big-O로 표기하곤 한다. 공간복잡도(space complexity)알고리즘의 공간 측면에서의 복잡도 입니다.시간복잡도와는 완전히 구분되며, 크기가 n인 배열을 만든다면 O(n)으로, n x n 배열을 만든다면, $O(n^2)$으로 공간복잡도를 표기합니다.// 예제 1int sum(int n) {\tif (n &lt;= 1){\t\treturn 0;\t}\treturn n + sum(n-1);}// 예제 2int pairSumSequence(int n) {\tint sum = 0;\tfor (int i=0; i &lt; n; i++) {\t\tsum += pairSum(i, i + 1);\t}\treturn sum;}int pairSum(int a, int b) {\treturn a + b;} 예제 1의 경우 O(n)의 시간복잡도와 O(n)의 공간복잡도를 갖습니다. 공간복잡도가 O(n)인 이유는, function call이 call stack에 n개 쌓이기 때문입니다. 이 function call또한 memory의 공간을 차지합니다. 단순히 function call을 n번했다고, O(n)의 공간을 사용하지는 않습니다. 예제 2의 경우를 보면, 예제 1과 같이 총 n번 호출하지만, 각 function이 모두 종료(이 과정에서 memory free가 발생하기 때문)된 후 실행되므로, O(1)의 공간을 사용한다. n번 iteration을 수행하기 때문에 O(n)의 공간복잡도를 갖는다고 생각할 수 있지만, n의 증가와 상관없이 상수(constant number)의 공간복잡도(O(1))을 갖기 때문에, O(1)이 맞습니다.big-O는 단순히 증가하는 비율을 표현합니다big-O 표기법의 기본적인 규칙들상수항(constant value)는 무시한다.big-O는 단순히 증가하는 비율을 나타냅니다. 특정 input데이터 안에서 $O(N)$이 $O(1)$보다 작은경우가 있지만(참고), ‘증가하는 비율’을 비교하고자 함으로, 상수는 생략하게 됩니다.이에 따라, 2개의 연속된 iteration(반복)구문(for, while등) 같은 경우$O(2N)$이 아닌 $O(N)$으로 표기합니다. $O(N)$이 항상 $O(2N)$보다 낫지는 않습니다. 이런일이 발생하는 이유는, 실제로 CPU에 전달되는 명령어(assembly어)가 내부 연산(+,* 등)에 따라 차이가 있기 때문입니다.지배적이지 않은 항은 무시하라(가장 영향력이 큰것만 표시한다) $O(N^2+N)$은 $O(N^2)$으로 $O(N+\\log{N})$은 $O(N)$으로 $O(5*2^{N}+1000N^{100})$은 $O(2^N)$으로표기합니다. N의 모수(parameter)가 아예 다른경우($O(B^2+A)$와 같은 경우)는 줄일 수 없습니다.big-O의 N크기에 따른 연산차이를 표현한 graph. 그 성능에 따라 horrible~excellent까지 추가로 표기되어 있습니다. 출처: https://www.bigocheatsheet.com/여러 단계로 이루어진 알고리즘은 어떻게 표현할까?실제로, 알고리즘은 여러 단계의 연산을 거치게 됩니다. 이럴때 시간 복잡도를 어떻게 표현해야 할까요?// O(A + B)for (int a: arrA) {\tprint(a);}for (int b: arrB) {\tprint(b)}// O(A * B)for (int a : arrA) {\tfor (int b : arrB) {\t\tprint(a + \" \" + b)\t}}Amortized Time(Amortized Analysis, 분할 상환 분석)알고리즘이, 상황에 따라 아주 나쁜 시간복잡도를 갖지만, 보통의 경우는 또 다른 시간복잡도를 갖는경우 표현하는 방법에 대해서 알아봅니다.Java의 ‘ArrayList’예시Java의 ‘ArrayList’는 ‘dynamic size array(동적으로 array크기가 바뀌는)’입니다. 즉, array이지만, 배열의 크기가 자유롭게 조절됩니다.‘ArrayList’는 배열의 크기가 가득찼을때, 기존보다 2배 더 큰 array를 만든뒤 이전 array의 모든 요소(element)를 새 array로 copy합니다.이런 spec을 기준으로, ‘ArrayList’의 삽입(insert)연산의 시간복잡도(Time Complexity)는 어떻게 될까?2가지 상황으로 나누어서 생각할 수 있습니다 array가 꽉찬 경우(배열 요소가 N개인 경우): 새로운 element를 만들기 위해, 2N크기의 array를 생성하고, 기존 array에서 N개의 element를 copy해야 하므로, 이 경우 $O(N)$이 됩니다. array가 비어있는 경우: 배열에 이미 공간이 있기 때문에, $O(1)$이 됩니다.이에 따라 최악의 경우인 $O(N)$으로 표기하는게 맞을까요?그러나, 최악의 경우(array가 꽉찬경우)에 마주하는 경우는 극히 드물기 때문에, 실제의 시간복잡도를 온전히 반영하고 있지 않습니다.이에 따라, 이 최악의 경우($O(N)$인 경우)를 분할해서, 다른 상황에 할당하는 방식으로 계산합니다.계속해서 ArrayList의 예시를 이어가면,ArrayList는 1, 2, 4, 8, 16, …, X처럼 $2^n$크기 일때, 최악의 경우($O(N)$)가 발생한다.이때, X개의 element를 삽입한다면, 총 시간복잡도는 $1+2+4+8+16+\\dots+X$이 됩니다.이를 거꾸로 뒤집어서 생각하면, X부터 $\\frac{1}{2}$씩 줄어든다고 볼 수 있습니다. 이를 적용하면,\\[X +\\frac{X}{2}+\\frac{X}{4}+\\frac{X}{8}+\\dots+1\\approx2X\\]로, 2X(근사값으로서)가 됩니다.이에 따라, X개의 원소를 모두 삽입(insert)할때 필요한 시간은 $O(2X)$이고,이를 분할하여, 1회 삽입(insert)에 드는 시간은 $O(1)$입니다.$\\log{N}$ runtime많은 알고리즘에서, $\\log{N}$의 시간복잡도(time complexity)를 갖고 있습니다. 이 $\\log{N}$은 어떻게 구해진걸까요?Binary Search Tree(BST, 이진 탐색 트리)binary search tree 예시 출처: https://en.wikipedia.org/wiki/Binary_search‘Binary Search Tree(BST)’는 특정 element를 찾을때, 그 탐색 대상을 절반($\\frac{1}{2}$)씩 줄여나갑니다.예를 들어, 탐색 대상(N)이 총 16개인 경우($N = 16$),$N=16$(첫번째 탐색대상)$N=8$(두번째 탐색대상)$N=4$(세번째 탐색대상)$N=2$(네번째 탐색대상)$N=1$(다섯번째 탐색대상, 찾음)로 탐색 대상이 각 단계 마다 $\\frac{1}{2}$씩 줄어듭니다. 이를 거꾸로 뒤집으면, 마지막으로 부터 2배씩 증가한다고 볼 수 있습니다.이때, ‘탐색 횟수’가 시간복잡도에 해당되며, 이는 $2^{k} = N \\text(k는 실행횟수)$를 만족하는 k를 찾는것과 같습니다.\\[2^k=N\\rightarrow\\log_2{N}=k\\]이렇게 실행횟수 k는 $\\log_2{N}$이 됩니다. 이 BST와 같이 알고리즘의 대상이 $\\frac{1}{2}$씩 줄어든다면, 해당 알고리즘의 시간복잡도는 $\\log{N}$일 가능성이 높습니다. big-O표기에서는 상수항은 무시하기 때문에, $\\log_2$의 밑에 해당하는 ‘2’는 생략하고 표시합니다. 즉,$\\log(N)$으로 표기합니다.Recursive runtime(재귀호출 수행시간)Recursive 알고리즘 같은경우, 시간복잡도(time complexity)를 구하기 까다롭습니다.int f(int n) {\tif (n &lt;= 1) {\t\treturn 1;\t}\treturn f(n - 1) + f(n - 1);}위와 같은 함수가 있을때, 성급하게 $O(N^2)$라고 할 수 있지만(f가 2회 호출되기 때문에), 이는 잘못된 답입니다.이 function을 수행하는 과정을 그려보면, 아래와 같이 tree형식으로 표현할 수 있습니다.이때, function call에 해당하는 ‘Node의 갯수’와 ‘tree의 깊이(depth)’의 관계는 아래와 같이 표현됩니다. 깊이(depth) 노드의 갯수(function call 횟수) 다르게 표현하면 또 다른 표현 0 1   2^0 1 2 2 * 이전 깊이 2^1 2 4 2 * 이전 깊이 2^2 3 8 2 * 이전 깊이 2^3 4 16 2 * 이전 깊이 2^4 이렇게 각 layer마다 node의 갯수는 $2^{depth}$가 됩니다.시간복잡도(Time complexity)는 이 node갯수의 총 합에 해당하므로,\\[2^0+2^1+2^2+2^3+2^4+\\dots+2^N(=2^{N+1}-1)\\]즉 시간복잡도(Time complexity)는\\[O(2^{N})\\]이 된다. recursive함수에서, 시간복잡도는 보통 $O(\\text{분기 갯수}^{깊이})$로 표현됩니다.공간복잡도는 $O(N)$인데,이는 특정 시각에 사용하고 있는 공간의 크기가 $O(N)$이기 때문입니다.Recursive Algorithm의 최적화Recursive Algorithm의 경우, 단순히 ‘recursion’만 적용하면, $O(2^{n})$의 Time complexity(시간복잡도)를 갖게 됩니다.fn fibonacci(n: u32) -&gt; u32 { if n &lt;= 1 { return n } return fibonacci(n-1) + fibonacci(n-2)}하지만, fibonacci 처럼, 같은 계산을 중복으로 실행해야하는 경우가 많은 Algorithm은, 이 결과 값을 저장(cache)함으로서 성능을 개선할 수 있습니다.이 경우, Time Complexity(시간복잡도)를 $O(n)$으로 줄일 수 있습니다.// 메모이제이션을 적용한 피보나치 함수fn fibonacci_memo(n: u32, memo: &amp;mut HashMap&lt;u32, u32&gt;) -&gt; u32 { if let Some(&amp;result) = memo.get(&amp;n) { return result; } let result = if n &lt;= 1 { n } else { fibonacci_memo(n - 1, memo) + fibonacci_memo(n - 2, memo) }; memo.insert(n, result); result}fn main() { let mut memo = HashMap::new(); let n = 10; // 계산할 n 값 let result = fibonacci_memo(n, &amp;mut memo); println!(\"Fibonacci({}) = {}\", n, result);}Reference Book CRACKING the CODING INTERVIEW 이미지 출처 Big-O Algorithm Complexity Cheat Sheet (Know Thy Complexities!) @ericdrowell" }, { "title": "Vector Similarity Explained(Vector의 유사도를 측정하는 방법에 관하여)", "url": "/posts/Vector-Similarity-Explained/", "categories": "ML, MachineLearningBootcamp", "tags": "ML, Vector, NLP, ComputerVision, Similarity", "date": "2024-08-27 19:00:00 +0900", "snippet": "목적‘vector embedding’은 NLP와 computer vision에서 그 효과를 입증했습니다.여기서는 이 ‘vector embedding’을 비교(check similarity)하여 metric으로 만드는 3가지 방법을 알아봅니다.Euclidean distance‘Euclidean distance’는 두개의 vector사이의 거리를 측정하는...", "content": "목적‘vector embedding’은 NLP와 computer vision에서 그 효과를 입증했습니다.여기서는 이 ‘vector embedding’을 비교(check similarity)하여 metric으로 만드는 3가지 방법을 알아봅니다.Euclidean distance‘Euclidean distance’는 두개의 vector사이의 거리를 측정하는 방법입니다.기존에 알고 있던, 유클리드 좌표기반의 ‘두점사이의 거리’를 구하는것과 같습니다.\\[d(a,b)= \\sqrt{(a_1​−b_1​)^2+(a_2​−b_2​)^2+...+(a_n​−b_n​)^2​}\\]이렇게 \\(a_1,b_1\\)부터 \\(a_n,b_n\\)까지의 모든 component(vector의 property)들을 계산해야합니다.sensitive to scale이 ‘Euclidean distance’는 vector의 크기(절대값)에 영향을 많이 받습니다.즉, 크기가 큰 vector들의 ‘euclidean distance’의 값이 작은 vector들의 값보다 큽니다. 2개의 vector가 유사하게 보여도, ‘euclidean distance’에선 차이를 보입니다.‘Euclidean distance’는 문자 그대로, vector사이의 거리를 구하는 것이기 때문에, 매우 직관적인 metric입니다. 이 ‘Euclidean distance’가 작다면, 실제로 두 vector의 거리가 아주 가깝다는것을 의미합니다. 이는 밑에 기술할, ‘Dot product’와 ‘Consine’ 일반적인(generally) 사실(true)이 아닙니다.ML에서의 평가DLM(deep learning model)에서는 자주 쓰지 않습니다.가장 기본적인 vector encoding 도구인 LSH (Locality Sensitive Hashing)같은곳에서 사용하고 있습니다.또, model을 train하는 과정에서, 특별히 정해진 loss function이 없다면 사용되곤 합니다.(직관적이라 접근성이 좋아서 자연스럽게 사용하게 된다는 뜻)‘Euclidean distance’가 vector크기에 민감(sensitive)하다는 면 때문에, 대상의 갯수나 크기가 정보가 데이터(training set과 같은 학습용 데이터)에 포함되어야 할때 사용되곤 합니다.예를 들면, 유저의 이전 구매이력을 기초로하는 추천시스템(recommendation system)과 같은것이 그렇습니다.이때는, 이미 구입한 상품과 절대적 거리를 측정하는 방법을 사용합니다.Dot product Similarity(vector 내적)vector의 dot product를 metric으로 사용하는 방법입니다.‘dot product’를 계산하는 방법은 아래와 같이 2가지 방법이 있습니다. 여기선, 2번째 방법인, vector의 크기와 cosine을 이용한 방법을 사용합니다.\\[a⋅b=\\sum_{\\substack{i=1}}^n{a_{i}b_{i}}=a_{1}b_{1}+a_{2}b_{2}+…+a_{n}b_{n}\\]\\[a⋅b=∣a∣∣b∣\\cos{\\alpha}\\] ‘Dot product’의 결과 값은 scalar값입니다(vector가 아닙니다). \\(\\alpha\\)가 90도 미만이면 +(positive) 값을 갖고, 90도보다 크다면, -(negative) 값을 갖고, 90도(수직, orthogonal)이면, 0이 됩니다. 이는 \\(\\cos{\\alpha}\\)에 따라 값이 바뀌는것을 말합니다. ‘Dot product’는 두 vector의 크기(length)와 방향(direction)모두에게 영향을 받습니다. 만약, 2개의 vector가 크기는 갖고, 방향이 반대(opposite)인 것보다, 방향이 동일한것이 더 큽니다.ML에서의 평가LLMs(Large Language models)와 같은 곳에서 자주 보게 됩니다.고도화된 추천시스템(recommender system)에서도 사용하게 됩니다.이 고도화된 시스템에선, 모든 유져(user)와 상품(item)이 ‘embedding’되어 있으며, model이 ‘user embedding’과 ‘item embedding’의 ‘dot product’를 학습하여 좋은 성능(item과 user의 매칭정도를 측정할때의 성능)을 내게 됩니다.만약 2개의 product가 같은 방향을 가리키지면, 다른 크기(magnitude)를 갖고 있다면, 이 2개의 상품은 같은 topic을 갖고 있지만, vector의 크기가 큰 상품을 더 좋게 평가합니다.Cosine Similarity‘Cosine Similarity’는 2개의 vector사이의 각도(angle)을 metric으로 사용하는 방법입니다.이 ‘Cosine Similarity’는 vector의 크기에 영향을 받지않고, 오직 각도(angle) 혹은 방향(direction)에만 영향을 받습니다.\\[\\text{sim}(a,b)=\\frac{a\\cdot{b}}{\\|a\\|\\cdot{\\|b\\|}}\\] ‘Cosine Similarity’는 -1에서 1사이의 값을 갖는다. 1은 $\\alpha$가 0이라는 뜻이고, 0은 수직(orthogonal) -1은 2개의 vector가 완전히 반대방향이라는 뜻입니다.만약, 이 ‘Cosine Similarity’을 사용한 model을 사용하고 있다면, 동일하게 사용해줘야 합니다. 또한, ‘normalize와 dot product’를 조합한것도 수학적으로 동일하기 때문에, 가능합니다.어떤 경우에는 ‘normalize후 dot product하는 것’이 더 나을때가 있고, ‘cosine similarity’가 나을때도 있습니다.ML에서의 평가‘Cosine similarity’는 ‘semantic search(의미를 통한 검색)’과 ‘document classification problems’에서 사용합니다. 이는 ‘cosine similarity’가 2개의 vector의 방향성만 비교하는 특징을 이용한것입니다.유사하게, ‘recommendation system’에서 유저의 과거 행동기록을 기반한 상품을 추천할때도 사용할 수 있습니다.반면에, magnitude(크기, 량) of vector’가 중요하고, 반드시 이 값이 ‘similarity’에 반영되어야 하는 경우에는 적합하지 않습니다.예를 들면, pixel intensities에 기반한 ‘image embeddings’ 유사도 비교와 같은 경우가 있습니다.Wrap-upModel이 train할때 사용한 ‘Similarity’ metric과 동일한 metric을 사용하는것이 중요합니다.만약, model에서 사용된 metric을 모른다면, 여러 metric중에 적당한걸 찾기위해 여러 실험을 거칠필요가 있습니다.ReferenceVector Similarity ExplainedDifferent types of Distances used in Machine Learning이미지 자료 출처Vector Similarity Explained" }, { "title": "V8 engine build and check machine code(V8 engine을 build하고 machine code 확인하기)", "url": "/posts/V8-engine-build-and-check-machine-code/", "categories": "Typescript, Javascript", "tags": "javascript, machine code, cpu, nodejs, v8", "date": "2024-07-05 14:10:02 +0900", "snippet": "Google의 V8엔진을 통해, javascript는 새로운 기회를 얻었습니다.다른 script언어들과 다르게, javascript코드를 compile하여 machine code로 cache함으로써 성능을 극한으로 끌어 올렸습니다.문득, V8이 생성하는 ‘Machine code’라는게 궁금해졌습니다.그래서 직접 Machine code를 확인하고자 합니...", "content": "Google의 V8엔진을 통해, javascript는 새로운 기회를 얻었습니다.다른 script언어들과 다르게, javascript코드를 compile하여 machine code로 cache함으로써 성능을 극한으로 끌어 올렸습니다.문득, V8이 생성하는 ‘Machine code’라는게 궁금해졌습니다.그래서 직접 Machine code를 확인하고자 합니다.What is ‘Machine code(기계어)’?‘Machine code’는 binary형식의 데이터이긴 하지만, CPU가 직접 읽고 실행할 수 있는 데이터를 말합니다.우리에게 익숙한 각종 파일들이 binary데이터 형식이긴 하지만, CPU가 직접 읽을 수 있는 명령들은 아닙니다.반면에, ‘Machine code’는 CPU가 읽을 수 있는 CPU의 API로 이루어진 데이터입니다. 이에 따라, CPU Architecture에 따라 ‘Machine code’는 바뀌게 됩니다.V8에서 생성하는 ‘Machine Code’ 확인하기V8은 C++ API를 지원합니다. 때문에, 이 C++ API에서 machine code를 출력해줄 수 있는 API가 있나 확인하려 했습니다. 이 C++ API를 통해, Rust에서도 v8을 사용할 수 있으며, 이를 이용해서 javascript를 rust안에서 실행할 수 있습니다.첫번째 시도rust에서 v8 crate(package)가 있어서, 이를 통해 complie결과물을 출력하고자 하였습니다.v8 - Rustlet platform = v8::new_default_platform(0, false).make_shared();v8::V8::initialize_platform(platform);v8::V8::initialize();let isolate = &amp;mut v8::Isolate::new(Default::default());let scope = &amp;mut v8::HandleScope::new(isolate);let context = v8::Context::new(scope, Default::default());let scope = &amp;mut v8::ContextScope::new(scope, context);let code = v8::String::new(scope, \"'Hello' + ' World!'\").unwrap();println!(\"javascript code: {}\", code.to_rust_string_lossy(scope));let script = v8::Script::compile(scope, code, None).unwrap();let result = script.run(scope).unwrap();let result = result.to_string(scope).unwrap();println!(\"result: {}\", result.to_rust_string_lossy(scope));여기서 compile결과물에 해당하는 ‘script’변수에 ‘machine code를 확인할 수 있는 method가 있지 않을까?’라고 생각했습니다.기대하는 method는 없었습니다.두번째 시도v8을 debug모드로 build하면, stdout(여기선 console)을 통해, ‘machine code’를 확인할 수 있다고 합니다.이를 위해선, 먼저 v8을 직접 build해야 합니다. v8 build환경 갖추기 git clone을 통해 v8의 source를 copy해서 시작할 수 있지만,Building V8 from source · V8 Don’t just git clone either of these URLs! if you want to build V8 from your checkout, instead follow the instructions below to get everything set up correctly. 이렇게 ‘정확하게(correctly)’ 설정하기 위해, 별도의 지시사항을 따라야 합니다. depot_tools를 세팅해야 합니다. ‘depot_tools’는 chromium작업할때 필요한, git workflow tools를 포함하고 있는 프로젝트입니다. ‘depot_tools’의 소스코드를 clone합니다. git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 우리가 사용하고 있는 shell이 ‘depot_tools’의 실행파일들을 찾을 수 있도록 환경변수(Environment Variable)를 설정합니다. ## bash shell을 사용하는 경우 echo 'export PATH=$PATH:/path/to/depot_tools' &gt;&gt; ~/.bashrc ## zsh를 사용하는 경우 echo 'export PATH=$PATH:/path/to/depot_tools' &gt;&gt; ~/.zshrc 이제 shell을 이용해서 어느 경로에 있든 ‘depot_tools’에 있는 기능들을 사용할 수 있습니다. v8 source code를 fetch합니다. ‘fetch’는 ‘depot_tools’에 포함된 기능입니다. 이 과정에서 web browser에서 구글 인증을 요구할 수 있습니다. shell에 뜬 url에 접속하여 인증을 완료하면 됩니다. mkdir ~/v8 ## v8 folder를 만듭니다. cd ~/v8 fetch v8 ## 'depot_tools'의 fetch기능을 이용해서 v8을 불러옵니다. 이 과정에서 v8 build를 위한 git head가 자동으로 선택됩니다. cd v8 v8을 debug모드로 build하기 build를 하기전, v8 source code경로가 있는 곳으로 갑니다. build를 하기전, 빌드에 사용할 구성을 만들어냅니다. ‘tools/dev/v8gen.py’ python script를 사용하여, debug 모드로 bulid하기 위한 구성을 만들어 냅니다.(각종 flag를 설정하기 위함.) tools/dev/v8gen.py x64.debug 이 결과고 out.gn/x64.debug 경로가 생성되었을겁니다. v8 build flag를 추가해줘야 합니다. 이는 v8이 ‘Machine code’를 stdout에 출력하도록 하기 위함입니다.out.gn/x64.debug/args.gn 파일에 아래와 같이 flag를 추가합니다. ## file. ./out.gn/x64.debug/args.gn is_debug = true target_cpu = \"x64\" v8_enable_backtrace = true v8_enable_slow_dchecks = true v8_optimized_debug = false ## 추가된 flag들 v8_enable_disassembler = true ## v8이 JIT 컴파일러가 생성한 코드를 disassemble하여 출력할 수 있도록 합니다. v8_enable_object_print = true v8_enable_verify_heap = true a에서 설정한 argument를 기반으로 v8 build하기 (엄청 오래 걸립니다..) cd ~/v8 ## v8 source code가 있는 폴더로 이동합니다. ninja -C out.gn/x64.debug/ d8 v8의 debugging용 버전임을 나타내기 위해, ‘d8’로 명명해서 사용합니다. ## execution example ✔ 13:58 ~/IdeaProjects/v8 [ :32289f80f4d | …4 ] $ ninja -C out.gn/x64.debug/ d8 ninja: Entering directory `out.gn/x64.debug/' [0/1] Regenerating ninja files [1893/2449] CXX obj/v8_base_without_compiler/wasm-features.o 이제 build한 v8엔진(d8)을 가지고 javascript 파일을 실행합니다. out.gn/x64.debug/d8 --print-opt-code --print-code your_script.js --print-code: v8이 생선한 모든 코드를 출력합니다. --print-opt-code: 최적화된 함수에 대한 코드만 출력합니다. 이때, ‘your_script.js’내용이 너무 간단하면, v8은 컴파일하지 않고, 인터프리터를 이용해 실행할 가능성이 높습니다. 때문에, 최적화 대상이 되도록(JIT를 사용하여 컴파일 하도록) code를 생성해야 합니다. 아래는 example로 사용할 javascript code입니다. function add(a, b) { return a + b; } for (let i = 0; i &lt; 100000; i++) { add(i, i + 1); } 출력 결과두번째 시도의 최종 shell command를 실행하고 나면, 아래와 같은 출력이 console에 나타납니다. Raw source 실행한 javascript source code를 나타냅니다. Optimized code JIT 컴파일러로 최적화한 code를 나타냅니다.compiler = turbofan: 최적화를 위한 compiler로 turbofan이 사용되었습니다.Instructions: 컴파일된 ‘machine code’를 나타냅니다. 0x178340c00 0 488d1df9ffffff REX.W leaq rbx,[rip+0xfffffff9] 명령어가 저장된 메모리 주소 byte offset. 이 명령어가 시작 부분에 위치하고 있음을 나타냄. 우측의 assembly명령어에 해당하는 16진수 값. CPU가 인식하는 명령어 머신 코드의 어셈블리 표현. --- Raw source ---function add(a, b) { return a + b;}for (let i = 0; i &lt; 100000; i++) { add(i, i + 1);}--- Optimized code ---optimization_id = 2source_position = 0kind = TURBOFANstack_slots = 16compiler = turbofanaddress = 0x134800040375Instructions (size = 644)0x178340c00 0 488d1df9ffffff REX.W leaq rbx,[rip+0xfffffff9]0x178340c07 7 483bd9 REX.W cmpq rbx,rcx0x178340c0a a 740d jz 0x178340c19 &lt;+0x19&gt;0x178340c0c c ba84000000 movl rdx,0x840x178340c11 11 41ff9568560000 call [r13+0x5668]0x178340c18 18 cc int3l0x178340c19 19 8b59f4 movl rbx,[rcx-0xc]0x178340c1c 1c 490b9de0010000 REX.W orq rbx,[r13+0x1e0]0x178340c23 23 f6431a20 testb [rbx+0x1a],0x200x178340c27 27 0f85933407a0 jnz 0x1183b40c0 (CompileLazyDeoptimizedCode) ;; near builtin entry0x178340c2d 2d 55 push rbp0x178340c2e 2e 4889e5 REX.W movq rbp,rsp0x178340c31 31 56 push rsi0x178340c32 32 57 push rdi0x178340c33 33 50 push rax0x178340c34 34 ba58000000 movl rdx,0x580x178340c39 39 41ff9568560000 call [r13+0x5668]0x178340c40 40 cc int3l0x178340c41 41 4883ec18 REX.W subq rsp,0x180x178340c45 45 488975a0 REX.W movq [rbp-0x60],rsi0x178340c49 49 493b65a0 REX.W cmpq rsp,[r13-0x60] (external value (StackGuard::address_of_jslimit()))0x178340c4d 4d 0f8622010000 jna 0x178340d75 &lt;+0x175&gt;0x178340c53 53 488b4dc0 REX.W movq rcx,[rbp-0x40]0x178340c57 57 f6c101 testb rcx,0x10x178340c5a 5a 0f85ea010000 jnz 0x178340e4a &lt;+0x24a&gt;0x178340c60 60 81f9400d0300 cmpl rcx,0x30d400x178340c66 66 0f8c1e000000 jl 0x178340c8a &lt;+0x8a&gt;0x178340c6c 6c 488b45c8 REX.W movq rax,[rbp-0x38]0x178340c70 70 488b4de8 REX.W movq rcx,[rbp-0x18]0x178340c74 74 488be5 REX.W movq rsp,rbp0x178340c77 77 5d pop rbp0x178340c78 78 4883f901 REX.W cmpq rcx,0x10x178340c7c 7c 7f03 jg 0x178340c81 &lt;+0x81&gt;0x178340c7e 7e c20800 ret 0x80x178340c81 81 415a pop r100x178340c83 83 488d24cc REX.W leaq rsp,[rsp+rcx*8]0x178340c87 87 4152 push r100x178340c89 89 c3 retl0x178340c8a 8a 488bf9 REX.W movq rdi,rcx0x178340c8d 8d d1ff sarl rdi, 10x178340c8f 8f 4c8bc7 REX.W movq r8,rdi0x178340c92 92 4183c001 addl r8,0x10x178340c96 96 0f80b2010000 jo 0x178340e4e &lt;+0x24e&gt;0x178340c9c 9c 4103f8 addl rdi,r80x178340c9f 9f 0f80ad010000 jo 0x178340e52 &lt;+0x252&gt;0x178340ca5 a5 41807db100 cmpb [r13-0x4f] (external value (StackGuard::address_of_interrupt_request(StackGuard::InterruptLevel::kNoH0x178340caa aa 0f85ee000000 jnz 0x178340d9e &lt;+0x19e&gt;0x178340cb0 b0 660f1f840000000000 nop0x178340cb9 b9 0f1f8000000000 nop0x178340cc0 c0 4181f8a0860100 cmpl r8,0x186a00x178340cc7 c7 0f8d95000000 jge 0x178340d62 &lt;+0x162&gt;0x178340ccd cd 498bc8 REX.W movq rcx,r80x178340cd0 d0 83c101 addl rcx,0x10x178340cd3 d3 0f807d010000 jo 0x178340e56 &lt;+0x256&gt;0x178340cd9 d9 498bf8 REX.W movq rdi,r80x178340cdc dc 03f9 addl rdi,rcx0x178340cde de 0f8076010000 jo 0x178340e5a &lt;+0x25a&gt;0x178340ce4 e4 81f9a0860100 cmpl rcx,0x186a00x178340cea ea 0f8d72000000 jge 0x178340d62 &lt;+0x162&gt;0x178340cf0 f0 4c8bc1 REX.W movq r8,rcx0x178340cf3 f3 4183c001 addl r8,0x10x178340cf7 f7 0f8061010000 jo 0x178340e5e &lt;+0x25e&gt;0x178340cfd fd 488bf9 REX.W movq rdi,rcx0x178340d00 100 4103f8 addl rdi,r80x178340d03 103 0f8059010000 jo 0x178340e62 &lt;+0x262&gt;0x178340d09 109 4181f8a0860100 cmpl r8,0x186a00x178340d10 110 0f8d4c000000 jge 0x178340d62 &lt;+0x162&gt;0x178340d16 116 498bc8 REX.W movq rcx,r80x178340d19 119 83c101 addl rcx,0x10x178340d1c 11c 0f8044010000 jo 0x178340e66 &lt;+0x266&gt;0x178340d22 122 498bf8 REX.W movq rdi,r80x178340d25 125 03f9 addl rdi,rcx0x178340d27 127 0f803d010000 jo 0x178340e6a &lt;+0x26a&gt;0x178340d2d 12d 81f9a0860100 cmpl rcx,0x186a00x178340d33 133 0f8d29000000 jge 0x178340d62 &lt;+0x162&gt;0x178340d39 139 4c8bc1 REX.W movq r8,rcx0x178340d3c 13c 4183c001 addl r8,0x10x178340d40 140 0f8028010000 jo 0x178340e6e &lt;+0x26e&gt;0x178340d46 146 488bf9 REX.W movq rdi,rcx0x178340d49 149 4103f8 addl rdi,r80x178340d4c 14c 0f8020010000 jo 0x178340e72 &lt;+0x272&gt;0x178340d52 152 41807db100 cmpb [r13-0x4f] (external value (StackGuard::address_of_interrupt_request(StackGuard::InterruptLevel::kNoH0x178340d57 157 0f8571000000 jnz 0x178340dce &lt;+0x1ce&gt;0x178340d5d 15d e95effffff jmp 0x178340cc0 &lt;+0xc0&gt;0x178340d62 162 488bcf REX.W movq rcx,rdi0x178340d65 165 03cf addl rcx,rdi0x178340d67 167 0f808e000000 jo 0x178340dfb &lt;+0x1fb&gt;0x178340d6d 16d 488bc1 REX.W movq rax,rcx0x178340d70 170 e9fbfeffff jmp 0x178340c70 &lt;+0x70&gt;0x178340d75 175 b9a0000000 movl rcx,0xa00x178340d7a 17a 51 push rcx0x178340d7b 17b 48bb90dd5d1a01000000 REX.W movq rbx,0x11a5ddd90 ;; external reference (Runtime::StackGuardWithGap)0x178340d85 185 b801000000 movl rax,0x10x178340d8a 18a 48be851a2800ee040000 REX.W movq rsi,0x4ee00281a85 ;; object: 0x04ee00281a85 &lt;NativeContext[300]&gt;0x178340d94 194 e867ee40a0 call 0x11874fc00 (CEntry_Return1_ArgvOnStack_NoBuiltinExit) ;; near builtin entry0x178340d99 199 e9b5feffff jmp 0x178340c53 &lt;+0x53&gt;0x178340d9e 19e 48bb50d75d1a01000000 REX.W movq rbx,0x11a5dd750 ;; external reference (Runtime::HandleNoHeapWritesInterrupts)0x178340da8 1a8 33c0 xorl rax,rax0x178340daa 1aa 48be851a2800ee040000 REX.W movq rsi,0x4ee00281a85 ;; object: 0x04ee00281a85 &lt;NativeContext[300]&gt;0x178340db4 1b4 48897d90 REX.W movq [rbp-0x70],rdi0x178340db8 1b8 4c894598 REX.W movq [rbp-0x68],r80x178340dbc 1bc e83fee40a0 call 0x11874fc00 (CEntry_Return1_ArgvOnStack_NoBuiltinExit) ;; near builtin entry0x178340dc1 1c1 488b7d90 REX.W movq rdi,[rbp-0x70]0x178340dc5 1c5 4c8b4598 REX.W movq r8,[rbp-0x68]0x178340dc9 1c9 e9f2feffff jmp 0x178340cc0 &lt;+0xc0&gt;0x178340dce 1ce 48897d90 REX.W movq [rbp-0x70],rdi0x178340dd2 1d2 4c894598 REX.W movq [rbp-0x68],r80x178340dd6 1d6 488b1dc3ffffff REX.W movq rbx,[rip+0xffffffc3]0x178340ddd 1dd 33c0 xorl rax,rax0x178340ddf 1df 48be851a2800ee040000 REX.W movq rsi,0x4ee00281a85 ;; object: 0x04ee00281a85 &lt;NativeContext[300]&gt;0x178340de9 1e9 e812ee40a0 call 0x11874fc00 (CEntry_Return1_ArgvOnStack_NoBuiltinExit) ;; near builtin entry0x178340dee 1ee 488b7d90 REX.W movq rdi,[rbp-0x70]0x178340df2 1f2 4c8b4598 REX.W movq r8,[rbp-0x68]0x178340df6 1f6 e9c5feffff jmp 0x178340cc0 &lt;+0xc0&gt;0x178340dfb 1fb c5832ac7 vcvtlsi2sd xmm0,xmm15,rdi0x178340dff 1ff 498b4d48 REX.W movq rcx,[r13+0x48] (external value (Heap::NewSpaceAllocationTopAddress()))0x178340e03 203 c5fb1145a0 vmovsd [rbp-0x60],xmm00x178340e08 208 488d790c REX.W leaq rdi,[rcx+0xc]0x178340e0c 20c 49397d50 REX.W cmpq [r13+0x50] (external value (Heap::NewSpaceAllocationLimitAddress())),rdi0x178340e10 210 0f8713000000 ja 0x178340e29 &lt;+0x229&gt;0x178340e16 216 ba0c000000 movl rdx,0xc0x178340e1b 21b e8e04707a0 call 0x1183b5600 (AllocateInYoungGeneration) ;; near builtin entry0x178340e20 220 488d48ff REX.W leaq rcx,[rax-0x1]0x178340e24 224 c5fb1045a0 vmovsd xmm0,[rbp-0x60]0x178340e29 229 488d790c REX.W leaq rdi,[rcx+0xc]0x178340e2d 22d 49897d48 REX.W movq [r13+0x48] (external value (Heap::NewSpaceAllocationTopAddress())),rdi0x178340e31 231 4883c101 REX.W addq rcx,0x10x178340e35 235 c741ff6d050000 movl [rcx-0x1],0x56d0x178340e3c 23c c5fb114103 vmovsd [rcx+0x3],xmm00x178340e41 241 488bc1 REX.W movq rax,rcx0x178340e44 244 e927feffff jmp 0x178340c70 &lt;+0x70&gt;0x178340e49 249 90 nop0x178340e4a 24a 41ff55d0 call [r13-0x30]0x178340e4e 24e 41ff55d0 call [r13-0x30]0x178340e52 252 41ff55d0 call [r13-0x30]0x178340e56 256 41ff55d0 call [r13-0x30]0x178340e5a 25a 41ff55d0 call [r13-0x30]0x178340e5e 25e 41ff55d0 call [r13-0x30]0x178340e62 262 41ff55d0 call [r13-0x30]0x178340e66 266 41ff55d0 call [r13-0x30]0x178340e6a 26a 41ff55d0 call [r13-0x30]0x178340e6e 26e 41ff55d0 call [r13-0x30]0x178340e72 272 41ff55d0 call [r13-0x30]0x178340e76 276 41ff55d8 call [r13-0x28]0x178340e7a 27a 41ff55d8 call [r13-0x28]0x178340e7e 27e 41ff55d8 call [r13-0x28]0x178340e82 282 6690 nopInlined functions (count = 1) 0x04ee00298ee5 &lt;SharedFunctionInfo add&gt;Deoptimization Input Data (deopt points = 14) index bytecode-offset node-id pc 0 21 31 NA 1 21 50 NA 2 2 60 NA 3 21 88 NA 4 2 98 NA 5 21 111 NA 6 2 122 NA 7 21 135 NA 8 2 145 NA 9 21 158 NA 10 2 169 NA 11 47 18 199 12 47 68 1c1 13 47 177 1eeSafepoints (entries = 4, byte size = 32)0x178340d99 199 slots (sp-&gt;fp): 00100011 deopt 11 trampoline: 2760x178340dc1 1c1 slots (sp-&gt;fp): 00100000 deopt 12 trampoline: 27a0x178340dee 1ee slots (sp-&gt;fp): 00100000 deopt 13 trampoline: 27e0x178340e20 220 slots (sp-&gt;fp): 00000000RelocInfo (size = 19)0x178340c29 near builtin entry0x178340d7d external reference (Runtime::StackGuardWithGap) (0x11a5ddd90)0x178340d8c full embedded object (0x04ee00281a85 &lt;NativeContext[300]&gt;)0x178340d95 near builtin entry0x178340da0 external reference (Runtime::HandleNoHeapWritesInterrupts) (0x11a5dd750)0x178340dac full embedded object (0x04ee00281a85 &lt;NativeContext[300]&gt;)0x178340dbd near builtin entry0x178340de1 full embedded object (0x04ee00281a85 &lt;NativeContext[300]&gt;)0x178340dea near builtin entry0x178340e1c near builtin entry--- End code ---References v8 build를 위한 참고문서 Documentation · V8 Machine code Machine code Understanding V8’s Bytecode Understanding V8’s Bytecode" }, { "title": "How to make a blog by using Jekyll(Jekyll을 사용해서 blog를 만드는 방법)", "url": "/posts/How-to-make-a-blog-by-using-Jekyll/", "categories": "Guide, Blog", "tags": "blog, github, site", "date": "2023-11-02 19:00:00 +0900", "snippet": "OverviewEngineer라면 한번쯤 ‘개인 Blog를 운영해야하나?’라는 생각이 들때가 있습니다.이때, 쉽게 blog를 호스팅(hosting)하는 방법중 하나인 Github Pages를 사용하게 되는데, Jekyll를 통해 그 내용을 구성하게 됩니다.여기선, ‘Jekyll’의 개념과 이를 이용해서 Blog를 만드는 방법을 다룹니다. 여기선, M...", "content": "OverviewEngineer라면 한번쯤 ‘개인 Blog를 운영해야하나?’라는 생각이 들때가 있습니다.이때, 쉽게 blog를 호스팅(hosting)하는 방법중 하나인 Github Pages를 사용하게 되는데, Jekyll를 통해 그 내용을 구성하게 됩니다.여기선, ‘Jekyll’의 개념과 이를 이용해서 Blog를 만드는 방법을 다룹니다. 여기선, MacOS기준으로 작성합니다. 다른 guide는 link로 첨부합니다.Jekyll을 이해하자(Jekyll의 개념)Jekyll은 규칙에 따라 text를 작성하면, 이 내용을 기반으로 static website를 만들어줍니다.프로그래밍 언어는 Ruby로 되어 있으며, RubyGems를 통해 배포됩니다. RubyGems는 Ruby에서 사용하는 Package관리자 입니다. 동명의 package배포를 위한 서버도 제공합니다. 여기선, RubyGem을 이용한 운영방법을 제시합니다. 이는 앞으로 있을 package update를 쉽게 반영하기 위함입니다.설치(Installation)Jekyll 설치Jekyll설치에 앞서, Jekyll이 구동되는 환경인 Ruby환경을 만들어야 합니다.Jekyll을 설치하는 과정은, ‘Ruby를 설치하는 과정’과 ‘Jekyll을 설치하는 과정’으로 2개의 단계로 진행됩니다. 원문은 Jekyll 설치 가이드에서 확인하실 수 있습니다Ruby 설치 MacOS의 ‘Command Line Tools’를 설치해야 합니다. 여러 개발환경에서 요구하는 만큼, 이미 설치되어 있을 수 있습니다. $ xcode-select --install Homebrew를 사용하여 ruby를 설치합니다. rbenv를 통해 여러개의 ruby버젼을 하나의 컴퓨터에서 사용할 수 있게 합니다. # Homebrew 설치 $ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" # rbenv 와 ruby-build 설치 $ brew install rbenv # 쉘 환경에 rbenv 가 연동되도록 설정 $ rbenv init # 설치한 rbenv의 initializing을 실행합니다. $ rbenv install 3.3.3 # ruby version 3.3.3을 설치합니다. $ rbenv global 3.3.3 # 설치한 3.3.3버젼을 PC환경내의 global default로 설정합니다. $ ruby -v # 설치된 버젼을 확인합니다. 1개의 ruby버젼만을 설치하여 사용합니다 $ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" $ brew install ruby Jekyll 설치 Jekyll과 Bundler(Gemfile을 통해 package를 관리할 수 있게 해줍니다.)를 설치합니다. $ gem install --user-install bundler jekyll 이제 shell(Command line)환경에서 gem package에서 제공하는 명령어(여기서는 bundler를 사용하기 위함입니다.)를 찾을 수 있도록 연결해줍니다. $ echo 'export PATH=\"$HOME/.gem/ruby/3.3.0/bin:$PATH\"' &gt;&gt; ~/.bash_profile 위에서 ruby를 ‘3.3.3’으로 설치했지만, directory는 ‘3.3.0’으로 생성됩니다. version을 바꾸시려면, 가장 마지막 숫자만 0으로 바꾸신후, 나머지 숫자만 맞춰주시면 됩니다. 이렇게 Jekyll설치가 끝났습니다. 이제 Gem과 Bundler를 통해, 언제든 Jekyll의 version을 업데이트할 수 있게 되었습니다.Blog Theme 설치하기Blog의 design을 바꾸고 싶다면, Theme를 설치해야 합니다.이 Theme또한 RubyGem을 통해 설치할 수 있으며, 업데이트가 빈번히 이루어지는 만큼 이 방법(package manager를 활용한)을 통해 설치하는게 좋습니다. bundle 명령어를 통해, theme를 다운받습니다. 여기선, ‘jekyll-theme-chirpy’를 예시로 사용합니다. $ bundle add jekyll-theme-chirpy # 'jekyll-theme-chirpy'를 설치하고 Gemfile에 기록합니다. theme설치가 완료되었습니다.Theme 업데이트 방법 Gemfile에 정의되어 있는 jekyll-theme-chirpy의 version을 원하는 정책으로 설정합니다. 예를 들면, 아래와 같습니다. # Gemfile에 입력하세요. gem \"jekyll-theme-chirpy\", \"~&gt; 4.0\" bundle명령어를 통해 위에서 수정한 내용을 반영합니다.(패키지를 다시 다운 받습니다) $ bundle update jekyll-theme-chirpy Test를 위한 Local 구동Local에서 website를 구동함으로서 생성되는 페이지를 빠르게 확인할 수 있습니다.$ bundle exec jekyll sReference Jekyll을 사용하여 Github Pages 사이트 설정 https://docs.github.com/ko/pages/setting-up-a-github-pages-site-with-jekyll Jekyll 설치 방법 https://jekyllrb-ko.github.io/docs/ Jekyll에서 Post 작성방법 https://jekyllrb-ko.github.io/docs/posts/ jekyll-theme-chirpy 테마의 Post작성에 필요한 추가 spec https://chirpy.cotes.page/posts/write-a-new-post/" }, { "title": "Multi-region Architecture 구성하기", "url": "/posts/AWS-Multi-Region/", "categories": "AWS", "tags": "aws, Architecture, Region", "date": "2022-08-15 19:55:00 +0900", "snippet": "Multi-region이란 무엇인가?서비스를 운영하는데 필요한 infra요소들을 여러 region에 걸쳐 운영하는것을 말합니다.Multi-region은 여러가지 목적으로 쓰이며, 주로 아래와 같은 상황에서 사용합니다. 서비스가 확장됨에 따라, 더 나은 Latency를 제공하기 위해서 Expansion to a global audien...", "content": "Multi-region이란 무엇인가?서비스를 운영하는데 필요한 infra요소들을 여러 region에 걸쳐 운영하는것을 말합니다.Multi-region은 여러가지 목적으로 쓰이며, 주로 아래와 같은 상황에서 사용합니다. 서비스가 확장됨에 따라, 더 나은 Latency를 제공하기 위해서 Expansion to a global audience as an application grows and its user base becomes more geographically dispersed, there can be a need to reduce latencies for different parts of the world. 장애복구 측면에서의 이점을 위해서(예비 infra를 통해, 장애 시간을 최소화할 수 있습니다) Reducing Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) as part of a multi-Region disaster recovery (DR) plan. 현지 법이나 규제와 관련된 문제를 해결하기 위해서 Local laws and regulations may have strict data residency and privacy requirements that must be followed. Multi-region을 선택하기 전에 고려해야할 사항Multi-region이 꼭 필요한가?대부분의 서비스에선, Cloudfront와 같은 CDN를 통해 Global service로서 운영이 가능합니다. region을 여러가지 운영한다는 것은, region간 보안 및 네트워크 연결문제에 대한 고도화가 필요해진다는 얘기입니다.즉, multi-region을 운영한다는 것은 관리해야한 것들이 늘어난다는 얘기입니다.비용(pricing)서비스 운용비용(pricing) 또한 고려해야할 대상입니다. 여러 region에 걸쳐 서비스를 운영하면, Cloud서비스의 private네트워크가 아닌, public네트워크를 써야하는 경우도 생깁니다.또한, Architecture에 따라 다르지만, 하드웨어(머신)를 추가로 확장해야하는 경우가 많습니다.Multi-region이 주는 이점과 비용 사이의 trade-off를 고려하여 결정합니다.lambda를 사용하는 경우lambda를 통해 Application을 서비스하고 있다면, Multi-region에 대한 비용 부담이 줄어듭니다. lambda 특성상 필요할때만 실행되어 요금이 부과됨에 따라, 하나의 region으로 서비스하던 것과 총 실행 시간의 차이는 나지 않습니다.데이터 동기화 전략data를 region별로 어떻게 동기화(sync) 할지, 혹은 동기화가 꼭 필요한지 고려해야 합니다. 가령, 특정 지역의 규제때문에 region을 분리한다면, data sync는 필요하지 않을수도 있습니다.Multi-region 세팅하기사용할 Multi-region Architecture 선택하기 Pilot light예비용 region을 구축하지만, active상태로 두진 않습니다. 장애가 발생하면, active상태로 들어가기 위한 warm-up이 필요합니다 Warm StandbyPilot light와 유사하지만, 예비용 region을 warm상태로 둡니다. 예비 region으로 traffic을 routing하진 않습니다. Active-Active여러개의 region을 active상태로 두고, 특정 조건에 따라 traffic을 처리합니다.(주로, 요청이 발생한 지리적 위치에 따라 routing합니다)여기선 다음과 같은 이유로, Active-Active를 기준으로 진행합니다. 장애극복(failover)로서의 기능을 기대할 수 있습니다. 예비 region을 놀리는게 아닌, production환경에서 계속 사용함으로서, 서비스 경험을 개선할 수 있습니다.(network latency 감소) lambda로 Application을 운영하기 때문에, 각 region을 active상태로 유지하는 비용부담이 크지 않습니다.서비스할 Region 선택하기선택한 Architecture에 따라 추후 이점을 취할 수 있는 region을 선택합니다.여기선, Active-Active로서 서비스 하기 때문에, 사용자가 많은 region을 선택합니다. 이때, 국가별로 직결되어 있는 해저케이블을 고려하면 좋습니다.Routing 전략 선택하기여러 region들이 (Infra에서) 동일한 level로서 작동한다면, routing을 어떻게 할 것인가?라는 문제에 부딪힙니다.A region과 B region이 있다고 가정합시다.만약, B region으로 가기 위해서, A region으로의 요청이 필요하다면, Multi-region의 의미가 없어집니다. 때문에, 전 세계에 걸쳐 동일하게 사용하는 Domain을 사용해 routing합니다.이 Domain을 활용한 방법은, Domain Resolve과정에서 접속할 region이 결정되도록 합니다. 이때, 다음과 같은 resolve 규칙을 설정할 수 있습니다 Latency 기반 Routing 요청이 발생한 위치에서, latency가 가장 낮은 target으로 resolve합니다(latency는 route53기준의 값입니다) Geolocation 기반 routing 지리적 위치에 따라 routing규칙을 정의합니다. 예를 들어, Asia지역에선 한국서버로 연결되도록 설정할 수 있습니다. Geoproximity routing 대륙을 나누는 개념이 아닌, 지리적 근접정도를 기준으로 routing합니다. 추가로, Cloudfront(CDN)를 이용해, routing하는 방법도 있습니다.Using latency-based routing with Amazon CloudFront for a multi-Region active-active architecture남은 과제Infra가 준비되었다면, CD(Continuous Deployment) 에서의 전략도 바꾸어야 합니다. Application을 배포애야할 target region이 여러개이므로, 다음과 같은 배포전략도 가능합니다. 특정 1개의 region에 배포 장애 확인 이상 없으면, 나머지도 배포 배포 완료이렇게 CD를 수정할 계획입니다Reference Multi-Region Application Architecture Creating a Multi-Region Application with AWS Services Disaster Recovery (DR) Architecture on AWS Using latency-based routing with Amazon CloudFront for a multi-Region active-active architecture 해저케이블 조회 AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications" }, { "title": "Lambda Application optimization on Monorepo", "url": "/posts/Lambda-Application-optimization-on-monorepo/", "categories": "Typescript, Javascript", "tags": "Typescript, Mono-repo, npm, MSA", "date": "2022-08-14 20:25:00 +0900", "snippet": "Monorepo(Monolithic repository)란?하나의 repository(git repository)에서 여러개의 project를 관리하는, repository를 사용하는 전략중에 하나입니다.MSA(MicroService Architecture)에서 각 도메인 단위로 서비스를 분리하면서, 이 코드들을 관리하기위한 전략으로서 많이 쓰입니다....", "content": "Monorepo(Monolithic repository)란?하나의 repository(git repository)에서 여러개의 project를 관리하는, repository를 사용하는 전략중에 하나입니다.MSA(MicroService Architecture)에서 각 도메인 단위로 서비스를 분리하면서, 이 코드들을 관리하기위한 전략으로서 많이 쓰입니다.Bundling이란?Application을 구동하는데 필요한 코드들만 모아, packaging하는것을 말합니다.code를 작성하면서, 실제로 사용할 코드와 그렇지 않은코드를 분리하여 관리하는것은 어렵습니다. 때문에, Code 작성시점이 아닌, 별도의 번들링과정을 통해, 작동에 필요한 코드들만 filtering하여 서비스합니다.Monorepo와 Bundling이 무슨 상관인데?Monorepo를 운영하다보면, 각 service에서 공통으로 사용되는 코드들이 생깁니다(ex: DB커넥션 생성코드). 때문에, 이런 코드들을 관리하기 위한 공통 모듈공간을 만들게 되는데,이때 공통모듈(이하 common모듈)을 참조하는 방식을 Bundling과정에서 최적화(Optimization)할 수 있습니다.이게 왜 필요한데?lambda를 이용하여 서비스를 운영한다면, lambda의 cold start 문제에 부딪히게 됩니다. 이때 app initialization시간을 최소화하기 위해 이런 최적화가 필요합니다.lambda에 initialize 시간에 영향을 미치는 요소는 다음과 같습니다.(Lambda performance optimization)The size of the function package, in terms of imported libraries and dependencies, and Lambda layers.The amount of code and initialization work.The performance of libraries and other services in setting up connections and other resources.HTTP 서버 운영을 lambda + API Gateway를 통해 한다면, lambda는 각 end-point별로 생성될 것입니다. 이때 각 lambda 구동에 필요한 코드들만 bundling하여 성능최적화를 달성합니다.공통(Common)모듈 운영하기 common모듈을 운영하는 가장 쉬운 방법은, 각 서비스에서 외부모듈로서 참조하는 방식입니다. 예를들어, npm을 사용하는 경우, local path로서 npm install을 사용하는 것입니다.이 방법은 운영할때, 외부 모듈로서 다루면 되기 때문에 적용하기 쉽지만, ‘외부 모듈’이기 때문에, bundling시에 optimization 대상이 되지 않는 단점이 있습니다. $ npm install ../common code관리는 별도의 path를 분리해서 관리하지만, 참조할때는 각 서비스 내부에서 생성한 파일처럼 사용합니다.이는 Code를 관리하는 측면과, 사용하는 측면을 분리하여 구성하는 방법입니다.공통된 code를 한곳에서 관리하기 위한 관리의 목적과, 코드를 참조할때는 service수준의 code로서 취급하여 optimization 대상이 되도록 하는 최적화를 각각 별도로 달성하는 방법입니다.이 포스트에서는 2번 방법을 다룹니다.본격적으로 최적화(Optimization) 하기typescript의 path aliastypescript는 특정 path를 alias로 지정할 수 있습니다.(tsconfig paths)예를 들어, 다음과 같이 tsconfig를 설정하여, path alias를 생성할 수 있습니다{ \"compilerOptions\": { \"baseUrl\": \".\", \"paths\": { \"@common\": \"../common\" } }}webpack bundlingcommon 모듈을 webpack에서 번들링 및 최적화할 수 있도록 설정합니다.nodejs를 사용하는 경우, 외부 모듈을 따로 번들링하지 않습니다. 이는, webpack-node-externals이라는 모듈을 이용해 세팅합니다.그러나, common모듈은 typescript의 path alias 기능을 사용한 것이기 때문에, 별도의 bundling과정을 거치도록, webpack-node-externals설정에서 allowlist에 포함시켜 줍니다.이렇게 설정하면, @common/~경로로 import한 모든 구문이 bundling대상이 되며, 구동에 필요한 모든 모듈을 가져와 packaging합니다.// webpack.config.tsimport nodeExternals from 'webpack-node-externals';import TsconfigPathsPlugin from 'tsconfig-paths-webpack-plugin';export default {\t..., module: { rules: [ { test: /\\.(tsx?)$/, use: [ { loader: 'ts-loader', options: { transpileOnly: true, experimentalWatchApi: true, }, }, ], exclude: [ path.resolve(contextDir, 'node_modules'), path.resolve(contextDir, '.serverless'), path.resolve(contextDir, '.webpack'), path.resolve(contextDir, 'coverage'), ], }, ], }, resolve: { extensions: ['.js', '.ts', '.mjs', '.json'], symlinks: false, cacheWithContext: false, plugins: [ new TsconfigPathsPlugin({ configFile: './tsconfig.json', extensions: ['.ts'], }), ], enforceExtension: false, }, optimization: { concatenateModules: false, minimize: true, minimizer: [new TerserPlugin()], }, externalsPresets: { node: true, }, externals: [ nodeExternals({ modulesDir: nodeModulePath, allowlist: [/^@common/, /^lodash/], }), ], ...,}결과이제 다음과 같이 import한 내용들은 모두 bundling및 최적화 과정을 거칩니다.import { test, addFunction } from '@common/util'; 이를 통해, lambda에 올릴 package를 구동에 필요한것만 갖추어 용량을 낮출 수 있었으며, 개발과정에서 따로 신경쓰지 않아도, 관련없는 initialize과정을 자동으로 filtering할 수 있게 되었습니다.Reference Monorepo Wiki tsconfig paths Lambda performance optimization" }, { "title": "Multi Environments 서버 환경에서, client에서 접속 환경 결정하기(VPN을 이용)", "url": "/posts/On-Multi-Environments,-how-to-set-target-environment-on-client/", "categories": "AWS", "tags": "aws, Architecture, VPN", "date": "2022-02-13 21:55:00 +0900", "snippet": "들어가면서Multi-Environments를 제공하는 Architecture는 회사의 모든 비지니스가 안전하고 효율적으로 작동할 수 있도록 도와줍니다.특히, 스타트업(빠른 실험과 적용을 통해 성장하는)에서 회사 구성원의 창의적인 생각을 안전하게 실험할 수 있는 환경을 제공함으로서, 장기적으로 회사의 성장에 기여합니다.이런 환경의 이점을 최대화 하기 위...", "content": "들어가면서Multi-Environments를 제공하는 Architecture는 회사의 모든 비지니스가 안전하고 효율적으로 작동할 수 있도록 도와줍니다.특히, 스타트업(빠른 실험과 적용을 통해 성장하는)에서 회사 구성원의 창의적인 생각을 안전하게 실험할 수 있는 환경을 제공함으로서, 장기적으로 회사의 성장에 기여합니다.이런 환경의 이점을 최대화 하기 위해선, 각 구성원이 자신의 의지에 따라 쉽게 서비스의 작동 환경을 바꿀 수 있어야 합니다.Multi-environments환경에서 Client가 접속할 환경을 선택할 수 있으면 좋은 이유 Client 개발자와 다른 구성원간의 업무 종속성(dependency)을 줄일 수 있습니다. 서비스 배포는 여러사람의 손을 거치며, 하나의 pipeline을 생성하게 됩니다. 이 과정에서, 각 구성원들이 서로의 업무에 대한 종속성(dependency)를 최소화 하는것이 중요한데, QA 혹은 서비스의 컨텐츠를 관리하는 구성원은 Client의 테스트용 빌드에 종속되어 있는 경우가 있습니다. 서버의 소스코드 변경에 대해, Legacy 어플에 대한 테스트를 손쉽게 진행할 수 있습니다. 종종 서버의 소스코드 변경에 Legacy유저가 크게 영향을 받는 경우가 있습니다. 하지만, 개발자는 모든 상황을 예측할순 없으며, ‘현재 개발중인 서버의 소스코드 + Production Client(유저가 사용중인)’의 조합으로 테스트를 진행함으로서 이런 Risk를 최소화할 수 있습니다. Client에서 VPN을 통해 접속 환경 선택하기VPN을 이용하는 이유는, 필요할때만, Client의 DNS Resolve를 변경하기 위함입니다.Client에서 접속할 서버를 선택하도록 구성하는 방법은 여러가지가 있겠지만, DNS Resolve과정에서 접속할 서버를 변경한다면, 네트워크에 대해 가장 효율적인 방법일 것입니다. 도메인 Resolve는 HTTP 요청과정에서 필수로 진행됩니다(ip로 요청보내진 않을테니). 때문에, proxy서버를 추가하여 네트워크에 대한 depth를 추가하는것 보다, domain Resolve과정에서 dev서버로 resolve되도록 하는것이 가장 효율적이라고 생각합니다.이를 구현하기 위해 해결해야할 문제는 다음과 같습니다. Private DNS를 통한 domain resolve. HTTPS 인증문제ArchitectureDomain을 다음과 같이 가정합니다.public domain // record type // custom domain에서의 api-gateway domain // api-gatewayapi.abc.com -&gt; cname -&gt; gateway-id-1.apigateway.com -&gt; Production API 서버api.dev.abc.com -&gt; cname -&gt; gateway-id-2.apigateway.com -&gt; Dev API 서버 VPC(이하 Dev VPC)생성하기 VPC에 DNS hostnames와 DNS resolution이 ‘Enabled’되어 있어야 합니다. dev서버가 있는 region에서, api.abc.com에 대한 Custom domain을 생성합니다.(생성된 도메인을 gateway-id-3.apigateway.com으로 가정합니다) Dev서버와 연결되지만, SSL인증은 api.abc.com기준으로 해야하기 때문에, 별도의 Custom domain을 통해 api.abc.com의 SSL인증서를 활용합니다 Route53에서, VPC에 대한 Private Hosted zone생성 Private Hosted Zone에 아래와 같이 Record를 추가합니다. // 'api.abc.com'도메인으로 custom domain을 추가로 생성합니다. 이 도메인은 dev api서버와 연결됩니다 private domain // record type // custom domain에서의 api-gateway domain // api-gateway api.abc.com -&gt; cname -&gt; gateway-id-3.apigateway.com (Dev API 서버) Dev VPC에 Client VPN을 생성합니다. Client VPN의 DNS설정 화면에서, DNS설정을 VPC의 DNS주소와 Public DNS주소를 사용합니다. Enable DNS Servers는 체크(true)합니다. DNS Server 1에 VPC의 DNS를 할당합니다. VPC의 DNS주소는 VPC의 IPv4네트워크 범위 + 2 입니다 DNS Server 2에 Public DNS주소를 입력합니다. Enable split-tunnel에 체크(true)합니다. 이 설정은 VPN에 접속한 client의 traffic이 필요할때만 VPN을 통하도록 합니다.(Route설정을 추가로 신경써줘야함). 설정하지 않으면, 모든 traffic이 VPN을 통해 전송됩니다. 추가로 Client VPN에 대한 설정을 마무리해줍니다(Authorize탭과 Route관련 설정) 인프라 설정은 완료되었습니다. Client VPN에 접속하여 테스트합니다.추가로 해결해야할 과제 API-Gateway Authorization문제. 위의 과정을 완료하면, Client VPN을 접속했을때는, api.abc.com에 대한 요청이 실제로는 dev서버로 전달됩니다. 하지만, dev와 production의 차이는 도메인의 차이뿐만 아니라, Authorization에 대한 차이도 있을것입니다. AWS SSO(Single Sign-On)를 통해 AWS와 G-Suite(Google)를 연결하고, Google계정으로 VPN연동하기VPN을 사용하기 위해선, Client인증서(Certificate)가 필요합니다. 그러나, 아래와 같은 이유로 Google계정과 연동하여 사용합니다. 어떤 구성원은, 인증서를 사용한 접속 방법에 어려움을 겪을 수 있습니다. 이런 과정을 단순화하기 위해, Google계정을 사용합니다.Reference How does DNS work with my AWS Client VPN endpoint? How do I resolve resource records in my private hosted zone using Client VPN How to use G Suite as an external identity provider for AWS SSO Gsuite와 AWS통합 Client VPN에 SAML인증 적용" }, { "title": "Running Puppeteer on AWS Lambda", "url": "/posts/Running-Puppeteer-on-AWS-Lambda/", "categories": "AWS, Lambda", "tags": "aws, lambda, Puppeteer", "date": "2022-02-05 20:55:00 +0900", "snippet": "What is Puppeteer? Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol. Puppeteer runs headless by default, but can be configured t...", "content": "What is Puppeteer? Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol. Puppeteer runs headless by default, but can be configured to run full (non-headless) Chrome or Chromium.What is AWS Lambda? AWS Lambda is serverless computing service.Use Puppeteer on AWS LambdaAWS Lambda has limitation about source code size.(Headless-Chrome installed with Puppeteer &gt; 280MB )So, Lambda run with Docker Image based on ‘amazon/aws-lambda-nodejs:14’Repositoryhttps://github.com/KanghoonYi/lambda-puppeteerReference Puppeteer Repository Chrome headless doesn’t launch on UNIX AWS Blog Field Notes: Scaling Browser Automation with Puppeteer on AWS Lambda with Container Image Support AWS Documentation Creating Lambda container images " }, { "title": "AWS Solution Architect-Associate 취득 후기", "url": "/posts/AWS-Certification-Solutions-Architect-Associate-%ED%9B%84%EA%B8%B0/", "categories": "AWS, Certification", "tags": "aws, Certification, Associate", "date": "2022-02-01 20:55:00 +0900", "snippet": "AWS Certification란?AWS에서 제공하는 Cloud활용 능력을 측정할 수 있는 자격증입니다응시 이유어떤 Tool이든, 그것을 만든 사람들의 관점을 이해한다면, 그 Tool을 100%, 아니면 그 이상으로 활용할 수 있다고 생각합니다.하지만, AWS에 대해서는 지금까지 업무 수행에 필요한 내용들만 찾지 않았나 싶습니다. 이번 기회에, AWS...", "content": "AWS Certification란?AWS에서 제공하는 Cloud활용 능력을 측정할 수 있는 자격증입니다응시 이유어떤 Tool이든, 그것을 만든 사람들의 관점을 이해한다면, 그 Tool을 100%, 아니면 그 이상으로 활용할 수 있다고 생각합니다.하지만, AWS에 대해서는 지금까지 업무 수행에 필요한 내용들만 찾지 않았나 싶습니다. 이번 기회에, AWS에서 어떤 관점으로 Cloud를 운영하고자 하는지 정리하고, 시야를 넓혀서, 앞으로의 Cloud활용 능력을 키우고자 하였습니다.처음부터, 최종 목적이었던, DevOps Engineer-Professional에 지원하고자 하였으나, 모의 시험 성적이 아슬아슬하여, 일단, Associate를 취득하고, pro자격증으로 넘어가야겠다고 생각했습니다.시험에 관한 정보 처음에는, 뭐부터 시작해야하는지 애매했습니다. 그래서, ‘시험가이드 확인(시험 방법과 범위에 대한 설명이 있음) -&gt; AWS Skill Builder 에서 시험에 관한 강의 수강’의 과정을 거쳤으며, 해당 강의 내용중 잘 모르고 있던 부분에 대해서 확인하였습니다.이후, 무료로 제공되는 연습문제set 을 통해 모의 시험을 보고, 그 결과에 따라 추가로 study를 진행하였습니다. Solutions Architect-Associate의 경우 특정 요구사항에 따라는 최적의 Cloud구조에 대해, 보기에서 고르는 질문이 많고, Big Data처리에 대한 질문도 많았습니다. PSI를 통해 응시했는데, 응시할때에, Secure Browser를 별도로 설치하는 과정이 필요합니다. 해당 프로그램은 응시 과정을 통해 제공되는 link로만 설치할 수 있습니다. Mac OS 12.0.1(monterey) 환경에서, Secure Browser를 구동시 무결성 체크에 문제가 있습니다. Mac의 모든 프로그램을 종료했는데도, ‘Remote Agent’가 실행중이라며 무결성체크에서 막힙니다. OS자체 프로세스인데 잡아내는것으로 보이며, 터미널에서 관련 process들을 kill하고 잽싸게 다시 check하는 방식으로 넘어갔습니다. 약속된 시간 +30분까지만 응시버튼을 누르면, 기술적인 문제가 있는 경우(시험화면에 애초에 진입하지 못하면), 시험시간이 보존됩니다 PSI로 원격 응시하는 경우, 온라인감독 시험 응시 방법 영상을 통해 어떤 과정이 있는지 미리 확인해보니, 시험 전에 당황하지 않고, 시험에만 집중할 수 있었습니다.Certification에 관한 앞으로의 계획최종목적지를 AWS DevOps - Professional로 설정했던 만큼, 다음달에 해당 시험에 응시할 계획입니다기타 Tip 합격하면, 해당 Certification은 2년동안 유효합니다. 다만, 이후에 더 높은 등급의 시험에 응시하여 합격하면, 이전에 취득했던 자격 또한 갱신됩니다. 자격을 취득하면, 다음 시험에 대한 50%로 쿠폰이 1개 주어지며, 유료 모의 시험에 대한 쿠폰도 1개 주어집니다.기타 혜택확인Reference AWS Certification AWS Certified Solutions Architect - Associate 응시에 필요한 자료" }, { "title": "Setup Datadog Agent with nvml", "url": "/posts/datadog-agent-nvml/", "categories": "Datadog", "tags": "Datadog, NVIDIA, NVML", "date": "2020-12-30 20:55:00 +0900", "snippet": "datadog-agent-nvmlfor using nvidia graphic metric on datadogthis project is working with docker.What is NVML?Monitoring NVIDIA GPUs status using Datadog.see https://github.com/ngi644/datadog_nvmlse...", "content": "datadog-agent-nvmlfor using nvidia graphic metric on datadogthis project is working with docker.What is NVML?Monitoring NVIDIA GPUs status using Datadog.see https://github.com/ngi644/datadog_nvmlsee https://developer.nvidia.com/nvidia-management-library-nvmlgo to repositoryhttps://github.com/KanghoonYi/datadog-agent-nvml" } ]
