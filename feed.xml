<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blog.devpour.net/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.devpour.net/" rel="alternate" type="text/html" hreflang="ko" /><updated>2026-02-28T18:55:39+09:00</updated><id>https://blog.devpour.net/feed.xml</id><title type="html">KanghoonYi(Pour)</title><subtitle>Software Engineer</subtitle><entry><title type="html">kubernetes 환경에서 GPU 서빙 | GPU인식</title><link href="https://blog.devpour.net/posts/k8s-gpu-1/" rel="alternate" type="text/html" title="kubernetes 환경에서 GPU 서빙 | GPU인식" /><published>2026-02-28T18:26:00+09:00</published><updated>2026-02-28T18:55:20+09:00</updated><id>https://blog.devpour.net/posts/k8s%20gpu-1</id><content type="html" xml:base="https://blog.devpour.net/posts/k8s-gpu-1/"><![CDATA[<h2 id="들어가면서">들어가면서</h2>

<p>AI시대에 회사에서 GPU를 어떻게 서빙할지에 대한 고민이 있습니다.<br />
이를 위해, GPU가 어떻게 Kubernetes와 결합해서 서빙 되는지 이해하고자, 조사하게 되었습니다.</p>

<blockquote class="prompt-info">
  <p>여러 Inference Accelerator(AWS Inferentia, Google TPU등)들이 있지만, 여기서는 회사에서 사용하고 있는, Nvidia GPU를 기준으로 알아보겠습니다.</p>
</blockquote>

<h2 id="kubernetes에서-gpu-인식시키기">Kubernetes에서 GPU 인식시키기</h2>

<p>GPU는 개별 Node의 자원이기 때문에, 크게 아래의 2개 단계로 나누어 해결하게 됩니다.</p>

<ol>
  <li>GPU를 갖고 있는 Node에 Pod를 할당합니다.</li>
  <li>Node안에서 GPU 하드웨어를 Pod에 연결해줍니다.</li>
</ol>

<h3 id="kubernetes의-device-plugin-framework">Kubernetes의 Device Plugin framework</h3>

<p>Kubernetes에서의 GPU인식은, ‘Device Plugin’이라는 기능을 Base로 작동합니다.</p>

<h4 id="device-plugin">Device Plugin?</h4>

<p>Kubernetes는 Hardware(이하 HW)의 추상화(Abstraction)을 제공합니다. 이 과정에서 수 많은 하드웨어를 지원하는 Interface가 ‘Device plugin’입니다.<br />
‘Device Plugin’을 통해, Hardware Vendor(하드웨어 공급자, Nvidia같은)는 Kubernetes내부 Code를 변경하지 않고, Node의 kubelet에 HW를 인식시킬 수 있습니다(kubelet이 인식한 HW를 Cluster에 Advertise합니다).</p>

<blockquote class="prompt-info">
  <p>단, Vendor사는 Resource name을 <code class="language-plaintext highlighter-rouge">vendor-domain/resourcetype</code> 와 같은 규칙으로 정의해야 합니다.<br />
대표적인 예시로, <code class="language-plaintext highlighter-rouge">nvidia.com/gpu</code> 가 있습니다.</p>
</blockquote>

<h4 id="device-plugin-workflow">Device Plugin Workflow</h4>

<p>Device Plugin은 크게 등록(Registration), 모니터링(ListAndWatch), 할당(Allocate)의 3단계로 작동합니다.</p>

<p><img src="/assets/img/for-post/k8s%20gpu-1/image.png" alt="Device Plugin 작동 Flow" />
<em>Device Plugin 작동 Flow | from <a href="https://github.com/kubernetes/design-proposals-archive/blob/main/resource-management/device-plugin.md#tldr">github.com/kubernetes/design-proposals-archive</a></em></p>

<p>이 Workflow를 위해, ‘Device Plugin’은 다음과 같은 Function이 구현되어야 합니다.</p>

<ul>
  <li>
    <p>GetDevicePluginOptions<br />
초기 연결 시 호출됩니다. 특정 옵션(예: <code class="language-plaintext highlighter-rouge">PreStartContainer</code> 실행 여부)을 지원하는지 확인합니다.</p>
  </li>
  <li>
    <p>ListAndWatch (필수)<br />
<strong>핵심 중의 핵심입니다.</strong><br />
gRPC 스트림을 열어둡니다. Plugin은 이 연결을 통해 장치 목록(ID)과 상태(Healthy/Unhealthy)를 실시간으로 Kubelet에 쏩니다. 장치에 장애가 발생하면 이 함수를 통해 즉시 보고됩니다.</p>
  </li>
  <li>
    <p>Allocate (필수)<br />
Pod가 GPU를 사용하려고 할 때 호출됩니다. Kubelet은 “GPU ID 0번을 쓰려고 하니 필요한 정보를 줘”라고 요청하고, Plugin은 해당 GPU를 사용하기 위한 <strong>환경 변수, 볼륨 마운트 경로, 장치 노드(/dev/nvidia0 등)</strong> 정보를 응답합니다.</p>
  </li>
  <li>
    <p>PreStartContainer (선택)<br />
컨테이너가 시작되기 직전에 실행됩니다. 장치를 초기화하거나 특정 설정을 초기화해야 할 때 사용합니다.</p>
  </li>
  <li>
    <p>GetPreferredAllocation (선택)<br />
여러 개의 장치 중 어떤 것을 우선적으로 할당할지 결정할 때 사용합니다 (Topology-aware 할당 등).</p>
  </li>
</ul>

<h3 id="kubelet-device-plugin-hardware로-이어지는-통신">Kubelet, Device Plugin, Hardware로 이어지는 통신</h3>

<h4 id="kubelet과-device-plugin의-통신">‘kubelet과 ‘Device Plugin’의 통신</h4>

<blockquote>
  <p>Device Plugins are expected to communicate with Kubelet through gRPC on an Unix socket.<br />
When starting the gRPC server, they are expected to create a unix socket at the following host path: /var/lib/kubelet/device-plugins/.<br />
- from: <a href="https://github.com/kubernetes/design-proposals-archive/blob/main/resource-management/device-plugin.md">kubernetes/design-proposal-archive</a></p>
</blockquote>

<p>‘Device Plugin’과 ‘kubelet’은 gRPC(Unix Socket기반)로 통신하게 됩니다.<br />
때문에, Host OS에서 <code class="language-plaintext highlighter-rouge">/var/lib/kubelet/device-plugins/</code> 경로로 가면, 이 통신에 사용하는 Socket을 확인할 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>root@<span class="k">***</span>:/var/lib/kubelet/device-plugins# <span class="nb">ls</span> <span class="nt">-al</span>
total 12
drwxr-xr-x  2 root root 4096 Feb 27 06:51 <span class="nb">.</span>
drw-r--r-- 10 root root 4096 Feb 28 00:06 ..
srwxr-xr-x  1 root root    0 Feb 27 06:51 kubelet.sock &lt;<span class="nt">--</span> kubelet이 device plugin용으로 열어둔 Socket입니다.
..
srwxr-xr-x  1 root root    0 Feb 27 06:51 nvidia-mig-3g.20gb.sock &lt;<span class="nt">--</span> 등록을 마친 GPU가 kubelet이 접속할 수 있게 열어둔 Socket입니다
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="device-plugin과-hardware의-통신">‘Device Plugin’과 Hardware의 통신</h4>

<p>‘Device Plugin’은 OS의 HW 드라이버와 연결됩니다.<br />
이때, Pod의 Container는 Driver를 포함하지 않습니다. Host(여기선 Node를 의미)의 Driver파일 Container에 Mount시켜서 사용합니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nvidia-device-plugin-daemonset-88s6m</span>
  <span class="s">...</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="s">...</span>
    <span class="s">- name</span><span class="err">:</span> <span class="s">driver-install-dir</span>
      <span class="s">hostPath</span><span class="err">:</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/run/nvidia/driver</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">DirectoryOrCreate</span>
<span class="nn">...</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<blockquote class="prompt-info">
  <p>Q: Host의 드라이버를 사용하면, Host의 OS와 Container의 OS가 일치해야 하는것 아닌가?<br />
A: Container 자체가 Host와 Kernel를 공유하기 때문에, 이미 Kernel을 일치합니다. OS는 달라도 됩니다.</p>
</blockquote>

<h2 id="nvidia-device-plugin">Nvidia Device Plugin</h2>
<p>‘Nvidia Device Plugin’은 위의 Device Plugin Framework를 이용해, Nvidia GPU Resource를 Kubernetes에 조달해줍니다.</p>

<h3 id="nvidia-device-plugin의-작동-flow">Nvidia Device Plugin의 작동 Flow</h3>

<p>‘Nvidia Device Plugin’은 일반적인 Device Plugin과 달리, ‘NVIDIA Container Stack(과거 Nvidia Container Runtime)’와의 상호작용이 핵심입니다.</p>

<blockquote class="prompt-info">
  <p>‘Nvidia Container Stack’은 Container Runtime(kubelet이 Container 생성을 지시하는 대상) 수준에서, 별도의 GPU를 사용할 수 있는 Container Runtime을 만들어냅니다.<br />
GPU를 사용하기 위해선, 이 Runtime을 사용해야 합니다.</p>
</blockquote>

<ol>
  <li>
    <p>탐색(Discovery)</p>

    <p><img src="/assets/img/for-post/k8s%20gpu-1/image%201.png" alt="Nvidia Device Plugin의 Discovery 과정." />
<em>Nvidia Device Plugin의 Discovery 과정. | from <a href="https://www.alibabacloud.com/blog/getting-started-with-kubernetes-%7C-gpu-management-and-device-plugin-implementation_596306">alibabacloud.com/blog</a></em></p>

    <ul>
      <li>Plugin이 실행되면 호스트의 NVML(NVIDIA Management Library)을 호출합니다.<br />
이를 통해, Node에 장착되어 있는 GPU 모델, 갯수, 드라이버 상태를 확인하게 됩니다.</li>
    </ul>
  </li>
  <li>할당(Allocate) &amp; 환경 변수 주입
    <ul>
      <li>사용자가 <code class="language-plaintext highlighter-rouge">resources: limits: nvidia.com/gpu: 1</code> 를 요청하면, GPU의 UUID(Host에서 관리하는 Device ID)를 환경변수(<code class="language-plaintext highlighter-rouge">NVIDIA_VISIBLE_DEVICES=UUID</code>)에 주입합니다.<br />
이렇게 각 Pod로 GPU가 분산, 할당됩니다.</li>
    </ul>
  </li>
  <li>런타임 가로채기 (The Hook)
    <ul>
      <li>Kubelet이 컨테이너를 생성할 때, ‘NVIDIA Container Runtime’이 이 과정을 가로챕니다.<br />
런타임(NVIDIA Container Runtime)은 ‘2’번 과정에 주입된 환경변수(<code class="language-plaintext highlighter-rouge">NVIDIA_VISIBLE_DEVICES</code>)를 확인하고,<br />
Container내부에 <code class="language-plaintext highlighter-rouge">/dev/nvidia0</code> 와 같이 Device와 구동에 필요한 라이브러리(<code class="language-plaintext highlighter-rouge">.so</code> 파일들)를 직접 Mount해 줍니다.</li>
    </ul>
  </li>
</ol>

<h2 id="cdicontainer-device-interface">CDI(Container Device Interface)</h2>

<p>최근 NVIDIA는 기존의 복잡한 환경 변수/볼륨 마운트 방식 대신, 더 표준화된 방식인 CDI로 전환하고 있습니다.</p>

<dl>
  <dt>CDI는</dt>
  <dd>위의 ‘Container Runtime’을 별도로 생성하는 방식이 아닌, <strong>표준화된 방식</strong>으로, GPU와 같은 외부 Device를 Container에 연결하기 위한 <strong>표준</strong>입니다.</dd>
  <dd>HW를 Container에 연결하는 과정을 단순하게 만들어줍니다.</dd>
</dl>

<h2 id="references">References</h2>

<dl>
  <dt>Kubernetes Device Plugin | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device Plugins</a></dd>
  <dt>Kubernetes Device Plugin Proposal | github.com/kubernetes/design-proposals-archive</dt>
  <dd>‘Device Plugin’의 세부 설계가 가장 잘 나와있는 문서입니다.</dd>
  <dd><a href="https://github.com/kubernetes/design-proposals-archive/blob/main/resource-management/device-plugin.md">https://github.com/kubernetes/design-proposals-archive/blob/main/resource-management/device-plugin.md</a></dd>
  <dt>Nvidia device plugin</dt>
  <dd><a href="https://github.com/NVIDIA/k8s-device-plugin">https://github.com/NVIDIA/k8s-device-plugin</a></dd>
  <dt>Container Device Interface | docs.nvidia.com</dt>
  <dd><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/cdi.html">Container Device Interface (CDI) Support in the GPU Operator — NVIDIA GPU Operator</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="DevOps" /><category term="kubernetes" /><category term="kubernetes" /><category term="cncf" /><category term="k8s" /><category term="nvidia" /><category term="gpu" /><summary type="html"><![CDATA[들어가면서]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/k8s%20gpu-1/k8s-gpu-cover.png" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/k8s%20gpu-1/k8s-gpu-cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Redis | Key Technologies - System Design Interview</title><link href="https://blog.devpour.net/posts/Redis/" rel="alternate" type="text/html" title="Redis | Key Technologies - System Design Interview" /><published>2025-08-04T18:57:00+09:00</published><updated>2025-08-04T22:12:32+09:00</updated><id>https://blog.devpour.net/posts/Redis</id><content type="html" xml:base="https://blog.devpour.net/posts/Redis/"><![CDATA[<h2 id="redis-소개">Redis 소개</h2>

<blockquote>
  <p>Redis is the world’s fastest in-memory database.
- from <a href="https://redis.io/about/">redis.io</a></p>
</blockquote>

<dl>
  <dt>Redis는</dt>
  <dd>C로 작성된, 오픈소스 in-memory key-value 저장소 입니다.(데이터를 Disk가 아닌 RAM에 저장하여 사용합니다.)</dd>
  <dd>빠른 속도와 다양한 자료구조(Data structure)를 지원하는, NoSQL DB입니다.</dd>
  <dd>String, List, Hash, Set, Sorted Set, 비트맵, HyperLogLog 등 다양한 자료구조(Data Structure)를 지원합니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>NoSQL은 기존에 많이 쓰이던, 관계형(SQL) DB가 아닌것을 의미합니다.<br />
관계형 DB와 다르게, 더 유연한 데이터 모델과 확장성을 제공합니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>만약 데이터 저장 과정에서, Durability(영속성)이 더 중요하다면, Redis는 적합하지 않습니다.<br />
Redis가 AOF(Append-Only File)을 통해, Data의 Persistent를 지원하고 있지만, 이는 RDB의 것만큼 보장(guarantee)해주지 못해기 때문입니다.<br />
단, AWS의 Memory DB와 같은 대안도 가능합니다.</p>
</blockquote>

<h3 id="redis는-key-value-store입니다">Redis는 Key-Value Store입니다.</h3>

<p><img src="/assets/img/for-post/Redis/redis-1.png" alt="redis-1.png" /></p>

<p>Redis는 Key-Value 저장소 입니다.<br />
Key는 반드시 ‘String’이어야 하며, Value는 Redis가 지원하는 데이터 구조(binary data and strings, sets, lists, hashes, sorted sets 등)는 모두 가능합니다.<br />
또한, Redis안에서의 모든 object들은 Key를 갖고 있어야 합니다.</p>

<h3 id="redis의-value-type">Redis의 Value Type</h3>

<p><img src="/assets/img/for-post/Redis/image.png" alt="Redis의 Value Type 및 구현 Algorithm" />
<em>Redis의 Value Type 및 구현 Algorithm | from <a href="https://blog.bytebytego.com/p/a-crash-course-in-redis">blog.bytebytego.com</a></em></p>

<p><br /></p>

<h2 id="redis의-single-thread와-성능">Redis의 Single Thread와 성능</h2>

<p>Redis는 <strong>단일 스레드(Single Thread)</strong> 구조임에도 불구하고, <strong>수십만 RPS(requests per second)</strong> 를 처리할 수 있을 정도로 매우 빠릅니다.<br />
여기서는, 이것이 가능한 이유를 살펴보고자 합니다.</p>

<blockquote class="prompt-info">
  <p>Redis 6부터는 I/O(read/write) 에 일부 멀티스레드가 도입되었습니다.<br />
클라이언트 요청을 읽어오거나 응답을 보내는 작업은 멀티스레드 가능하지만, <strong>명령(Command)실행 자체는 여전히 Single Thread</strong>로 작동합니다.</p>
</blockquote>

<h3 id="ram-기반-처리">RAM 기반 처리</h3>

<p>모든 데이터는 디스크가 아닌, RAM에 저장되어 있기 때문에, I/O 병목이 거의 없습니다.<br />
즉, 디스크 → 메모리로 데이터를 불러오는 비용이 들지않고, 즉시 데이터를 이용한 연산이 가능합니다.</p>

<h3 id="lock-경합race-condition-없음">Lock 경합(Race Condition) 없음</h3>

<p><img src="/assets/img/for-post/Redis/image%201.png" alt="Lock Race Condition을 표현하는 이미지" class="w-50" />
<em>Lock Race Condition을 표현하는 이미지</em></p>

<p>Redis는 Single Thread를 사용해서, 여러 Thread간의 동시성 문제를 위해 Lock을 사용할 필요가 없습니다.(애초에 Single Thread이기 때문에)</p>

<p><img src="/assets/img/for-post/Redis/image%202.png" alt="CPU의 Context Switching을 비유적으로 표현하는 이미지" class="w-50" />
<em>CPU의 Context Switching을 비유적으로 표현하는 이미지</em></p>

<p>이는 ‘Context Switching(CPU Level에서의 작업 전환)’ 비용과 ‘Lock Condition Race’(Lock을 얻기 위한 경합)을 완전히 제거해줍니다.<br />
이를 통해, 훨씬 예측 가능하고 빠른 처리 시간을 제공합니다.</p>

<h3 id="epoll-기반-비동기-이벤트-처리">Epoll 기반 비동기 이벤트 처리</h3>

<p>Redis는 <strong>리눅스 epoll을 사용</strong>해 수천 개의 클라이언트 연결을 하나의 이벤트 루프에서 효율적으로 처리합니다.</p>

<p><img src="/assets/img/for-post/Redis/image%203.png" alt="Linux Epoll의 역할을 보여주는 이미지" /></p>

<p><em>Linux Epoll의 역할을 보여주는 이미지 | <a href="https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll/">devarea.com</a></em></p>

<dl>
  <dt>‘epoll’은</dt>
  <dd>수많은 파일 디스크립터(FD)(예: 클라이언트 소켓)의 입출력 가능 상태(I/O readiness) 를 효율적으로 감시합니다.</dd>
  <dd>이벤트가 발생한 것만 알려주는 고성능 커널 기능입니다.</dd>
</dl>

<p>이벤트가 없는 Connection은 무시하고, 이벤트가 발생한 소켓만 처리하여, 매우 빠른 성능을 보여줍니다.</p>

<blockquote class="prompt-info">
  <p>‘Epoll’은 시스템 콜로 호출되는 커널 기능입니다. 이 또한 Redis의 Single Thread에서 작동합니다</p>
</blockquote>

<h3 id="command가-단순하고-작다">‘Command’가 단순하고 작다.</h3>

<p>Redis에서 사용하는 명령어는, 대부분 매우 가볍습니다.(GET, SET, INCR 등은 O(1) 또는 O(log N) 수준의 처리 시간.)<br />
Redis 내부 데이터 구조가 최적화되어 있어 탐색, 삽입, 정렬 등에 강력한 성능을 보여줍니다.</p>

<h3 id="c언어-기반">C언어 기반</h3>

<p>Redis는 C언어로 개발되어 있어, 운영체제와 매우 가까운, 저수준에서 동작합니다.<br />
즉, 별도의 VM없이 실행되어 메모리와 CPU사용을 최적화 하였습니다.</p>

<p><br /></p>

<h2 id="redis의-infra-구성-방법들">Redis의 Infra 구성 방법들</h2>

<p><img src="/assets/img/for-post/Redis/image%204.png" alt="Redis의 Infra Architecture의 변화" />
<em>Redis의 Infra Architecture의 변화 | <a href="https://bytebytego.com/guides/how-redis-architecture-evolve/">bytebytego.com</a></em></p>

<p>Redis는 기본적으로 Single Node로 동작할 수 있지만, HA(High Availability)를 위해 Replica나 Cluster형태로도 구성 가능합니다.</p>

<h3 id="redis-cluster">Redis Cluster</h3>

<p><img src="/assets/img/for-post/Redis/redis-2.png" alt="Redis Cluster와 Key분배" />
<em>Redis Cluster와 Key분배</em></p>

<h4 id="gossip-protocol">Gossip Protocol</h4>

<p><img src="/assets/img/for-post/Redis/image%205.png" alt="Redis Cluster와 gossip protocol" />
<em>Redis Cluster와 gossip protocol</em></p>

<dl>
  <dt>‘gossip protocol’은</dt>
  <dd>Redis Cluster 환경에서 <strong>노드 간 상태 정보를 전파(synchronize)</strong> 하는 데 사용되는 <strong>간단하고 효율적인 통신 프로토콜</strong>입니다.</dd>
  <dd>클러스터의 각 노드들이 서로의 상태 정보를 주기적으로 교환하면서, 장애 감지, 노드 변경 전파, 뷰 일관성 유지 등을 가능하게 해주는 경량 분산 통신 방식입니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>Q: 왜 ‘gossip’ protocol이라고 부르나요?<br />
A: Node간 데이터를 주고받는 방식이, 마치 사람들이 소문을 주고받는 것처럼, 일부 정보만 여러 노드 간에 점진적으로 퍼지기 때문입니다.<br />
ex) “노드 X가 다운된 것 같아” → Y가 듣고 Z에게 전파</p>
</blockquote>

<h4 id="hash-slot">Hash Slot</h4>

<p>Redis Cluster로 구성한다면, ‘Hash Slot’이라는 개념을 사용하게 됩니다.</p>

<dl>
  <dt>‘Hash Slot’은</dt>
  <dd>Cluster에서 데이터를 분산저장하는 Sharding의 최소 단위입니다.</dd>
  <dd>데이터의 Key값과 Node를 맵핑(mapping)해줍니다.</dd>
  <dd>키를 어느 노드에 저장할지를 결정하는 데 사용되는 <strong>고정된 범위(0 ~ 16383)의 해시 공간</strong>입니다.</dd>
  <dd>만약 구성 Node의 변경이 있다면, Hash Slot단위로 Node에 재분배됩니다.(키 단위가 아닌 “슬롯 단위”로 이동되므로 성능과 일관성 측면에서 유리합니다.)</dd>
  <dd>Redis Client는 이 ‘Hash Slot’을 Caching하여, Key값에 따라 해당 Key를 갖고 있는 Node에 바로 접속합니다.</dd>
</dl>

<p><br /></p>

<h2 id="redis가-consistency를-보장하는-방법">Redis가 Consistency를 보장하는 방법</h2>

<h3 id="single-thread를-기반으로-명령어를-원자적으로-처리">Single Thread를 기반으로 명령어를 원자적으로 처리</h3>

<p><img src="/assets/img/for-post/Redis/image%206.png" alt="여러 Client 요청을 처리하는 단일 Redis Instance" />
<em>여러 Client 요청을 처리하는 단일 Redis Instance</em></p>

<p>Redis 서버 프로세스는 단일 스레드로 동작하기 때문에, 들어오는 모든 명령이 순서대로(serialized) 처리됩니다.<br />
덕분에 하나의 명령(command) 은 실행 중 중단되지 않으며, 원자성(Atomicity) 을 자연스럽게 보장합니다.</p>

<blockquote class="prompt-info">
  <p>단, Redis 6부터, ‘클라이언트 소켓 읽기/쓰기’, ‘명령을 실행하기 전/후의 버퍼 작업’에 대해서 멀티쓰레드 I/O가 도입되었습니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>Epoll?<br />
epoll은 Linux 커널에서 제공하는 고성능 I/O 이벤트 통지 메커니즘입니다.<br />
수많은 파일 디스크립터(FD)(예: 클라이언트 소켓)의 입출력 가능 상태(I/O readiness) 를 효율적으로 감시하고, 이벤트가 발생한 것만 알려주는 고성능 커널 기능입니다.</p>
</blockquote>

<h3 id="replication에서는-eventual-consistency사용">Replication에서는 Eventual Consistency사용</h3>

<ul>
  <li>기본적으로 Redis는 Primary에서 처리된 쓰기 작업을 Replica에게 비동기적으로 전파합니다.</li>
  <li>이론적으로, 마스터에 쓰기가 완료된 직후 장애가 나면 일부 업데이트가 슬레이브에 전파되지 않을 수 있어, <strong>최종 일관성(Eventual Consistency)</strong> 을 따릅니다.</li>
</ul>

<h3 id="cluster에서의-consistency">Cluster에서의 Consistency</h3>

<p><img src="/assets/img/for-post/Redis/image%207.png" alt="Cluster로 구성된 Redis" />
<em>Cluster로 구성된 Redis</em></p>

<ul>
  <li>Redis Cluster는 데이터를 16,384개의 <strong>해시 슬롯(hash slot)</strong> 으로 분산 저장합니다.(Sharding)(Cluster가 하나의 instance처럼 작동)</li>
  <li><strong>샤드 간 트랜잭션</strong>을 지원하지 않으며, <strong>하나의 키에 대한 연산은 항상 그 키를 소유한 노드(master)에서 처리</strong>됩니다.</li>
  <li>클러스터 구성원 간 복제는 마스터–슬레이브 모델을 따르므로, 기본적으로 <strong>비동기 복제</strong>에 따른 최종 일관성을 제공합니다.</li>
</ul>

<h2 id="redis의-persistence">Redis의 Persistence</h2>

<p><img src="/assets/img/for-post/Redis/durable-redis-1.png" alt="Redis에서, 'Persistence를 위한 기능들'의 작동 방식" />
<em>Redis에서, ‘Persistence를 위한 기능들’의 작동 방식 | from <a href="https://redis.io/technology/durable-redis/">redis.io</a></em></p>

<blockquote>
  <p>Redis can persist your data either by periodically dumping the dataset to disk or by appending each command to a disk-based log.<br />
- from <a href="https://redis.io/about/">redis.io</a></p>
</blockquote>

<dl>
  <dt>Redis에서는 Data를 보존하기 위해,</dt>
  <dd>정기적으로 Disk에 데이터를 쓰거나(periodically dumping the dataset to disk)</dd>
  <dd>각각의 Command를 Disk기반의 Log에 기록하는(appending each command to a disk-based log)</dd>
</dl>

<p>방법을 사용합니다.</p>

<h3 id="정기적으로-disk에-데이터를-쓰기rdb파일로-쓰기">정기적으로 Disk에 데이터를 쓰기(rdb파일로 쓰기)</h3>

<p>지정된 시점(snapshot)의 메모리 데이터를 통째로 덤프해서 .rdb 파일로 저장하는 방식을 말합니다.</p>

<h3 id="각각의-command를-disk기반의-log에-기록하기aof">각각의 Command를 Disk기반의 Log에 기록하기(AOF)</h3>

<p>Redis에서 실행된 모든 쓰기 명령을 순차적으로 로그 파일에 기록하는 방식입니다.<br />
AOF 파일은 계속 커지므로, 주기적으로 <strong>압축 및 재작성</strong>이 필요합니다.<br />
Redis는 자동으로 ‘BGREWRITEAOF(AOF최적화 명려어)’를 수행해 오래된 명령을 요약합니다.</p>

<h3 id="rdb와-aof조합-방법">rdb와 AOF조합 방법</h3>

<ul>
  <li>RDB만 사용: 단순하고 빠른 복구가 필요한 경우에 사용합니다.</li>
  <li>AOF만 사용: 안정성이 중요한 경우에 사용합니다.</li>
  <li>RDB + AOF 함께 사용<br />
대부분의 실무 환경에서 추천됩니다.<br />
이 경우, 복구 시 AOF가 더 최신이면 AOF 우선으로 사용합니다.(AOF를 더 신뢰)</li>
</ul>

<h2 id="redis와-memcached">Redis와 Memcached</h2>

<p>Redis와 Memcached는 모두 <strong>인메모리 기반의 캐시 시스템</strong>이지만, 목적과 기능 측면에서 다음과 같은 차이점이 있습니다.</p>

<h3 id="데이터-구조"><strong>데이터 구조</strong></h3>

<ul>
  <li><strong>Redis</strong>는 리스트, 해시, 셋, 정렬셋 등 다양한 구조를 제공 → 큐, 랭킹, 통계에 유용합니다.</li>
  <li><strong>Memcached</strong>는 단순한 문자열 key-value 구조 → 복잡한 로직은 애플리케이션에서 직접 구현해야 합니다.</li>
</ul>

<h3 id="persistence-데이터-보존"><strong>Persistence (데이터 보존)</strong></h3>

<ul>
  <li><strong>Redis</strong>는 RDB(rdb파일로 저장)/AOF(Append-Only File) 방식으로 디스크에 데이터를 저장 가능하며, 이를 통해 장애 후 복구가 가능합니다.</li>
  <li><strong>Memcached</strong>는 서버 재시작 시 모든 데이터가 사라집니다.</li>
</ul>

<h3 id="분산성과-고가용성"><strong>분산성과 고가용성</strong></h3>

<ul>
  <li>Redis는 Sentinel, Cluster 등을 통해 <strong>자동 failover</strong> 와 <strong>수평 확장</strong>이 가능합니다.</li>
  <li>Memcached는 클라이언트 단에서 key hashing을 통해 수동 샤딩(consistent hashing)을 구현해야 합니다.</li>
</ul>

<h3 id="pubsub-기능"><strong>Pub/Sub 기능</strong></h3>

<ul>
  <li>Redis는 채널 기반 메시지 전달 기능(Pub/Sub)을 제공하여 실시간 이벤트 알림 등에 사용 가능합니다.</li>
  <li>Memcached는 이런 기능 없이, 단순 Cache용도로 사용합니다.</li>
</ul>

<h3 id="요약">요약</h3>

<dl>
  <dt>‘Memcached’는</dt>
  <dd>가볍고 빠른, 단순 캐시용으로 사용하고,</dd>
  <dt>‘Redis’는</dt>
  <dd>다기능 인메모리 데이터 플랫폼으로 사용합니다.</dd>
</dl>

<p><br /></p>

<h2 id="redis-사용-방법들">Redis 사용 방법들</h2>

<h3 id="redis-as-a-cache">Redis as a Cache</h3>

<p><img src="/assets/img/for-post/Redis/image%208.png" alt="image.png" />
<em>Redis를 Cache로 사용할때의 Flow</em></p>

<p>Redis를 Cache로 사용하는것은 가장 흔하게 사용되는 사례중 하나입니다.</p>

<dl>
  <dt>Redis를 Cache로 사용할때는,</dt>
  <dd>각 key에 대해 TTL(Time-to-live)를 설정합니다.(이는 Redis가 데이터를 관리하는 방법을 가이드 해주는 역할을 합니다.)</dd>
</dl>

<blockquote class="prompt-info">
  <p>Cache용도로 Redis를 사용하다 보면, ‘Hot key’문제에 부딪히게 되는데, 이는 뒤에서 다루려고 합니다.</p>
</blockquote>

<h3 id="redis-as-a-distributed-lock">Redis as a Distributed Lock</h3>

<p>‘Distributed Lock’으로 Redis를 사용하는것 또한 가장 흔한 사례중 하나입니다.<br />
만약 System에서, ‘Ticket 구매’와 같이 ‘Strong Consistency’가 필요한 경우, Redis를 사용하여 Lock을 구현할 수 있습니다.</p>

<blockquote class="prompt-info">
  <p>만약, 사용하고 있는 DB레벨에서 이미 Consistency를 제공하고 있다면, 해당 기능을 쓰는게 좋습니다. Redis를 통해 Lock을 구현하면, 불필요하게 복잡도를 높이게 됩니다.</p>
</blockquote>

<p>Redis를 이용하여 ‘Distributed Lock(분산 Lock)’을 구현하는 알고리즘 및 프로토콜로 ‘Redlock’이 있습니다.</p>

<h3 id="redis-for-leaderboards">Redis for LeaderBoards</h3>

<p>Redis의 ‘sorted set’데이터 타입은 정렬된 데이터를 제공해주며, $log(N)$시간의 Query응답을 제공해줍니다.<br />
이는 LeaderBoard App에 적합한 스펙입니다.<br />
만약 LeaderBoard와 같이, Write throughput은 높고, Read Latency는 낮아야 하는 경우를 SQL DB로 대응하려고 하면, 꽤나 힘들겁니다.</p>

<h3 id="redis-for-rate-limiting">Redis for Rate Limiting</h3>

<p>Rate Limiting은 특정 클라이언트가 지나치게 많은 요청을 보내지 못하도록 제한하는 기법입니다.</p>

<dl>
  <dt>Redis는</dt>
  <dd>고속의 in-memory 연산 + TTL 기능 + 원자적 연산을 제공하기 때문에, ‘Rate Limiting’용으로 사용할 수 있습니다.</dd>
  <dd>카운터 기반, 토큰 버킷, 슬라이딩 윈도우 등 다양한 알고리즘으로 지원할 수 있습니다.</dd>
</dl>

<p><a href="https://redis.io/learn/howtos/ratelimiting">How to build a Rate Limiter using Redis</a></p>

<h3 id="redis-for-proximity-search근접-검색-예-위치-기반-검색-또는-유사-단어-검색">Redis for Proximity Search(근접 검색, 예: 위치 기반 검색 또는 유사 단어 검색)</h3>

<p>Redis는 기본적으로 전통적인 RDBMS의 공간 인덱스(GIS) 나 벡터 검색 엔진은 아니지만, 몇 가지 기능을 조합하여 근접 검색을 구현할 수 있습니다.</p>

<p><a href="https://redis.io/learn/howtos/solutions/geo/getting-started">Getting Started With Geo Location Search in Redis</a></p>

<p>‘Proximity Search에는 2가지 맥락이 있는데, ‘지리적 근접 검색’과 ‘유사 문자열 검색’이 그것입니다.</p>

<h4 id="redis-geo-기능으로-위치-기반-proximity-search">Redis GEO 기능으로 위치 기반 Proximity Search</h4>
<p>Redis는 GEOADD, GEORADIUS, GEODIST, GEOPOS 등의 명령어로 <strong>지리 정보(위도, 경도)</strong>를 저장하고 검색할 수 있습니다.<br />
내부적으로는 Geohash + Sorted Set으로 구현되어 있습니다.</p>

<h4 id="문자열-유사성-기반-proximity-search">문자열 유사성 기반 Proximity Search</h4>

<p>Redis 자체적으로 지원하지는 않고, <a href="https://github.com/RediSearch/RediSearch">RediSearch</a> 모듈을 통해 구현할 수 있습니다.<br />
‘RediSearch’는 텍스트 인덱싱, 검색, 유사 단어 매칭, <strong>벡터 검색 (ANN)</strong> 까지도 지원합니다.</p>

<h3 id="redis-for-event-sourcing">Redis for Event Sourcing</h3>

<p>Redis 5.0부터 추가된 데이터 구조인 ‘Stream’을 통해, ‘Event Sourcing’패턴을 구현할 수 있습니다.</p>

<p><img src="/assets/img/for-post/Redis/redis-3.png" alt="Redis streams and consumer groups" />
<em>Redis streams and consumer groups | <a href="https://www.hellointerview.com/learn/system-design/deep-dives/redis#redis-for-event-sourcing">hellointerview.com</a></em></p>

<dl>
  <dt>‘Event Sourcing’은</dt>
  <dd>상태를 저장하는 대신, 모든 변경 이벤트의 로그(event stream)를 기록해서, 나중에 그 이벤트들을 재생(replay)하여 현재 상태를 만들어내는 아키텍처 패턴입니다.</dd>
  <dt>‘Redis Stream’은</dt>
  <dd>‘Append-only Log’처럼 작동하는 자료구조입니다.</dd>
  <dd>시간 순서대로, Log형식으로 데이터를 저장합니다.</dd>
  <dd>내부적으로는 Kafka의 topic-like 구조와 매우 유사하며, ID 순 정렬, 범위 조회, consumer group 처리 등이 가능합니다.</dd>
</dl>

<p>때문에 ‘Stream’을 통해 ‘Event Sourcing’을 구현할 수 있으며, 메모리 기반이라 매우 빠른 성능을 제공해줍니다.(Redis 특성에서 오는 데이터 유실 가능성이 있음.)</p>

<h3 id="redis-for-pubsub">Redis for Pub/Sub</h3>

<p><img src="/assets/img/for-post/Redis/image%209.png" alt="Redis Pub/Sub" />
<em>Redis Pub/Sub | from <a href="https://www.geeksforgeeks.org/system-design/redis-publish-subscribe/">geeksforgeeks.org</a></em></p>

<p>Redis는 자체적으로 ‘publish/subscribe(Pub/Sub)’ Messaging pattern을 지원하며, 메시지 브로커처럼 채널 기반으로 메시지를 전달하는 기능입니다.<br />
이는 주로 Chat System이나 실시간 Notification 혹은 ‘Message생산자와 소비자를 decouple하는 시나리오’에서 사용됩니다.</p>

<blockquote class="prompt-info">
  <p>Redis는, Pub/Sub에 대해서도 sharding을 지원합니다. (Redis 구버전에서는 불가)</p>
</blockquote>

<p><strong>장점</strong></p>
<ul>
  <li>메모리 기반이라 초 저지연 메세징 기능을 제공합니다.</li>
  <li>간단한 설정으로 바로 사용할 수 있습니다.</li>
  <li>Pub/Sub Client는 Redis Cluster를 구성하는 각 Node마다 하나의 Connection을 사용합니다.<br />
Pub/Sub Channel마다 하나씩 쓰는게 아니라서, 커넥션 사용을 최소화할 수 있습니다.</li>
</ul>

<p><strong>단점</strong></p>
<ul>
  <li>
    <p>메세지 영속성(Persistence)이 없습니다.<br />
Subscriber가 연결되어 있지 않으면, 메시지는 버려집니다.</p>
  </li>
  <li>
    <p>큐(queue) 가 아닌, 모든 구독자에게 동시에 전달하는 ‘Broadcast’구조입니다.</p>
  </li>
</ul>

<p><br /></p>

<h2 id="redis의-단점과-개선방법">Redis의 단점과 개선방법</h2>

<h3 id="hot-key-issues">Hot Key Issues</h3>

<p><img src="/assets/img/for-post/Redis/redis-4.png" alt="Redis Cluster에서 0-100범위에 대해 Hot Key Issue가 발생" />
<em>Redis Cluster에서 0-100범위에 대해 Hot Key Issue가 발생 | from <a href="https://www.hellointerview.com/learn/system-design/deep-dives/redis#redis-for-event-sourcing">hellointerview.com</a></em></p>

<dl>
  <dt>‘Hot Key Issues’는</dt>
  <dd>특정 키(key)에 과도하게 많은 요청이 집중되어 시스템의 성능 저하 또는 병목이 발생하는 문제를 말합니다.</dd>
  <dd>Redis의 특정 key에 트래픽이 몰리면서 해당 key와 연관된 노드나 자원이 비정상적으로 과부하 되는 문제입니다.</dd>
</dl>

<p>Redis는 기본적으로 <strong>단일 스레드 기반이기 때문</strong>에, 하나의 key에 너무 많은 명령이 몰리면 그 key를 포함한 처리 루프 전체가 지연될 수 있습니다.<br />
만약 Cluster환경이라면, 해당 key가 할당된 해시 슬롯(slots)이 포함된 특정 노드에만 부하가 집중될 수 있으며, 이는 노드간 불균형(CPU, 메모리, 네트워크 I/O에 대한)을 초래합니다.</p>

<h4 id="해결-방법">해결 방법</h4>

<ul>
  <li>Client level에서 In-memory Cache를 추가하여, Redis에 너무 많은 요청이 발생하지 않도록 합니다.</li>
  <li>Redis에서 사용하는 Key에 Random number를 추가하여, 여러 Node에 걸쳐서 데이터가 분산되도록 합니다. (Key Sharding)<br />
예: “rank:global” → “rank:global:shard1”, “rank:global:shard2”</li>
</ul>

<h3 id="big-key-issues">Big Key Issues</h3>

<dl>
  <dt>Big Key Issue란,</dt>
  <dd>Redis의 단일 key가 너무 많은 요소(예: 리스트, 해시, 셋 등)를 가지고 있어서, 해당 key에 대한 연산이 느려지거나 전체 Redis 인스턴스에 영향을 주는 문제입니다.</dd>
  <dd>key 자체가 크다는 의미가 아니라, 하나의 key에 저장된 데이터가 지나치게 많은 경우를 말합니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>‘Big Key’에 대한 정의 및 평가 기준은 실제 사용 및 애플리케이션의 특정 요구 사항에 따라 달라질 수 있습니다.<br />
예를 들어, 높은 동시성 및 낮은 지연 시간 시나리오에서는 10KB의 키만 빅 키로 간주될 수 있습니다. 그러나 낮은 동시성 및 고용량 환경에서는 빅 키의 임계값이 약 100KB일 수 있습니다</p>
</blockquote>

<h4 id="해결-방법-1">해결 방법</h4>
<ul>
  <li>
    <p>Key를 나눕니다.(Sharding / Chunking)<br />
큰 리스트를 “chat:room:123:page:1”, “chat:room:123:page:2” 처럼 분할합니다.</p>
  </li>
  <li>Key에 대한 전체 연산은 피합니다.</li>
  <li>문자열 data인 경우, 압축을 고려합니다.</li>
</ul>

<h2 id="references">References</h2>
<dl>
  <dt>Redis | hellointerview.com</dt>
  <dd><a href="https://www.hellointerview.com/learn/system-design/deep-dives/redis">Hello Interview | System Design in a Hurry</a></dd>
  <dt>Redis About | redis.io</dt>
  <dd><a href="https://redis.io/about/">About - Redis</a></dd>
  <dt>Redis Distributed Caching | redis.io</dt>
  <dd><a href="https://redis.io/glossary/distributed-caching/">Distributed Caching</a></dd>
  <dt>Redis Cluster Architecture | redis.io</dt>
  <dd><a href="https://redis.io/technology/redis-enterprise-cluster-architecture/">Redis Cluster Architecture | Redis Enterprise</a></dd>
  <dt>Three Ways to Maintain Cache Consistency | redis.io</dt>
  <dd><a href="https://redis.io/blog/three-ways-to-maintain-cache-consistency/">Three Ways to Maintain Cache Consistency | Redis</a></dd>
  <dt>Redis Race Condition | redis.io</dt>
  <dd><a href="https://redis.io/glossary/redis-race-condition/">Redis Race Condition</a></dd>
  <dt>High-Concurrency Practices of Redis: Snap-Up System | alibabacloud.com</dt>
  <dd><a href="https://www.alibabacloud.com/blog/high-concurrency-practices-of-redis-snap-up-system_597858">High-Concurrency Practices of Redis: Snap-Up System</a></dd>
  <dt>Understanding the Failover Mechanism of Redis Cluster | alibabacloud.com</dt>
  <dd><a href="https://www.alibabacloud.com/blog/understanding-the-failover-mechanism-of-redis-cluster_594707">Understanding the Failover Mechanism of Redis Cluster</a></dd>
  <dt>Snapshotting | redis.io</dt>
  <dd><a href="https://redis.io/docs/latest/operate/oss_and_stack/management/persistence/#snapshotting">Redis persistence</a></dd>
  <dt>Append-only file | redis.io</dt>
  <dd><a href="https://redis.io/docs/latest/operate/oss_and_stack/management/persistence/#append-only-file">Redis persistence</a></dd>
  <dt>Redis Architecture의 진화 | bytebytego.com</dt>
  <dd><a href="https://bytebytego.com/guides/how-redis-architecture-evolve/">ByteByteGo | How Redis Architecture Evolved</a></dd>
  <dt>What makes Redis lightning fast ? | engineeringatscale.substack.com</dt>
  <dd><a href="https://engineeringatscale.substack.com/p/what-makes-redis-lightning-fast">What makes Redis lightning fast ?</a></dd>
  <dt>Linux – IO Multiplexing – Select vs Poll vs Epoll | devarea.com</dt>
  <dd><a href="https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll/">Linux – IO Multiplexing – Select vs Poll vs Epoll</a></dd>
  <dt>BGREWRITEAOF | redis.io</dt>
  <dd><a href="https://redis.io/docs/latest/commands/bgrewriteaof/">BGREWRITEAOF</a></dd>
  <dt>Distributed Locks with Redis | redis.io</dt>
  <dd><a href="https://redis.io/docs/latest/develop/clients/patterns/distributed-locks/">Distributed Locks with Redis</a></dd>
  <dt>How to build a Rate Limiter using Redis | redis.io</dt>
  <dd><a href="https://redis.io/learn/howtos/ratelimiting">How to build a Rate Limiter using Redis</a></dd>
  <dt>Getting Started With Geo Location Search in Redis | redis.io</dt>
  <dd><a href="https://redis.io/learn/howtos/solutions/geo/getting-started">Getting Started With Geo Location Search in Redis</a></dd>
  <dt>How to Use Redis as an Event Store for Communication Between Microservices | redis.io</dt>
  <dd><a href="https://redis.io/blog/use-redis-event-store-communication-microservices/">How to Use Redis as an Event Store for Communication Between Microservices | Redis</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="System Design Interview" /><category term="Key Technologies" /><category term="System Design" /><category term="interview" /><category term="redis" /><summary type="html"><![CDATA[Redis 소개]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/Redis/redis-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/Redis/redis-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Consistent Hashing | Core Concepts - System Design Interview</title><link href="https://blog.devpour.net/posts/Consistent-Hashing/" rel="alternate" type="text/html" title="Consistent Hashing | Core Concepts - System Design Interview" /><published>2025-07-30T02:19:00+09:00</published><updated>2025-07-30T02:19:00+09:00</updated><id>https://blog.devpour.net/posts/Consistent%20Hashing</id><content type="html" xml:base="https://blog.devpour.net/posts/Consistent-Hashing/"><![CDATA[<dl>
  <dt>‘Consistent Hashing’은</dt>
  <dd>분산 시스템(Distributed System)의 Cluster에서, 데이터를 분산 저장할때 사용하는 기초적인 알고리즘 입니다.</dd>
</dl>

<h2 id="예시로-보는-consistent-hashing의-필요성">예시로 보는 Consistent Hashing의 필요성</h2>

<p>‘Ticketing System을 구성한다고 해봅시다.<br />
<img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-1.png" alt="Simple System과 Sharding이 적용된 System" />
<em>Simple System과 Sharding이 적용된 System</em></p>

<p>Client-Servcer-DataBase가 하나씩 구성되어 있는 ‘Simple System’으로 운영이 가능할때는 괜찮지만, 곧 다루는 이벤트(행사의 이벤트)가 늘면서 Data를 여러 Node에 분산시켜야 하게 됩니다.</p>

<h3 id="데이터를-분산시키는-방법--simple-modulo-hashing">데이터를 분산시키는 방법 : Simple Modulo Hashing</h3>

<p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-3.png" alt="단순한 Modulo를 이용한 알고리즘" />
<em>단순한 Modulo를 이용한 알고리즘</em></p>

<p>가장 단순하게 접근하면,</p>

<ol>
  <li>Event의 ID를 Hash처리하여, number값으로 만듭니다.</li>
  <li>해당 number값을 Modulo(%)연산을 통해 데이터베이스를 할당합니다.</li>
  <li>할당된 DB에 데이터를 저장합니다.</li>
</ol>

<h4 id="이때의-문제점">이때의 문제점</h4>

<p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-4.png" alt="DB Cluster에 Node가 추가된다면.." />
<em>DB Cluster에 Node가 추가된다면..</em></p>

<p>이때, DB Cluster에 Node가 추가된다면, Modulo의 값이 바뀌게 되면서, <strong>전체 Data가 재분배(redistributed)되어야 하는 상황</strong>이 발생합니다.</p>

<blockquote class="prompt-info">
  <p>이런 Data 재분배(redistributed)상황은 DB의 많은 리소스를 차지해서, 시스템에 장애를 만들어낼 수 있습니다. 즉, DB운영 작업에 리소스가 많이 투입되어, 외부 요청을 처리하지 못하게 됩니다.</p>
</blockquote>

<p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-5.png" alt="DB Cluster에 Node가 제거된다면.." />
<em>DB Cluster에 Node가 제거된다면..</em></p>

<p>반대로, DB Cluster에서 Node가 제거되는 상황에서도, Data 재분배(redistributed)가 발생하게 됩니다.</p>

<p><br /></p>

<h3 id="consistent-hashing의-필요성">Consistent Hashing의 필요성</h3>

<p>‘Consistent Hashing’은 이런, 분산환경(Distributed System)에서 DB Instance구성이 변경되는 상황에 대한 솔루션을 제공해줍니다.</p>

<blockquote class="prompt-info">
  <p>Q: 여기서 ‘Consistent’의 의미는 무엇인가요?<br />
A: 노드가 추가되거나 제거되더라도(분산 환경 변화에도) 키–노드 매핑이 일관되게(consistent) 유지된다는 특성에서 유래하고 있습니다.<br />
이전 방식인 Modulo방식은 Node 구성이 변경됨에 따라, 전체 데이터가 re-hashing되어야 하지만, ‘Consistent Hashing’은 일관된 Hash값을 제공하여, re-hashing을 최소화합니다.</p>
</blockquote>

<p>핵심 아이디어는, <strong>Data와 Database를 ‘hash ring’이라 불리는 ‘Circular space(순환 공간)’에 정리</strong>해 두는 것입니다.</p>

<p><img src="/assets/img/for-post/Consistent%20Hashing/image.png" alt="Consistent Hash Ring" />
<em>Consistent Hash Ring</em></p>

<p><br /></p>

<h2 id="consistent-hashing의-작동-과정">Consistent Hashing의 작동 과정</h2>

<ol>
  <li>
    <p>DB Node가 4개 있다고 가정했을때, 이를 통해 ‘Hash Ring’을 구성합니다.<br />
그리고 이 Hash Ring의 고정값의 범위를 0 ~ 100으로 구성합니다.(예시로 구성, 100 이상도 가능합니다.)<br />
DB Node들은 이 Hash Ring에 일정하게 분포되도록 합니다.(데이터를 균일하게 분포시키기 위해)</p>

    <p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-6.png" alt="Hash Ring" class="w-50" />
<em>Hash Ring | <a href="https://www.hellointerview.com/learn/system-design/deep-dives/consistent-hashing#consistent-hashing">hellointerview.com</a></em></p>
  </li>
  <li>
    <p>여기서 삽입(insert)하고자 하는 데이터의 hash값이 만들어지면, 시계 방향으로 제일 가까운 DB Node에 저장합니다.<br />
이 과정은, 아래와 같습니다.</p>
    <ol>
      <li>데이터의 Key값을 Hash처리 합니다.</li>
      <li>
        <p>Hash값을 기반으로, Hash Ring위의 위치를 계산합니다.<br />
이때, Hash처리한 값을 Hash Ring위의 좌표로 변환하기 위해, Hash Ring 값의 범위로 Modulo하는 과정을 거칩니다.</p>

        <p>\(h_{\rm ring} = \text{hash}(\,\text{key}\,) \bmod M \text{(M은 Ring의 크기)}\)</p>
        <blockquote class="prompt-info">
          <p>이는, $H = \text{hash}(key)$의 값의 범위가, 보통 Hash Ring의 범위보다 크기 때문에 필요합니다.</p>
        </blockquote>
      </li>
      <li>$h_{\rm ring}$(Hash Ring위의 좌표)에서 값을 증가시키며, 가장 가까운 DB Node를 찾습니다.(Ceil연산)</li>
      <li>DB Node를 찾게 되면, 해당 Node에 데이터를 저장합니다.</li>
    </ol>
  </li>
</ol>

<h3 id="db-node가-추가되는-경우">DB Node가 추가되는 경우</h3>

<p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-7.png" alt="DB Node가 추가되는 경우" />
<em>DB Node가 추가되는 경우 | <a href="https://www.hellointerview.com/learn/system-design/deep-dives/consistent-hashing#consistent-hashing">hellointerview.com</a></em></p>

<p>이 경우, 전체 데이터가 아닌, <strong>일부 데이터에 대해 데이터 재배치가 이루어집니다.</strong></p>

<dl>
  <dt>만약, DB4와 DB1사이에 새로운 DB5를 위치시킨다면,</dt>
  <dd>DB1에 가야 했던 일부 데이터가 DB5로 재배치됩니다.</dd>
  <dd>다른 데이터들은 그대로 위치합니다.</dd>
  <dd>이 예시의 경우, DB1에 있던 데이터의 약 30%정도의 데이터만 재배치 됩니다.(Hash Ring의 값에 따른 추정)</dd>
</dl>

<h3 id="db-node가-제거되는-경우">DB Node가 제거되는 경우</h3>

<p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-8.png" alt="DB Node가 제거되는 경우" />
<em>DB Node가 제거되는 경우 | <a href="https://www.hellointerview.com/learn/system-design/deep-dives/consistent-hashing#consistent-hashing">hellointerview.com</a></em></p>

<p>이 경우, 제거된 Node에 있던 데이터만 재배치 됩니다.</p>

<dl>
  <dt>만약, DB2가 시스템 장애로 Cluster에서 제거됐다면,</dt>
  <dd>DB2에 있던 데이터 전부가, DB3에 재배치됩니다.</dd>
  <dd>다른 데이터들은 그래도 위치합니다.</dd>
</dl>

<p>여기까지, ‘단순 Modulo방식’에 비해 많이 개선된 부분이 있지만, 여전히 ‘DB Node가 제거되는 경우 Node의 전체 데이터가 재배치되는 문제’가 있습니다.<br />
이는 Hash Ring위에서 <strong>DB Node간 데이터양의 불균형</strong>을 만들어냅니다.<br />
<strong>이를 해결하기 위해, ‘Virtual Node’로 Hash Ring을 구성하는 방법을 사용</strong>합니다.</p>

<p><br /></p>

<h2 id="virtual-nodes로-hash-ring-구성하기">Virtual Nodes로 Hash Ring 구성하기</h2>

<p><img src="/assets/img/for-post/Consistent%20Hashing/consistent-hashing-9.png" alt="Hash Ring을 Virtual Node로 구성합니다." />
<em>Hash Ring을 Virtual Node로 구성합니다. | <a href="https://www.hellointerview.com/learn/system-design/deep-dives/consistent-hashing#virtual-nodes">hellointerview.com</a></em></p>

<dl>
  <dt>이 ‘Virtual Nodes’방법은,</dt>
  <dd>Hash Ring위에 단 하나의 지점에만 Node를 배치하는것이 아닌, 여러 지점에 가상(Virtual)으로 배치하는 방법입니다.</dd>
  <dt>만약, DB2가 Fail상태가 된다면,</dt>
  <dd>Hash값에 따라, 데이터가 재배치 됩니다.</dd>
  <dd>이때, Hash값에 따라, 남아 있는 다른 Node(DB1, DB3, DB4)에 분산되어 재배치됩니다.</dd>
</dl>

<p>즉, Node가 Fail상태가 되면, 해당 Node의 데이터가 <strong>여러 Node로 분산되어 재배치</strong>됩니다.</p>

<h3 id="virtual-nodes를-사용하는-이유">Virtual Nodes를 사용하는 이유</h3>

<ul>
  <li>
    <p><strong>데이터 분포 균일화</strong><br />
실제 노드가 링 위에 딱 하나의 점으로만 존재하면, 해시 함수 특성에 따라 특정 영역에 데이터가 몰릴 수 있습니다.<br />
각 실제 노드를 여러 개의 작은 “가상 노드”로 분할해 링 위에 고르게 흩어 놓으면, 키가 더 고르게 분산됩니다.</p>
  </li>
  <li>
    <p><strong>노드 용량·성능 차이 반영</strong><br />
머신마다 처리 성능이 다를 때, <strong>노드마다 할당할 가상 노드 수를 달리 줌으로써</strong> “무거운” 머신에 더 많은 샤드를 몰아줄 수 있습니다.<br />
예) CPU·메모리가 2배인 노드는 가상 노드를 2배 배치</p>
  </li>
  <li>
    <p><strong>노드 추가·제거 시 부드러운 재배치</strong><br />
실제 노드 하나를 추가/삭제할 때마다 전체 키 공간 중 가상 노드 하나 분량만 이동하면 되므로,<br />
“영향받는 키 비율”이 <strong>1/N</strong> → <strong>1/(N·v)</strong> 수준으로 더욱 작아집니다.(여기서 v는 각 실제 노드당 가상 노드 개수)</p>
  </li>
  <li>
    <p><strong>운영 유연성</strong><br />
가상 노드 단위로 릴리스·점검이 가능해, 실제 노드를 직접 건드리지 않고도 롤링 업데이트나 장애 격리가 수월해집니다.</p>
  </li>
</ul>

<p><br /></p>

<h2 id="consistent-hashing-사용-사례">Consistent Hashing 사용 사례</h2>

<p>‘Consistent Hashing’은 데이터를 분산하는 ‘방법’에 해당하기 때문에, DB뿐만 아니라, Cache, Message Broker등의 사례가 있습니다.</p>

<h3 id="apache-cassandra">Apache Cassandra</h3>

<dl>
  <dt>‘Apache Cassandra’는</dt>
  <dd>분산 키-값 저장소로, 내부적으로 Consistent Hashing 기반의 토큰 링(token ring) 구조를 사용해 데이터를 분산·저장합니다</dd>
  <dd>Node(virtual node) 개념을 도입해, 클러스터 내 각 물리 Node에 여러 개(기본 256개)의 토큰을 랜덤 배치하며, 데이터 분포를 훨씬 고르게 만듭니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>Apache Cassandra는 Amazon의 ‘Dynamo’ 분산 저장 시스템을 사용하고 있습니다.<br />
(이런 Case를 Dynamo-style 시스템이라고 합니다.)</p>
</blockquote>

<p><img src="/assets/img/for-post/Consistent%20Hashing/image%201.png" alt="Apache Cassandra의 Token Ring" class="w-50" />
<em>Apache Cassandra의 Token Ring | <a href="https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html#consistent-hashing-using-a-token-ring">cassandra.apache.org</a></em></p>

<h3 id="amazon-dynamodb">Amazon DynamoDB</h3>

<dl>
  <dt>‘Amazon DynamoDB’은</dt>
  <dd>완전관리형 NoSQL 키–값·문서(Document) 데이터베이스입니다.</dd>
  <dd>데이터의 ‘Partition’ 과정에서 ‘Consistent Hashing’을 사용하고 있습니다.</dd>
</dl>

<h3 id="content-delivery-networkscdns">Content Delivery Networks(CDNs)</h3>

<dl>
  <dt>‘CDNs’은</dt>
  <dd>전 세계에 분산된 엣지(Edge) 서버들에 콘텐츠를 캐시(Cache)하고, 사용자의 요청을 가장 적절한 서버로 라우팅해 응답 지연(latency)을 줄이는 시스템입니다.</dd>
  <dd>CDN에서 Server(Node) Pool을  관리할때, ‘Consistent Hashing’을 사용합니다.</dd>
  <dd>요청 URL에 따라 이 Traffic을 처리할 Node가 정해지는 방식입니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>단순히 URL로만 Node가 정해지면, 특정 Server(Node)가 ‘Hot’상태에 도달하게 됩니다.<br />
때문에, 실제 CDN의 설계는 Consistent Hashing 단독이 아니라, 여러 기법을 조합해 핫스팟을 완화하게 됩니다.(인기 콘텐츠일 수록 여러 Edge서버에 복제본을 두는 등..의 방식 사용)</p>
</blockquote>

<p><br /></p>

<h2 id="references">References</h2>

<dl>
  <dt>Consistent Hashing | hellointerview.com</dt>
  <dd><a href="https://www.hellointerview.com/learn/system-design/deep-dives/consistent-hashing">Hello Interview | System Design in a Hurry</a></dd>
  <dt>Consistent hash ring | researchgate.net</dt>
  <dd><a href="https://www.researchgate.net/figure/Consistent-hash-ring-and-forwarding-process_fig3_328161011">FIGURE 4: Consistent hash ring and forwarding process.</a></dd>
  <dt>Consistent Hashing Explained | systemdesign.one</dt>
  <dd><a href="https://systemdesign.one/consistent-hashing-explained">Consistent Hashing Explained</a></dd>
  <dt>Design Consistent Hashing | bytebytego.com</dt>
  <dd><a href="https://bytebytego.com/courses/system-design-interview/design-consistent-hashing">System Design · Coding · Behavioral · Machine Learning Interviews</a></dd>
  <dt>Dynamo | cassandra.apache.org</dt>
  <dd>
    <blockquote>
      <p>Apache Cassandra relies on a number of techniques from Amazon’s Dynamo distributed storage key-value system.</p>
    </blockquote>
  </dd>
  <dd>Apache Cassandra에서의 분산 저장 시스템은 Amazon의 Dynamo의 테크닉에 영향을 받았다는 얘기.(Dynamo-style 시스템)</dd>
  <dd><a href="https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html">Dynamo | Apache Cassandra Documentation</a></dd>
  <dt>Dynamo: Amazon’s Highly Available Key-value Store(논문) | www.cs.cornell.edu</dt>
  <dd><a href="https://www.cs.cornell.edu/courses/cs5414/2017fa/papers/dynamo.pdf">www.cs.cornell.edu</a></dd>
  <dt>How is hashing speeding up your CDN | cdn77.com</dt>
  <dd><a href="https://www.cdn77.com/blog/how-is-hashing-speeding-up-your-cdn">How is hashing speeding up your CDN | CDN77.com</a></dd>
  <dt>Distributing Content to Open Connect | netflixtechblog.com</dt>
  <dd>
    <blockquote>
      <p>We use Consistent Hashing to distribute content across multiple servers as follows.</p>
    </blockquote>
  </dd>
  <dd><a href="https://netflixtechblog.com/distributing-content-to-open-connect-3e3e391d4dc9">Distributing Content to Open Connect</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="System Design Interview" /><category term="Core Concepts" /><category term="System Design" /><category term="interview" /><category term="Computer Science" /><category term="Consistent Hashing" /><category term="hash" /><summary type="html"><![CDATA[‘Consistent Hashing’은 분산 시스템(Distributed System)의 Cluster에서, 데이터를 분산 저장할때 사용하는 기초적인 알고리즘 입니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/Consistent%20Hashing/consistent-hashing-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/Consistent%20Hashing/consistent-hashing-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">CAP Theorem(정리) | Core Concepts - System Design Interview</title><link href="https://blog.devpour.net/posts/CAP-Theorem/" rel="alternate" type="text/html" title="CAP Theorem(정리) | Core Concepts - System Design Interview" /><published>2025-07-25T00:04:00+09:00</published><updated>2025-07-25T22:26:04+09:00</updated><id>https://blog.devpour.net/posts/CAP%20Theorem</id><content type="html" xml:base="https://blog.devpour.net/posts/CAP-Theorem/"><![CDATA[<h2 id="cap-theorem-소개">CAP Theorem 소개</h2>

<p>CAP는 각각 Consistency, Availability, Partition Tolerance를 의미합니다.</p>

<dl>
  <dt>이 ‘CAP Theorem’은</dt>
  <dd>‘Distributed System(분산처리 시스템)’의 <strong>3가지 핵심 속성에서, 이중 딱 2개만 취할 수 있다</strong>는 theorem(정리, 일정한 조건하에 참이라는 것이 증명됨)입니다.</dd>
  <dd>즉, <strong>3가지 속성사이의 Trade-off 관계</strong>를 설명하는 이론입니다.</dd>
</dl>

<p><img src="/assets/img/for-post/CAP%20Theorem/960px-CAP_Theorem_Venn_Diagram.png" alt="CAP theorem Euler diagram" class="w-50" />
<em>CAP theorem Euler diagram | <a href="https://en.wikipedia.org/wiki/CAP_theorem">en.wikipedia.org</a></em></p>

<p>각각의 속성은 다음과 같습니다.</p>

<ul>
  <li><strong>Consistency(일관성)</strong><br />
모든 노드가 ‘같은 시점에 동일한 데이터를 갖고 있다는것’을 보장하는것을 말합니다.<br />
분산환경에서, 클라이언트가 어떤 노드에 요청하든 항상 동일한 응답을 받을 수 있습니다.
    <blockquote class="prompt-info">
      <p>여기서의 Consistency는 DB의 ACID에 있는 Consistency와는 다른 맥락을 갖고 있습니다.<br />
ACID의  Consistency의 경우, 트랜잭션 전후, 데이터에 대한 ‘무결성 제약조건’을 말합니다.<br />
CAP의 Consistency<strong>:</strong> “우리 가게 모든 지점에서 같은 가격표를 붙이자”<br />
ACID의 Consistency<strong>:</strong> “가격표는 항상 숫자이고, 재고보다 많은 수량은 팔 수 없다”</p>
    </blockquote>
  </li>
  <li>
    <p><strong>Availability(가용성</strong>)<br />
모든 요청에 대해 ‘항상 응답이 오며, 실패하지 않는것’을 말합니다.<br />
응답이 늦어지거나 오류 없이 처리되는걸 의미합니다.</p>
  </li>
  <li><strong>Partition Tolerance(분할 허용)</strong><br />
네트워크가 분할되어 일부 노드 간 통신이 불가능해져도, 시스템은 계속 동작해야 하는것을 말합니다.</li>
</ul>

<p><br /></p>

<h3 id="cap조합과-적용-예시">CAP조합과 적용 예시</h3>

<table>
  <thead>
    <tr>
      <th><strong>조합</strong></th>
      <th><strong>설명</strong></th>
      <th><strong>대표 시스템 예시</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>CP (일관성 + 분할 허용)</strong></td>
      <td>네트워크 분할 시, 일부 요청은 차단되더라도 일관성을 유지합니다.</td>
      <td>HBase, MongoDB (옵션에 따라), Redis Sentinel</td>
    </tr>
    <tr>
      <td><strong>AP (가용성 + 분할 허용)</strong></td>
      <td>네트워크 분할 시에도 응답을 주지만, 일관성은 잠시 깨질 수 있습니다.</td>
      <td>Cassandra, Couchbase, DynamoDB</td>
    </tr>
    <tr>
      <td><strong>CA (일관성 + 가용성)</strong></td>
      <td>네트워크가 항상 정상적이라는 가정에서 가능합니다. (현실적으로는 어려움)</td>
      <td>단일 노드 시스템 (e.g. RDBMS에서 분산 미적용 시)</td>
    </tr>
  </tbody>
</table>

<h3 id="현대-분산환경에서-partition-tolerance는-필수">현대 분산환경에서, Partition Tolerance는 필수.</h3>

<p>현대 분산 시스템은 네트워크 지연, 패킷 손실, 장애 등으로 인해 <strong>Partition(분할)이 언제든 발생할 수 있으므로</strong>, P를 포기할 수 없습니다.<br />
따라서, <strong>CAP 이론의 실질적인 선택은 C와 A 중 어떤 것을 포기할 것인가</strong>에 대한 문제입니다.<br />
Network Partition(네트워크 분할)이 발생했을때, ‘Consistency와 Availability중 어떤걸 선택하는냐’의 문제라고 정리할 수 있습니다.</p>

<p><br /></p>

<h2 id="cap-theorem-시나리오-예시">CAP Theorem 시나리오 예시</h2>

<h3 id="시나리오상의-idle-시스템-상황">시나리오상의 Idle 시스템 상황</h3>

<p>우리가, USA와 유럽 Region에 각각 서버를 운영하고 있다고 가정합니다.<br />
만약 유저가 자신의 프로필 정보(Public으로 노출되는)를 변경한다면, 다음과 같은 Flow가 만들어질 수 있습니다.</p>

<ol>
  <li>UserA는 USA에 서버에 있는 프로필 정보를 update합니다.</li>
  <li>USA에서 변경된 Profile이 유럽으로 복제(전파)됩니다.</li>
  <li>UserB가 유럽서버에 대해, UserA의 정보를 조회합니다.</li>
</ol>

<p><img src="/assets/img/for-post/CAP%20Theorem/cap-scenario-1.png" alt="시나리오상 Idle상황" />
<em>시나리오상 Idle상황</em></p>

<p><br /></p>

<h3 id="문제-발생">문제 발생</h3>

<p><img src="/assets/img/for-post/CAP%20Theorem/cap-scenario-2.png" alt="Region간 Network가 끊긴 상황" />
<em>Region간 Network가 끊긴 상황</em></p>

<p>이런 Flow에서, USA와 유럽서버간 Network연결이 끊긴다면, 우리는 2가지 옵션중 선택해야 합니다.</p>

<ul>
  <li>
    <p>Option A (Consistency 우선 옵션)<br />
Error을 Return합니다. 우리는 최신정보가 반환되지 못하면 에러로 판단합니다.(Consistency를 선택하는 경우)</p>
  </li>
  <li>
    <p>Option B (Availability 우선 옵션)<br />
최신데이터가 아니어도, 데이터를 반환합니다.(Availability를 선택하는 경우)</p>
  </li>
</ul>

<h3 id="시나리오-결과-정리">시나리오 결과 정리</h3>

<p>이렇게, C(Consistency)와 A(Availability)중에 반드시 하나를 선택해야 하는 상황이 만들어집니다.<br />
‘시스템의 방향’과, ‘제공하고자 하는 서비스의 형태’에 따라 적합한 전략을 취해야 합니다.</p>

<p><br /></p>

<h2 id="cap-theorem과-시스템-디자인-인터뷰">CAP Theorem과 시스템 디자인 인터뷰</h2>

<p>‘CAP Theorem’를 다루는건, 시스템 디자인 인터뷰에서 첫번째로 해야하는것 중 하나입니다.
<br />
시스템 디자인 인터뷰는, 두가지 핵심적인 요구사항을 정리하면서 시작합니다.</p>

<ol>
  <li>functional requirements(Feature)를 정리합니다.(반드시 달성해야 하는것들)</li>
  <li>non-functional requirements를 정의합니다.(system의 quality와 관련된것들)</li>
</ol>

<p>이때, ‘non-functional requirements’를 고려할때, CAP theorem이 ‘시작점’ 역할을 할 수 있습니다.</p>
<blockquote class="prompt-info">
  <p>이때, 스스로에게 다음의 질문을 하면 좋습니다.<br />
“이 시스템에서, Consistency와 Availability중에 어떤걸 우선시 해야 할까?”</p>
</blockquote>

<h3 id="consistency를-우선시-한다면">Consistency를 우선시 한다면..</h3>

<p>다음을 포함시켜, 시스템을 디자인하는게 좋습니다.</p>

<ul>
  <li>Distributed Transactions(분산 트랜잭션)<br />
여러개의 Data Source간에 강하게 Sync되도록 하려면, ‘<a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit protocol</a>‘을 사용해야 합니다.
    <blockquote class="prompt-info">
      <p><strong>Two-phase commit protocol(2PC, 2단계 커밋 프로토콜) :</strong><br />
분산 시스템에서, 여러 노드가 하나의 트랜잭션에 참여할 때, “모두 성공하거나, 모두 실패”해야 합니다. 이를 위해 2PC컨셉을 적용하여, <strong>트랜잭션을 두 단계로 나누어 처리하는것</strong>을 말합니다.</p>
    </blockquote>

    <p>이는 시스템의 복잡도를 높이지만, 모든 Node들에 걸친 Consistency를 보장합니다.<br />
이 경우, 유저들이 높은 Latency를 경험할 수 있습니다.(여러 Node에 대한 consistency를 위해 시간이 오래걸려서)</p>
  </li>
  <li>
    <p>Single-Node Solutions<br />
하나의 DB Instance를 사용하여, 장애가 전파되는 문제를 해결할 수 있습니다.<br />
이 경우, Scalability(확장성)을 제한하게 되지만, ‘Single source of Truth’가 되어 Consistency를 쉽게 확보할 수 있습니다.</p>
  </li>
  <li>Technology Choices는 다음과 같이..
    <ul>
      <li>PostgreSQL, MySQL과 같은 전통적인 RDBMS</li>
      <li>Goole Spanner</li>
      <li>Strong consistency mode를 기반으로한 DynamoDB</li>
    </ul>
  </li>
</ul>

<h3 id="availability를-우선시-한다면">Availability를 우선시 한다면…</h3>

<p>다음을 포함시켜, 시스템을 디자인하는게 좋습니다.</p>

<ul>
  <li>
    <p>Multiple Replicas<br />
여러 복제본을 만들어서, Replication환경을 만듭니다.(몇몇 Replica가 최신화 되지 않을 수 있는 환경을 허용)<br />
이는 Read에 대한 퍼포먼스와 가용성(Availability)에 대한 큰 개선을 제공해줍니다.</p>
  </li>
  <li>
    <p>Change Data Capture(CDC)<br />
Primary DB의 데이터가 바뀐다면, 이를 비동기적으로 Replica나 Cache혹은 다른 시스템에게 전달합니다.<br />
이를 통해 업데이트가 시스템에 반영되는 동안 기본 시스템을 계속 사용할 수 있습니다.</p>
  </li>
  <li>
    <p>Technology Choices는 다음과 같이…</p>
    <ul>
      <li>Cassandra</li>
      <li>여러 AZ에 걸친 Cluster형 DynamoDB</li>
      <li>Redis Cluster</li>
    </ul>
  </li>
</ul>

<blockquote class="prompt-info">
  <p>현대의 DB들은 ‘Consistency’와 ‘Availability’에 대한 옵션 모두 제공한다고 합니다.<br />
(Configuration을 통해, 둘중 하나를 선택할 수 있다는 뜻)</p>
</blockquote>

<p><br /></p>

<h2 id="consistency-level">Consistency Level</h2>

<p>CAP에서 Consistency는 ‘Strong Consistency’만 의미하는것은 아닙니다.<br />
Consistency에는 여러 Spectrum이 있으며, 이를 이해하는것은 시스템 디자인을 할때에 큰 도움을 줍니다.</p>

<h3 id="strong-consistency-강한-일관성">Strong Consistency (강한 일관성)</h3>

<p>데이터 쓰기(write) 직후에, 모든 읽기(read)에 그 값이 반영되어야 합니다.<br />
항상 최신값을 읽을 수 있지만, Consistency Model중에 가장 비용(컴퓨팅 리소스 관점)이 비쌉니다.<br />
주로 ‘은행 계좌의 잔액’에 사용됩니다.</p>

<p>예시 DB</p>
<ul>
  <li>Spanner (Google): TrueTime을 기반으로 strong consistency 보장합니다.</li>
  <li>etcd, ZooKeeper: 리더를 통해 순차적으로 처리합니다.</li>
  <li>MongoDB (readConcern: “linearizable”): 선택적으로 제공
<br /></li>
</ul>

<h3 id="causal-consistency-인과-일관성">Causal Consistency (인과 일관성)</h3>

<p>관련된 Event들이 <strong>모든 유저들에게 동일하게, 동일한 순서로 반영</strong>되는것을 말합니다.<br />
<strong>서로 의존성이 있는 Action들</strong>의 논리적 순서를 보장합니다.<br />
예를 들면, Post에 Comment를 단다면, Post가 먼저 존재해야합니다. ‘Causal Consistency’는 이 <strong>순서를 보장</strong>해 줍니다.</p>

<p>예시 DB:</p>
<ul>
  <li>Cassandra (with client-side tracking)</li>
  <li>Azure Cosmos DB (선택 가능)
<br /></li>
</ul>

<h3 id="sequential-consistency-순차-일관성">Sequential Consistency (순차 일관성)</h3>

<blockquote>
  <p>Sequential consistency is a consistency model used in the domain of concurrent computing (e.g. in distributed shared memory, distributed transactions, etc.).
- from <a href="https://en.wikipedia.org/wiki/Sequential_consistency">en.wikipedia.org</a></p>
</blockquote>

<p><strong>모든 연산이 일관된 순서로 적용</strong>됩니다.</p>
<blockquote class="prompt-info">
  <p>위의 ‘Causal Consistency’와 유사해 보이지만,<br />
‘Causal Consistency’가 Action간의 의존성이 있는경우에만 그 순서를 보장한다면,<br />
‘Sequential Consistency’은 의존성과 상관없이 모든 Action들의 순서에 대한 일관성을 보장합니다.</p>
</blockquote>

<p>대표적으로 ‘concurrent computing’에 쓰입니다.<br />
각 사용자가 보는 ‘연산 순서’는 동일하지만, 보는 시점에 따라 최신 값이 아닐 수도 있습니다.</p>

<p>예시 DB:</p>
<ul>
  <li>일부 메지 큐 시스템, 일부 분산 캐시</li>
</ul>

<h4 id="causal-consistency와-sequential-consistency-비교">‘Causal Consistency’와 ‘Sequential Consistency’ 비교</h4>

<table>
  <thead>
    <tr>
      <th><strong>항목</strong></th>
      <th><strong>Sequential Consistency</strong></th>
      <th><strong>Causal Consistency</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>순서 기준</strong></td>
      <td>모든 연산을 <strong>하나의 글로벌 순서</strong>로 정렬합니다.</td>
      <td>인과관계(causal relationship)만 보장합니다.</td>
    </tr>
    <tr>
      <td><strong>전체 순서 필요 여부</strong></td>
      <td>모든 연산 순서를 동일하게 유지합니다.</td>
      <td>인과관계가 있는 연산만 순서 보장합니다.</td>
    </tr>
    <tr>
      <td><strong>병렬 연산 간 순서</strong></td>
      <td>순서를 <strong>강제로 정합니다.</strong></td>
      <td><strong>자유롭게 재배열 가능</strong> (인과관계 없으면)</td>
    </tr>
    <tr>
      <td><strong>성능</strong></td>
      <td>더 느릴 수 있습니다.</td>
      <td>더 빠르고 병렬성이 높습니다.</td>
    </tr>
  </tbody>
</table>

<h3 id="read-your-own-writesryw-consistency">Read-your-own-writes(RYW) Consistency</h3>

<p>‘내가 쓴(write) 데이터’는 내가 읽을 때 항상(‘즉시’ 포함) 보이는 일관성을 말합니다. ‘다른 유저들은 Older version을 read할 수 있음’을 허용합니다.<br />
주로 소셜미디어에서 사용합니다.</p>

<p>예시 DB:</p>
<ul>
  <li>MongoDB (with session): 같은 세션 내에서 RYW 보장합니다.</li>
  <li>Firebase: 클라이언트 기반 동기화에서 자주 사용합니다.
<br /></li>
</ul>

<h3 id="eventual-consistency-최종-일관성">Eventual Consistency (최종 일관성)</h3>

<p><strong>시간이 지나면 언젠가</strong> 모든 노드가 일관된 상태에 도달하는 형태의 일관성입니다. ‘<strong>일시적으로</strong> 일관성이 깨지는것’을 허용합니다.<br />
<strong>가장 허용적인(느슨한) Consistency</strong>입니다.<br />
DNS사용 되며, 대부분의 Distributed Database에서의 Default 설정입니다.</p>

<p>예시 DB:</p>
<ul>
  <li>DynamoDB, Cassandra, Riak</li>
  <li>S3, DNS: 변경 직후에 전파가 늦어질 수 있음</li>
</ul>

<p><img src="/assets/img/for-post/CAP%20Theorem/image.png" alt="Eventual Consistency Pattern들" />
<em>Eventual Consistency Pattern들 | from <a href="https://bytebytego.com/guides/top-eventual-consistency-patterns-you-must-know/">bytebytego.com</a></em></p>

<h4 id="event-based-eventual-consistency">Event-based Eventual Consistency</h4>

<p>한 서비스(Service A)가 Event를 실행하면, 다른 서비스가 그 이벤트를 받아서 자기 자신의 시스템에 반영합니다.
<br /></p>

<h4 id="background-sync-eventual-consistency">Background Sync Eventual Consistency</h4>

<p>여기서는 별도의 Background Job(Cron같은)이 Database Node간에 데이터를 일치시켜, Consistency를 만들어 냅니다.<br />
스케쥴되어서 실행되기 때문에, 훨씬 느린 Consistency를 제공합니다.
<br /></p>

<h4 id="saga-based-eventual-consistency">Saga-based Eventual Consistency</h4>

<dl>
  <dt>‘Saga-based Eventual Consistency’는</dt>
  <dd>분산 트랜잭션을 처리하는 현대적인 방식으로, 특히 마이크로서비스 아키텍처에서 많이 사용됩니다.</dd>
  <dd>2PC의 한계(복잡성, 블로킹)를 극복하고, <strong>일관성보다 가용성과 확장성을 우선할 때 사용</strong>합니다.</dd>
  <dd>트랜잭션을 <strong>여러 개의 작은 지역 트랜잭션(local transaction)으로 쪼개고</strong>, 각 단계가 실패하면 보상 작업(compensation, 흔히 rollback과정이라고 불리는)을 수행하여 이전 상태로 되돌리는 방식입니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>Saga Pattern은 <strong>느슨한 결합을 추구</strong>합니다.<br />
이를 통해 서비스 간 의존성을 최소화하고, 서로 독립적으로 개발·배포·운영될 수 있도록 만드는 구조가 만들어집니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>‘Event-based Eventual Consistency’유사하지만, ‘Event-based Eventual Consistency’는 단위가 Event인 반면,<br />
‘Saga-based Eventual Consistency’는 ‘Transaction’단위로 작동합니다.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th><strong>항목</strong></th>
      <th><strong>Saga-based Eventual Consistency</strong></th>
      <th><strong>Event-based Eventual Consistency</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>핵심 개념</td>
      <td>트랜잭션 단위로 보상/실패 흐름을 정의</td>
      <td>이벤트를 기반으로 비동기적 상태 동기화</td>
    </tr>
    <tr>
      <td>구조</td>
      <td>보상 트랜잭션 정의 필수</td>
      <td>보상 없음 (대부분 리드모델 갱신)</td>
    </tr>
    <tr>
      <td>사용 목적</td>
      <td>분산 <strong>비즈니스 트랜잭션 처리</strong></td>
      <td>시스템 간 <strong>데이터 복제/동기화</strong></td>
    </tr>
    <tr>
      <td>대표 시나리오</td>
      <td>결제 실패 시 환불, 롤백 등</td>
      <td>주문이 생성되면 배송 시스템이 동기화</td>
    </tr>
  </tbody>
</table>

<h4 id="cqrscommand-query-responsibility-segregation-based-eventual-consistency">CQRS(Command Query Responsibility Segregation)-based Eventual Consistency</h4>

<p>읽기(Read, Query)와 쓰기(Write, Command) 작업에 대한 책임(Responsibility)을 각각의 DB로 분리하는 ‘Eventual Consistency’방식입니다.<br />
<br /></p>

<p>보통 아래와 같은 상황에서 사용됩니다.</p>
<ul>
  <li>읽기와 쓰기 성능 요구가 <strong>비대칭</strong>일 때.</li>
  <li>조회 요구사항이 <strong>복잡</strong>할 때.</li>
  <li><strong>확장성과 응답속도</strong>가 중요한 시스템일때.</li>
  <li>일관성보다 <strong>가용성과 유연성</strong>이 중요한 시스템일때.</li>
</ul>

<p><br /></p>

<h2 id="references">References</h2>

<dl>
  <dt>CAP Theorem | hellointerview.com</dt>
  <dd><a href="https://www.hellointerview.com/learn/system-design/deep-dives/cap-theorem">Hello Interview | System Design in a Hurry</a></dd>
  <dt>Spanner, TrueTime &amp; The CAP Theorem | static.googleusercontent.com</dt>
  <dd><a href="https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45855.pdf">static.googleusercontent.com</a></dd>
  <dt>Towards robust distributed systems (abstract) | dl.acm.org</dt>
  <dd><a href="https://dl.acm.org/doi/10.1145/343477.343502">Towards robust distributed systems (abstract) | Proceedings of the nineteenth annual ACM symposium on Principles of distributed computing</a></dd>
  <dt>Top Eventual Consistency Patterns You Must Know | bytebytego.com</dt>
  <dd><a href="https://bytebytego.com/guides/top-eventual-consistency-patterns-you-must-know/">ByteByteGo | Top Eventual Consistency Patterns You Must Know</a></dd>
  <dt>CAP, PACELC, ACID, BASE - Essential Concepts for an Architect’s Toolkit | blog.bytebytego.com</dt>
  <dd><a href="https://blog.bytebytego.com/p/cap-pacelc-acid-base-essential-concepts">CAP, PACELC, ACID, BASE - Essential Concepts for an Architect’s Toolkit</a></dd>
  <dt>Two-phase commit protocol | en.wikipedia.org</dt>
  <dd><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">Two-phase commit protocol</a></dd>
  <dt>Engineering Trade-offs: Eventual Consistency in Practice | blog.bytebytego.com</dt>
  <dd><a href="https://blog.bytebytego.com/p/a-guide-to-eventual-consistency-in">Engineering Trade-offs: Eventual Consistency in Practice</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="System Design Interview" /><category term="Core Concepts" /><category term="System Design" /><category term="interview" /><category term="Computer Science" /><category term="CAP Theorem" /><category term="SAGA Pattern" /><summary type="html"><![CDATA[CAP Theorem 소개]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/CAP%20Theorem/cap-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/CAP%20Theorem/cap-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is Kafka? | CloudNative</title><link href="https://blog.devpour.net/posts/What-is-Kafka/" rel="alternate" type="text/html" title="What is Kafka? | CloudNative" /><published>2025-07-23T14:09:00+09:00</published><updated>2025-07-23T14:09:00+09:00</updated><id>https://blog.devpour.net/posts/What%20is%20Kafka</id><content type="html" xml:base="https://blog.devpour.net/posts/What-is-Kafka/"><![CDATA[<h2 id="kafka-소개">Kafka 소개</h2>

<dl>
  <dt>Kafka는</dt>
  <dd>대용량의 실시간 데이터 스트리밍을 처리하는 <strong>분산(Distributed) 메시징 시스템</strong>입니다.</dd>
  <dd>원래 <strong>LinkedIn</strong>에서 개발되었고, 이후 <strong>Apache Software Foundation</strong>에서 오픈소스로 관리되고 있습니다.</dd>
</dl>

<h3 id="탄생-배경linkedin에서">탄생 배경(LinkedIn에서)</h3>

<ul>
  <li>
    <p>로그와 사용자 이벤트 데이터 폭증<br />
‘LinkedIn’에서는 Web에서 사용자들의 행동을 추적(tracking)하고 있었는데, 사용자가 급격하게 늘어남에 따라, 데이터가 폭발적으로 증가하였습니다.<br />
수천만 명의 사용자가 활동하면서 광고 클릭, 검색, 프로필 조회, 추천 요청 등의 이벤트가 초당 수십만 건씩 발생하였으며, 기존의 로그 수집 시스템(파일 로그 → 수집기 → Hadoop ETL)은 이를 감당하지 못했습니다.</p>
  </li>
  <li>
    <p>실시간 분석 불가능<br />
모든 데이터는 Hadoop에 쌓인 후 <strong>배치 처리</strong>해야 했습니다.(즉, 실시간 분석이 불가능.)<br />
실시간 분석은 거의 불가능하고, 결과를 보려면 <strong>수 시간 또는 하루 이상 지연</strong>되었습니다.</p>
  </li>
  <li>
    <p>AMQP(Advanced Message Queuing Protocol)의 한계<br />
LinkedIn에서 처음에는 AMQP서비스들을 고려했다고 합니다. 하지만, AMQP로 분산처리 환경을 구현하는데에 여러 어려움이 있었습니다.</p>
  </li>
</ul>

<p>이를 극복하기 위해, 기존에 있던 Messaging System을 고려했지만, 적합한게 없어, Kafka를 만들게 되었습니다.</p>

<h3 id="kafka의-초기-설계-철학">Kafka의 초기 설계 철학</h3>

<p>Kafka는 단순한 메시지 큐가 아니라, <strong>범용 로그 시스템(log-centric system)</strong>으로 설계되었습니다.<br />
핵심 철학은 다음과 같습니다.</p>

<h4 id="모든-데이터를-이벤트-로그로-저장합니다">모든 데이터를 이벤트 로그로 저장합니다.</h4>
<p>이벤트 중심(Event-driven) 아키텍처를 채택하였습니다.</p>

<blockquote class="prompt-info">
  <p>모든 시스템 간 통신과 상태 변화를 이벤트 로그로 기록·전파하여, 비동기·분산·확장 가능한 구조를 구현했다는 의미입니다.</p>
</blockquote>

<p>모든 시스템 간 통신도 <strong>로그 기반 이벤트 스트림</strong>으로 표현합니다.</p>

<h4 id="고성능--고내구성">고성능 &amp; 고내구성</h4>

<p><strong>디스크에 바로 쓰되, 디스크 I/O를 최적화</strong>하여 메모리 버퍼처럼 빠르게 동작합니다.<br />
메시지를 디스크에 저장하되 복제(replication)로 데이터 유실 방지합니다.</p>

<h4 id="단순하고-확장-가능한-api">단순하고 확장 가능한 API</h4>

<p>Producer, Consumer 모두 단순한 API로 통신합니다.<br />
파티셔닝(partitioning)을 통해 <strong>수평 확장(Horizontal Scaling)</strong>이 쉽습니다.</p>

<h4 id="리플레이-가능한-스트림-처리">리플레이 가능한 스트림 처리</h4>

<p>데이터는 소비 후에도 삭제되지 않으며, <strong>필요할 때 다시 읽을 수 있습니다.</strong><br />
이 덕분에 <strong>재처리, 재분석, 에러 복구</strong>가 쉬워집니다.</p>

<h4 id="다양한-시스템-연결을-위한-허브">다양한 시스템 연결을 위한 허브</h4>

<p>Kafka를 중심으로 Hadoop, HDFS, Cassandra, Storm 등과 연계하곤 합니다.<br />
즉, Kafka는 <strong>데이터의 허브</strong>가 됩니다.</p>

<p><br /></p>

<h2 id="kafka-architecture">Kafka Architecture</h2>
<p>Kafka는 여러 Broker를 통해 Cluster를 생성하여, 분산처리를 위한 Architecture를 갖추고 있습니다.</p>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image.png" alt="Kafka Cluster Architecture" />
<em>Kafka Cluster Architecture | from <a href="https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/">ibm-cloud-architecture.github.io</a></em></p>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%201.png" alt="Kafka Architecture with Offset" />
<em>Kafka Architecture with Offset | from <a href="https://d3s.mff.cuni.cz/files/teaching/nswi080/lectures/notes/ch02s11.html">d3s.mff.cuni.cz</a></em></p>

<h3 id="components">Components</h3>

<h4 id="brokers">Brokers</h4>

<p>Kafka는 여러 Broker로 이루어진 Cluster로 운영됩니다. 즉, Broker는 Kafka 서버 인스턴스 하나하나를 의미합니다.</p>

<dl>
  <dt>이 Broker들은</dt>
  <dd>데이터의 Replication을 관리하고,</dd>
  <dd>Topic/partition을 관리하고,</dd>
  <dd>Broker안에 있는 Partition의 Offset을 관리합니다.</dd>
</dl>

<p>Broker에는 Topic(실제로 Record Stream이 저장되는 곳)의 Partition이 저장됩니다.</p>

<blockquote class="prompt-info">
  <p>만약, 여러 데이터센터를 걸쳐서 Kafka를 구성한다면, 센터간 15ms이하의 network latency를 요구합니다.<br />
이는 Kafka Broker와 zookeeper사이에 많은 통신이 있기때문입니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>Production level에선, 최소한 5개의 노드(Broker)를 구성하는게 좋다고 합니다.</p>
</blockquote>

<h4 id="zookeeper최신버전에서는-kraft-controller-사용">Zookeeper(최신버전에서는 KRaft Controller 사용)</h4>

<dl>
  <dt>‘Zookeeper’는</dt>
  <dd>Component나 Kafka의 상태를 유지하는데 사용됩니다.</dd>
  <dd>HA(High Availability)를 위해, Cluster형태로 실행됩니다.</dd>
  <dd>클러스터의 메타데이터(파티션 배치, ISR, 브로커 생·사 등) 관리합니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>Kafka Version에 따라, zookeeper에서 offset을 관리합니다.<br />
최신 버젼에서는 Kafka내부에서 ‘consumer offset’이라는 이름으로 관리합니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>Zookeeper는 Kafka 3.5부터 deprecated처리 되며, 4.0부터는 완전히 제거됩니다.</p>
</blockquote>

<h4 id="kraft-controller40이상부터-default">KRaft Controller(4.0이상부터 default)</h4>

<dl>
  <dt>KRaft Controller는</dt>
  <dd>Zookeeper의 역할을 대체하면서도, Kafka 외부(Zookeeper)의 도움 없이 Kafka 스스로 클러스터가 운영될 수 있도록 합니다.</dd>
  <dd>Kafka의 아키텍처를 단순화하고, 운영 부담을 감소시켜 줍니다.(ZooKeeper에 대한 관리가 불필요)</dd>
</dl>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%202.png" alt="ZooKeeper mode vs. KRaft mode" />
<em>ZooKeeper mode vs. KRaft mode</em></p>

<p>Kafka 3.3부터 KRaft가 새 클러스터에 “프로덕션 레디”로 승인되고, 3.5에서 Zookeeper 모드가 deprecated, 4.0에서 제거될 예정입니다.(현재시점 4.0.0 출시)</p>

<blockquote class="prompt-info">
  <p>‘Raft’는 뗏목을 의미하며, Cluster에서 Leader를 선출하는 합의 알고리즘중 하나입니다.</p>
</blockquote>

<h4 id="topics과-partitions-replication">Topics과 Partitions, Replication</h4>

<dl>
  <dt>‘Topic’은</dt>
  <dd>메시지 논리 채널입니다.</dd>
  <dd>Record를 Publish하고, Consume하는 엔드포인트를 제공합니다.</dd>
  <dd>하나의 Topic을 N개의 Partition으로 나누어, 각 Partition은 독립적인 append-only(추가만 되는) 로그로 관리합니다.</dd>
</dl>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%203.png" alt="하나의 Topic은 여러개의 Partition과 Replicas로 구성되어 있습니다." />
<em>하나의 Topic은 여러개의 Partition과 Replicas로 구성되어 있습니다.</em></p>

<dl>
  <dt>Partition은</dt>
  <dd>단일 서버가 모든 이벤트를 처리할 수 없을 때, Broker Clustering을 사용하여 이벤트 처리를 병렬화하는 데에 사용됩니다.</dd>
  <dd>Topic을 병렬 처리 단위로 분할하는 개념이며, Broker에 분산 배치되어 쓰기·읽기 부하 분산 구현합니다.</dd>
  <dd>Consumer나 ‘Traffic pattern’에 따라 파티션의 갯수를 조절할 수 있으며, 각 Broker는 2000개의 파티션을 가질 수 있습니다.</dd>
  <dd><strong>append-only Log파일로 구현</strong>되어 있으며(<strong>Disk에 저장</strong>), 오래된 Record는 정해진 시간이나, 파일 limit에 도달하면 지워집니다.</dd>
</dl>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%204.png" alt="Kafka의 Topic은 여러 Partition으로 이루어져 있습니다." />
<em>Kafka의 Topic은 여러 Partition으로 이루어져 있습니다.</em></p>

<dl>
  <dt>Replication은</dt>
  <dd>Partition이 여러 Broker(서버)에 걸쳐 복제된 ‘복제본’을 말합니다.</dd>
  <dd>각 Partition은 Leader와 하나 이상의 Follower Replica를 갖고 있어, 장애 시에도 데이터 가용성·내구성 유지합니다.</dd>
  <dd><strong>Leader가 모든 Read/Write 요청을 다루고</strong>, Follower는 Leader의 컨텐츠를 복제합니다.</dd>
</dl>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%205.png" alt="Topic의 Partition과 Replication" />
<em>Topic의 Partition과 Replication</em></p>

<h4 id="producer">Producer</h4>

<blockquote>
  <p>A producer is a thread safe kafka client API that publishes records to the cluster.<br />
- from <a href="https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers/">ibm-cloud-architecture.github.io</a></p>
</blockquote>

<dl>
  <dt>‘Producer’는</dt>
  <dd>Kafka에 메세지를 등록(publish)하는 Client입니다.</dd>
  <dd>초기 연결(initial bootstrap connection)후에, 토픽(파티션)과 연결할 Leader Broker에 대한 메타데이터를 얻습니다.</dd>
  <dt>메세지를 파티션에 할당할때,</dt>
  <dd>Key가 지정되지 않은경우, Round-robin을 사용합니다.</dd>
  <dd>Key가 정해져 있다면, Key의 Hash값으로 partition이 정해집니다.</dd>
  <dd>혹은 이 과정을 Custom할 수 있습니다.</dd>
</dl>

<h4 id="consumer와-consumer-group">Consumer와 Consumer Group</h4>

<dl>
  <dt>Consumer는</dt>
  <dd>특정 Topic의 Partition에서 Record를 Pull방식으로 가져와서 처리하는 컴포넌트입니다.</dd>
  <dd>일반적으로 poll() 루프를 돌며 일정량씩 읽고(fetch), 처리 후 offset을 커밋(commit)합니다.</dd>
  <dt>Consumer Group은</dt>
  <dd>동일한 <code class="language-plaintext highlighter-rouge">group.id</code> 를 가진, Consumer들의 집합을 ‘Consumer Group’이라고 합니다.</dd>
  <dd>하나의 Topic 파티션을 그룹 내에서 단 1개의 Consumer만 소비하도록 해서 작업을 분산 합니다(Queue semantics).</dd>
  <dd>각 partition마다 최소 1개의 Consumer가 있어야 하며, Consumer가 Group에 하나만 있다면, 모든 partition의 데이터를 다룹니다.</dd>
</dl>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%206.png" alt="image.png" />
<em>Consumer와 Consumer Group의 관계</em></p>

<h4 id="message-구조">Message 구조</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%207.png" alt="Kafka Message Anatomy" />
<em>Kafka Message Anatomy | <a href="https://www.geeksforgeeks.org/java/how-kafka-producers-message-keys-message-format-and-serializers-work-in-apache-kafka/">geeksforgeeks.org</a></em></p>

<ul>
  <li>
    <p>Message의 Key는 nullable입니다.<br />
key가 null이라면, Round-robin을 통해 random하게 Partition에 할당됩니다.</p>
  </li>
  <li>Compression Type을 통해, Message의 합축을 표현할 수 있습니다.</li>
  <li>Header는 key/value 형식의 Metadata를 저장합니다.</li>
  <li>Partition은 전송 대상을 의미합니다.(어떤 partition에 저장될 지)</li>
  <li>Offset은 Broker가 메세지를 partition에 저장할때 부여되며, Consumer가 읽을 때 확인합니다.</li>
  <li>Timestamp는 이벤트의 생성시간 혹은 Log에 추가된 시간을 의미합니다.</li>
</ul>

<h4 id="avro">Avro</h4>

<p>Avro는 메시지를 직렬화(serialize)할 때 쓰는 포맷/스키마 언어입니다.(Protobuf와 유사)</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="err">//</span><span class="w"> </span><span class="err">user_event.avsc</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"record"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"UserEvent"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"namespace"</span><span class="p">:</span><span class="w"> </span><span class="s2">"com.example"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"fields"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w"> </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"id"</span><span class="p">,</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w"> </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"email"</span><span class="p">,</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w"> </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"age"</span><span class="p">,</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"null"</span><span class="p">,</span><span class="w"> </span><span class="s2">"int"</span><span class="p">],</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w"> </span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>데이터의 Schema를 JSON형식으로 입력하여 사용합니다. 이를 기반으로 데이터를 Binary로 변환하여 전송합니다.<br />
Schema Registry(Confluent, Apicurio)를 통해, Schema를 등록해서 재사용 하곤 합니다.</p>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%208.png" alt="Schema Registry의 사용 Flow" />
<em>Schema Registry의 사용 Flow | <a href="https://docs.confluent.io/platform/current/schema-registry/index.html">docs.confluent.io</a></em></p>

<p><br /></p>

<h2 id="kafka의-기능과-구현">Kafka의 기능과 구현</h2>

<h3 id="producer의-exactly-once-delivery-exactly-once-semantics-eos">Producer의 Exactly-Once Delivery(= Exactly-Once Semantics, EOS)</h3>

<dl>
  <dt>‘Exactly-Once Delivery’는</dt>
  <dd>“재시도·네트워크 오류가 있어도 Kafka 로그에 동일 레코드가 단 한 번만 쓰이고, 다운스트림(다른 토픽/컨슈머)에도 중복 없이 한 번만 보이게 하는 것”을 목표로 합니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>분산 시스템에서 정확히 한 번만 Delivery하는 것이, 가장 해결하기 어려운 문제 중 하나입니다.</p>
</blockquote>

<p>이를 위해 2가지 기능이 결합되어 있습니다.</p>

<h4 id="멱등성idempotent을-제공하는-producer">멱등성(Idempotent)을 제공하는 Producer</h4>
<p>Producer에서 메세지의 멱등성(Idempotent)을 위한 기능을 제공합니다.<br />
Producer가 ProducerId(PID) 와 시퀀스 번호를 붙여 전송합니다. → 브로커가 중복 쓰기 감지 후 무시하도록 처리합니다.<br />
같은 파티션에 대한 중복 기록 방지(“딱 한 번 쓰기”)는 보장할 수 있습니다</p>

<h4 id="트랜잭셔널transactional-producer">트랜잭셔널(Transactional) Producer</h4>
<p>여러 파티션·토픽에 걸친 쓰기와 Consumer 오프셋 커밋까지 하나의 트랜잭션으로 묶어 정확히 한 번 처리를 보장합니다.
<br /></p>

<h3 id="broker의-offset-management">Broker의 Offset Management</h3>

<p>“브로커가 오프셋을 관리한다”는 말은 보통 두 가지 Layer를 포함합니다.</p>

<ol>
  <li><strong>메시지 자체의 오프셋(Log Offset)</strong>을 브로커가 붙입니다.</li>
  <li><strong>“Consumer가 어디까지 읽었는지”</strong>를 브로커가 저장해 줍니다(Committed Offset)</li>
</ol>

<h4 id="log-offset">Log Offset</h4>

<p><strong>메시지 자체의 오프셋(Log Offset)</strong>을 브로커가 붙입니다.<br />
각 파티션은 <strong>append-only 로그 파일</strong>이고, 브로커가 새 레코드가 들어올 때마다 0,1,2…처럼 증가하는 번호(Offset)를 붙여 저장합니다.<br />
그래서 <strong>파티션 안에서만</strong> 순서가 보장되고, Consumer는 “나는 offset=123부터 읽을래”처럼 요청할 수 있습니다.</p>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%209.png" alt="Distributed Log와 Offset" />
<em>Distributed Log와 Offset | <a href="https://oso.sh/blog/apache-kafka-101/">oso.sh</a></em></p>

<h4 id="consumer-group과-committed-offset">Consumer group과 Committed Offset</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2010.png" alt="Consumer Group과 Offset의 관계" />
<em>Consumer Group과 Offset의 관계</em></p>

<p><strong>“Consumer가 어디까지 읽었는지”</strong>를 브로커가 저장해 줍니다. (Committed Offset)<br />
Consumer Group은 처리 완료한 위치(offset)를 <strong>커밋(commit)</strong>합니다.<br />
이 커밋 정보는 브로커 내부의 <strong>__consumer_offsets라는 내부(compacted) 토픽</strong>에 저장됩니다.</p>
<ul>
  <li>키: (group, topic, partition)</li>
  <li>값: committed offset + 메타데이터<br />
이렇게 해두면 Consumer가 재시작하거나 다른 인스턴스로 넘어가도 <strong>“마지막으로 읽은 지점”을 브로커에서 다시 가져올 수</strong> 있습니다.</li>
</ul>

<blockquote class="prompt-info">
  <p><strong>메세지 순서 보장</strong><br />
같은 Partition 내에서는 메시지 순서(order)가 보장되지만, Partition 간 순서는 보장되지 않습니다.</p>
</blockquote>

<p><br /></p>

<h3 id="consumer-rebalancing">Consumer Rebalancing</h3>

<dl>
  <dt>‘Consumer Rebalancing’은</dt>
  <dd>같은 Consumer Group 안에서 어떤 Consumer가 어떤 파티션을 읽을지 다시 배분하는 과정을 말합니다.</dd>
  <dd>새 Consumer가 추가/제거되거나, 파티션 수가 변할 때, “공평하게 나눠 읽도록” 그룹 전체가 잠깐 멈추고 재조정(Rebalancing)합니다.</dd>
</dl>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2011.png" alt="Stop-the-world rebalancing" />
<em>Stop-the-world rebalancing | <a href="https://www.redpanda.com/guides/kafka-performance-kafka-rebalancing">redpanda.com</a></em></p>

<h4 id="rebalancing-발생-환경">Rebalancing 발생 환경</h4>

<ul>
  <li>
    <p><strong>그룹 구성에 변화가 있을때.</strong><br />
새 Consumer 추가되거나, 기존 Consumer가 <strong>leave</strong> 또는 <strong>죽음(heartbeat 끊김)</strong> 상태일때.</p>
  </li>
  <li><strong>토픽 파티션 수가 변경될때.</strong></li>
  <li><strong>Group Coordinator가 교체되거나 장애가 발생했을때.</strong></li>
</ul>

<h4 id="rebalancing-과정eager-방식">Rebalancing 과정(Eager 방식)</h4>

<ol>
  <li>
    <p>JoinGroup 요청<br />
모든 Consumer가 Coordinator(그룹을 관리하는 브로커)에게 “나 여기 있어요”라고 알립니다.</p>
  </li>
  <li>
    <p>리더 선출 &amp; 파티션 할당 계산<br />
Coordinator가 <em>그룹 내 리더 Consumer</em>를 하나 선출합니다.<br />
리더가 partition.assignment.strategy(Range, RoundRobin, Sticky 등)에 따라 ‘파티션 분배’ 세부 내용을 계산합니다.</p>
  </li>
  <li>
    <p>SyncGroup<br />
리더가 계산 결과를 Coordinator에 전달합니다.<br />
Coordinator가 모든 Consumer에게 “너는 이 파티션들 담당” 통보합니다.</p>
  </li>
  <li>
    <p>재시작<br />
각 Consumer는 자신에게 배정된 파티션을 다시 poll()하기 시작합니다.</p>
  </li>
</ol>

<blockquote class="prompt-info">
  <p>Eager방식에서는, 모든 Consumer가 한 번에 파티션을 반납했다가 다시 받기 때문에, ‘Stop-the-world’ 구간이 크다는 단점이 있습니다.</p>
</blockquote>

<h4 id="side-effects-of-kafka-rebalancing">Side effects of Kafka rebalancing</h4>

<p>Kafka Rebalancing과정으로 인해, 예기지 않은 side effect가 발생할 수 있습니다.</p>
<ul>
  <li>
    <p>Latency 증가(Increased latency)<br />
대량 이벤트를 처리하고 있다면, 잦은 리밸런스로 처리가 지연될 수 있습니다.</p>
  </li>
  <li>
    <p>처리량 감소(Reduced Throughput)<br />
poll()이 멈춰 Consumer Lag 증가할 수 있습니다.</p>
  </li>
  <li>컴퓨팅 리소스 사용량 증가(Increased resource usage)</li>
  <li>중복 처리 가능성<br />
재시작 전에 커밋 못한 레코드는 재처리될 수 있음 (At-least-once)</li>
</ul>

<h4 id="cooperativeincremental-rebalancing">Cooperative(Incremental) Rebalancing</h4>

<dl>
  <dt>‘Cooperative(Incremental) Rebalancing’은</dt>
  <dd>Kafka Consumer Group에서 필요한 파티션만 “부분적으로” 재할당해, 기존의 “모든 파티션을 일단 반납(Eager)” 방식이 만들던 Stop-the-world(중단 시간) 문제를 줄이려는 메커니즘입니다.</dd>
  <dd>Kafka 2.4+ 클라이언트부터 도입되었습니다.</dd>
</dl>

<p><br /></p>

<h3 id="consumer-group과-partition-구성-best-practice">Consumer Group과 Partition 구성 Best Practice</h3>

<ul>
  <li>
    <p><strong>파티션 수 ≥ Consumer 수</strong>로 구성해야 합니다<br />
Consumer가 파티션보다 많으면 일부는 놀게되는 상황이 만들어집니다.</p>
  </li>
  <li>
    <p><strong>수동 커밋 + 에러 처리 전략 수립합니다.</strong><br />
처리가 끝난 후 커밋(At-least-once, 최소한 1회)하도록 설계하고,<br />
재처리를 허용하도록 설계하는게 좋습니다(Idempotency, 멱등성 확보).</p>
  </li>
  <li>
    <p><strong>리밸런스 상황을 최소화 합니다.</strong><br />
긴 처리 작업은 별도 쓰레드/큐로 넘기고 poll 주기를 짧게 유지합니다.<br />
최신 클라이언트를 통해, Cooperative rebalancing 사용하여, Balancing자체를 효율화 합니다.</p>
  </li>
  <li>
    <p><strong>Lag를 모니터링합니다</strong><br />
Latency가 커지면 스케일 아웃 또는 처리 로직 최적화가 필요합니다.</p>
  </li>
  <li>
    <p><strong>여러 Consumer Group으로 팬아웃(fanout, 입력 확장)합니다.</strong><br />
서로 다른 용도(실시간 분석, ETL, 알림 등)는 <strong>각기 다른 Consumer Group(다른 group.id)</strong>로 독립 소비하도록 구성합니다.</p>
  </li>
</ul>

<p><br /></p>

<h3 id="producer-partition-strategies">Producer partition strategies</h3>

<p>Producer가 Partition에 메세지를 할당하는 Strategy(전략)에 대해서 알아봅니다.</p>

<h4 id="키-기반-해싱-keyed-partitioningdefault-strategy">키 기반 해싱 (Keyed Partitioning)(default strategy)</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2012.png" alt="Hash로 작동하는 Default Strategy." />
<em>Hash로 작동하는 Default Strategy.</em></p>

<p><strong>키가 null이 아닐 때</strong>의 기본 전략입니다.<br />
<strong>같은 키는 항상 같은 파티션에 할당됩니다.</strong> → 키 단위 순서 보장해서, 로컬 캐시/상태 활용에 유리합니다.</p>

<h4 id="round-robin-partition-strategy">Round-Robin Partition Strategy</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2013.png" alt="Round-Robin으로 Random하게 분배." />
<em>Round-Robin으로 Random하게 분배.</em></p>

<p>이 전략은 메시지 내용에 관계없이 메시지를 파티션에 순환적으로 할당합니다.<br />
모든 파티션에 메시지가 균등하게 분배되도록 보장하지만, 관련 메시지가 서로 다른 파티션에 배치될 수 있으므로 순서대로 처리된다는 보장은 없습니다.</p>

<h4 id="sticky-partitioning-키가-null일-때-기본">Sticky Partitioning (키가 null일 때 기본)</h4>

<p><strong>키가 null이면 “한 파티션을 당분간 고정(sticky)”</strong>해서 배치를 크게 묶어 성능을 향상시켰습니다.<br />
배치를 flush 하거나 파티션 추가/에러 발생 시 다른 파티션을 새로 선택합니다.</p>

<h4 id="uniform-sticky-partition-strategy">Uniform Sticky Partition Strategy</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2014.png" alt="Uniform Sticky Partition Strategy" />
<em>Uniform Sticky Partition Strategy</em></p>

<p>Sticky의 장점(큰 배치 유지)을 살리면서 <strong>토픽 전체 파티션에 더 균일하게 분배</strong>하는 전략입니다.<br />
특히 <strong>많은 토픽/파티션</strong>을 동시에 다룰 때 분포 불균형을 줄여줍니다.
<br /></p>

<h3 id="consumer-assignment-strategies">Consumer assignment strategies</h3>
<p>같은 <strong>Consumer Group 안에서 파티션을 어떤 규칙으로 나눠 가질지</strong> 결정하는 알고리즘입니다.<br />
Rebalancing 때, partition.assignment.strategy에 지정된 클래스가 실행돼 Partition→Consumer 매핑을 계산합니다.</p>

<h4 id="range-assignor-default">Range assignor (default)</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2015.png" alt="Range assignor" />
<em>Range assignor | from <a href="https://developer.confluent.io/courses/architecture/consumer-group-protocol/">developer.confluent.io</a></em></p>

<p>토픽별로 파티션을 정렬 후 컨슈머 수로 나눠 “연속 구간(range)” 배정합니다.<br />
즉, 단순하게 파티션을 순서대로 컨슈머에 분배하되, 파티션과 컨슈머가 짝수로 나누어떨어지지 않으면, 앞부분의 컨슈머들에 파티션을 좀 더 가져갑니디.(Round Robin과는 여기서 차이가 있음)</p>

<h4 id="round-robin-assignor">Round-robin assignor</h4>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2016.png" alt="Round-robin Assignor" />
<em>Round-robin Assignor | from <a href="https://developer.confluent.io/courses/architecture/consumer-group-protocol/">developer.confluent.io</a></em></p>

<p>모든 토픽의 파티션을 하나의 리스트로 붙여 라운드로빈을 돌립니다.(토픽 구분 없이 모든 파티션을 균등 분배.)<br />
사용되는 Consumer 수를 극대화하는 것을 목표로 할때 사용합니다.<br />
하지만, Rebalancing시에, 전체 Partition에 대해 재배치가 이루어저야 하는 문제(Eager Rebalance)가 있습니다. 즉, 일부분의 Partition만 조정하지 못하는 문제가 있습니다.</p>

<h4 id="sticky-assignor">Sticky assignor</h4>

<p>‘Round-robin’과 유사하지만, Rebalancing 할 때에 일부의 Partition만 재배치됩니다.</p>

<h4 id="cooperative-sticky-assignor권장">Cooperative Sticky assignor(권장)</h4>
<p>Sticky기반에, ‘필요 파티션만 천천히 이동’하는 assignor입니다.</p>

<blockquote class="prompt-info">
  <p>Eager(전통적)인 assignore는 컨슈머 목록에 변화가 생기면, 모든 Consumer를 반납하고 다시 할당 받습니다.
이 과정에서 ‘Stop-the-world’가 발생하여, Lag 급증, 중복 처리 위험이 발생하게 됩니다.</p>
</blockquote>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2017.png" alt="Avoid Pause with CooperativeStickyAssignor Step 1" />
<em>Avoid Pause with CooperativeStickyAssignor Step 1</em></p>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2018.png" alt="Avoid Pause with CooperativeStickyAssignor Step 2" />
<em>Avoid Pause with CooperativeStickyAssignor Step 2</em></p>

<p>두 단계를 거쳐, “필요한 파티션만” 이동시켜 Stop-the-world를 피합니다.</p>

<h3 id="kafka-message의-forwardbackward-compatibility-필요성">Kafka Message의 Forward/Backward Compatibility 필요성</h3>

<p>System이 성장하면서, Kafka의 Message도 계속 성장합니다.<br />
Kafka에서는 <strong>Avro를 통해, Schema의 진화(혹은 성장, Schema Evolution)를 가능하게 합니다.</strong><br />
여기서는 Avro를 이용하여, Message의 ‘Forward/Backward Compatibility’가 왜 필요한지 알아봅니다.</p>

<h4 id="kafka는-장기-보존--리플레이reprocess가-가능합니다">Kafka는 장기 보존 &amp; 리플레이(Reprocess)가 가능합니다.</h4>
<p>Kafka는 메시지를 며칠~몇 달 이상 보존하고, 언제든 <strong>과거 데이터를 다시 읽어 처리</strong>합니다.<br />
시간이 흐르면서 <strong>메시지 구조가 바뀌어도</strong> 과거 이벤트를 해석할 수 있어야 합니다.<br />
Avro는 <strong>작성 시점(writer) 스키마</strong>와 <strong>읽기 시점(reader) 스키마</strong>를 맞추는 규칙으로, 과거 데이터도 안전하게 디코딩할 수 있게 합니다.</p>

<h4 id="producerconsumer의-독립-배포가-이루어집니다">Producer/Consumer의 독립 배포가 이루어집니다</h4>

<p>마이크로서비스 환경에서 Producer와 Consumer는 <strong>동시에 배포되지 않습니다.</strong></p>

<dl>
  <dt>Producer가 필드를 추가/변경했는데 만약, Consumer가 구버전이라면..?</dt>
  <dd>JSON처럼 암묵적인 약속에만 의존하면 쉽게, 시스템이 깨질 수 있습니다.</dd>
  <dd>Avro + Schema Registry는 <strong>호환성 모드</strong>(BACKWARD/FORWARD/FULL 등)를 통해 “새 스키마 등록” 자체를 통제하고, 깨지는 변경을 차단합니다.</dd>
</dl>

<h4 id="다양한-consumeranalytics-etl-alert-등에-대응해야-합니다">다양한 Consumer(Analytics, ETL, Alert 등)에 대응해야 합니다.</h4>

<p>하나의 토픽을 <strong>여러 Consumer Group</strong>이 서로 다른 목적/언어/프레임워크로 읽는 환경입니다.<br />
모든 Consumer가 <strong>정확한 필드 타입, 기본값, null 허용 여부</strong>를 알아야만 안정적으로 처리할 수 있습니다.</p>

<p><br /></p>

<h2 id="kafka와-rabbitmq전통적인-message-queue의-차이">Kafka와 RabbitMQ(전통적인 Message Queue)의 차이</h2>

<p>Kafka와 RabbitMQ는 그 사용 목적과 디자인된 방향성에 차이가 있습니다.</p>

<p><strong>메시지 전송 방식</strong></p>
<ul>
  <li><strong>RabbitMQ</strong>: 메시지 큐 방식으로 메시지를 큐에 저장한 후, 소비자가 가져가면 메시지가 삭제됩니다. <strong>메시지의 지속성</strong>보다는 즉각적인 전달이 중요할 때 적합합니다.</li>
  <li><strong>Kafka</strong>: 로그 스트림 방식으로, 메시지가 브로커에 쓰여지면 <strong>로그처럼 유지</strong>됩니다. 소비자는 오프셋을 기반으로 메시지를 읽기 때문에 여러 소비자가 같은 메시지를 반복적으로 읽을 수 있으며, 메시지가 삭제되지 않고 설정된 기간 동안 저장됩니다.</li>
</ul>

<p><strong>데이터 영속성 및 내구성</strong></p>
<ul>
  <li><strong>RabbitMQ</strong>: 메시지는 큐에 있고 기본적으로 소비 후 사라지므로 일시적인 데이터 전송에 적합합니다. 영구 큐 설정을 통해 메시지 영속성을 유지할 수 있지만, Kafka와 같은 로그 저장 방식과는 다릅니다.</li>
  <li><strong>Kafka</strong>: 기본적으로 데이터가 로그로 유지되기 때문에 저장 기간을 설정하지 않는 한 삭제되지 않습니다. 따라서 <strong>데이터의 내구성이 높고</strong> 이벤트의 순차적 흐름을 유지하기 위해 적합합니다.</li>
</ul>

<p><strong>성능 및 처리량</strong></p>
<ul>
  <li><strong>RabbitMQ</strong>: 낮은 지연 시간과 빠른 전송 속도를 제공하여, <strong>단일 메시지의 빠른 처리</strong>가 필요한 경우 유리합니다. 하지만 대용량의 데이터 스트리밍이나 로그 수집에는 성능이 한계에 도달할 수 있습니다.</li>
  <li><strong>Kafka</strong>: 초당 수백 MB의 데이터를 처리할 수 있는 고성능 스트리밍 시스템으로, <strong>대용량의 실시간 로그 및 이벤트 스트리밍</strong>에 적합합니다.</li>
</ul>

<p><strong>메시지 순서와 중복 처리</strong></p>
<ul>
  <li><strong>RabbitMQ</strong>: 메시지 순서를 보장하지는 않지만, 필요한 경우 순서가 보장되도록 설정할 수 있습니다. <strong>중복 처리 방지</strong>를 지원하며 각 메시지를 고유하게 식별하고 소비자에게 전달합니다.</li>
  <li><strong>Kafka</strong>: 파티션 내에서 메시지 순서를 보장하며, 메시지가 중복될 수 있습니다. 여러 소비자가 같은 메시지를 반복적으로 읽을 수 있으므로 <strong>순차적 로그 분석</strong>에 적합합니다.</li>
</ul>

<p>RabbitMQ는 빠른 메시지 전달과 작업 큐가 필요한 마이크로서비스 통신에 적합하며, Kafka는 대규모 데이터 스트리밍 및 로그 처리에 강점을 갖고 있습니다.</p>

<p><br /></p>

<h2 id="적용-예시">적용 예시</h2>

<h3 id="espresso">Espresso</h3>

<p>Espresso는 LinkedIn의 분산 NoSQL DB입니다.<br />
Kafka는 그 Espresso에서 발생한 변경 사항을 복제·전파하기 위한 내부 커밋 로그/스트림(backbone)으로 쓰입니다.<br />
즉, Espresso가 ‘Source of truth’라면, <strong>Kafka는 그 변경을 다른 레플리카나 시스템으로 정확하고 순서 있게 전달하는 파이프 역할</strong>을 합니다.</p>

<p><img src="/assets/img/for-post/What%20is%20Kafka/image%2019.png" alt="MySQL DB를 Kafka를 통해 Replication하는 Flow. Kafka에 이벤트를 전달하기까지만 보여줌." />
<em>Yelp에서 MySQL DB를 Kafka를 통해 Replication하는 Flow. Kafka에 이벤트를 전달하기까지만 보여줌. | <a href="https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html">engineeringblog.yelp.com</a></em></p>

<h3 id="kubernetes에서-적용하기">Kubernetes에서 적용하기</h3>
<p>Kubernetes에서 Kafka를 구성할때는, Operator를 통해 구성합니다.</p>

<h4 id="strimzi">Strimzi</h4>
<p>Strimzi는 Kubernetes(또는 OpenShift) 위에서 Apache Kafka를 쉽게 배포·운영·보안·업그레이드할 수 있도록 해주는 오픈소스 Kafka 오퍼레이터 세트입니다.</p>

<h2 id="references">References</h2>

<dl>
  <dt>Apache Kafka | kafka.apache.org</dt>
  <dd><a href="https://kafka.apache.org/">Apache Kafka</a></dd>
  <dt>Kafka Overview | ibm-cloud-architecture.github.io</dt>
  <dd><a href="https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/">Kafka Overview - IBM Automation - Event-driven Solution - Sharing knowledge</a></dd>
  <dt>LinkedIn and Apache Kafka | linkedin.com</dt>
  <dd><a href="https://www.linkedin.com/pulse/linkedin-apache-kafka-muhammad-waqas-dilawar/">LinkedIn and Apache Kafka</a></dd>
  <dt>Kafka Ecosystem at LinkedIn | linkedin.com</dt>
  <dd><a href="https://www.linkedin.com/blog/engineering/open-source/kafka-ecosystem-at-linkedin">Kafka Ecosystem at LinkedIn</a></dd>
  <dt>Kafka at LinkedIn: Current and Future | engineering.linkedin.com</dt>
  <dd><a href="https://engineering.linkedin.com/kafka/kafka-linkedin-current-and-future">Kafka at LinkedIn: Current and Future</a></dd>
  <dt>Kafka on Kubernetes: Reloaded for fault tolerance | engineering.grab.com</dt>
  <dd><a href="https://engineering.grab.com/kafka-on-kubernetes">Kafka on Kubernetes: Reloaded for fault tolerance</a></dd>
  <dt>The Fundamentals of Apache Kafka Architecture | developer.confluent.io</dt>
  <dd><a href="https://developer.confluent.io/courses/architecture/get-started/">Apache Kafka Architecture Deep Dive</a></dd>
  <dt>How LinkedIn Customizes Its 7 Trillion Message Kafka Ecosystem | blog.bytebytego.com</dt>
  <dd><a href="https://blog.bytebytego.com/p/how-linkedin-customizes-its-7-trillion">How LinkedIn Customizes Its 7 Trillion Message Kafka Ecosystem</a></dd>
  <dt>Jay Kreps Hadoop Summit 2011 Building Kafka and LinkedIn’s Data Pipeline | youtube.com</dt>
  <dd><a href="https://youtu.be/Eq3i2m8aJBI?si=YwTu9QHnq9x8889L">Jay Kreps Hadoop Summit 2011 Building Kafka and LinkedIn’s Data Pipeline</a></dd>
  <dt>Building a Real-time Data Pipeline: Apache Kafka at LinkedIn | youtube.com</dt>
  <dd><a href="https://youtu.be/MA_3fPBFBtg?si=aLnAu9DlcTcH3XcI">Building a Real-time Data Pipeline: Apache Kafka at LinkedIn</a></dd>
  <dt>How Kafka Producers, Message Keys, Message Format and Serializers Work in Apache Kafka? | geeksforgeeks.org</dt>
  <dd><a href="https://www.geeksforgeeks.org/java/how-kafka-producers-message-keys-message-format-and-serializers-work-in-apache-kafka/">How Kafka Producers, Message Keys, Message Format and Serializers Work in Apache Kafka? - GeeksforGeeks</a></dd>
  <dt>Apache Kafka | d3s.mff.cuni.cz</dt>
  <dd><a href="https://d3s.mff.cuni.cz/files/teaching/nswi080/lectures/notes/ch02s11.html">2.11 Apache Kafka</a></dd>
  <dt>Apache Avro | avro.apache.org</dt>
  <dd><a href="https://avro.apache.org/docs/">Documentation</a></dd>
  <dt>About Schema Registry | docs.confluent.io</dt>
  <dd><a href="https://docs.confluent.io/platform/current/schema-registry/index.html">Schema Registry for Confluent Platform | Confluent Documentation</a></dd>
  <dt>Kafka Partition Strategies | github.com/AutoMQ</dt>
  <dd><a href="https://github.com/AutoMQ/automq/wiki/Kafka-Partition:-All-You-Need-to-Know-&amp;-Best-Practices">Kafka Partition: All You Need to Know &amp; Best Practices</a></dd>
  <dt>Kafka partition strategy | redpanda.com</dt>
  <dd><a href="https://www.redpanda.com/guides/kafka-tutorial-kafka-partition-strategy">Kafka Partition Strategies: Optimize Your Data Streaming</a></dd>
  <dt>Consumer Group Protocol | developer.confluent.io</dt>
  <dd><a href="https://developer.confluent.io/courses/architecture/consumer-group-protocol/">Consumer Group Protocol: Scalability and Fault Tolerance</a></dd>
  <dt>Guide to Consumer Offsets: Manual Control, Challenges, and the Innovations of KIP-1094 | confluent.io</dt>
  <dd><a href="https://www.confluent.io/blog/guide-to-consumer-offsets/">Kafka Consumer Offsets Guide—Basic Principles, Insights &amp;  Enhancements</a></dd>
  <dt>Kafka Rebalancing: Triggers, Side Effects, and Mitigation Strategies | redpanda.com</dt>
  <dd><a href="https://www.redpanda.com/guides/kafka-performance-kafka-rebalancing">Kafka Rebalancing: Triggers, Effects, and Mitigation</a></dd>
  <dt>Kafka consumer lag—Measure and reduce | redpanda.com</dt>
  <dd><a href="https://www.redpanda.com/guides/kafka-performance-kafka-consumer-lag">Kafka consumer lag - Measure and reduce</a></dd>
  <dt>Kafka와 RabbitMQ비교 | ibm.com</dt>
  <dd><a href="https://www.ibm.com/think/topics/apache-kafka">What is Apache Kafka? | IBM</a></dd>
  <dt>Introducing Espresso - LinkedIn’s hot new distributed document store | engineering.linkedin.com</dt>
  <dd><a href="https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store">Introducing Espresso - LinkedIn’s hot new distributed document store</a></dd>
  <dt>Espresso Database Replication with Kafka | confluent.io</dt>
  <dd><a href="https://www.confluent.io/ko-kr/resources/kafka-summit-2016/espresso-database-replication-kafka/">Espresso Database Replication with Kafka - Confluent | KR</a></dd>
  <dt>Streaming MySQL tables in real-time to Kafka | engineeringblog.yelp.com</dt>
  <dd><a href="https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html">Streaming MySQL tables in real-time to Kafka</a></dd>
  <dt>strimzi | strimzi.io</dt>
  <dd><a href="https://strimzi.io/">Strimzi - Apache Kafka on Kubernetes</a></dd>
  <dt>Scaling Elasticsearch Across Data Centers With Kafka | elastic.co</dt>
  <dd><a href="https://www.elastic.co/blog/scaling_elasticsearch_across_data_centers_with_kafka">Scaling Elasticsearch Across Data Centers With Kafka</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="DevOps" /><category term="CloudNative" /><category term="aws" /><category term="kubernetes" /><category term="cncf" /><category term="k8s" /><category term="kafka" /><summary type="html"><![CDATA[Kafka 소개]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/What%20is%20Kafka/kafka-cover.png" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/What%20is%20Kafka/kafka-cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is CloudNativePG? | CloudNative</title><link href="https://blog.devpour.net/posts/What-is-CloudNativePG/" rel="alternate" type="text/html" title="What is CloudNativePG? | CloudNative" /><published>2025-07-21T16:50:00+09:00</published><updated>2025-07-23T14:56:30+09:00</updated><id>https://blog.devpour.net/posts/What%20is%20CloudNativePG</id><content type="html" xml:base="https://blog.devpour.net/posts/What-is-CloudNativePG/"><![CDATA[<h2 id="cloudnativepg-소개">CloudNativePG 소개</h2>

<dl>
  <dt>CloudNativePG(이하 CNPG)는</dt>
  <dd><strong>PostgreSQL을 Kubernetes 환경에 네이티브하게 배포 및 운영</strong>할 수 있도록 해주는 <strong>오픈소스 오퍼레이터(Operator)</strong>입니다.</dd>
  <dd>CNCF(Cloud Native Computing Foundation)의 <strong>인큐베이팅 프로젝트</strong>로 채택되어 있으며, PostgreSQL를 Kubernetes에서 안정적이고 확장 가능하게 운영할 수 있도록 도와줍니다.</dd>
</dl>

<h3 id="kubernetes에서-operator가-뭔가요">Kubernetes에서 Operator가 뭔가요?</h3>

<blockquote>
  <p>Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components.<br />
from <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">kubernetes.io</a></p>
</blockquote>

<dl>
  <dt>Kubernetes에서의 Operator는</dt>
  <dd>Custom Resource를 구성하기 위한 ‘extensions’입니다.</dd>
  <dd>애플리케이션이나 서비스의 수명 주기를 자동으로 관리하기 위한 컨트롤러 패턴입니다.</dd>
  <dd>주로, Stateful 어플리케이션(DB, Message Queue 등)에 적합합니다.</dd>
</dl>

<p><br /></p>

<h3 id="왜-필요한가요">왜 필요한가요?</h3>

<h4 id="kubernetes에서-stateful-어플리케이션을-구성하는것의-어려움">Kubernetes에서 Stateful 어플리케이션을 구성하는것의 어려움</h4>

<p>Kubernetes에서, Stateful 어플리케이션을 안정적이고 효율적으로 운영하기 위해서는, 많은 설정들이 추가로 필요합니다.<br />
데이터 저장, 복제, 장애 조치, 업그레이드 등 운영 지식이 많이 필요한 서비스를 직접 구성하기에는 여러 어려움이 있고, Kubernetes의 기본 기능만으로는 이런 복잡한 요구사항을 자동화하는것에도 어려움이 있습니다.</p>

<p>예를 들면,
  개별 Pod마다 정해져 있는 PV(Persistent Volume)를 사용하도록 설정하거나,
  Instance에 대한 일관된 Discovery기능을 제공해야 합니다.</p>

<p>이런 어려움을, Operator 패턴(여기서는 CNPG)이 해소시켜 줍니다.</p>

<h4 id="고가용성ha-high-availability-구성-자동화">고가용성(HA, High Availability) 구성 자동화</h4>

<p>HA를 제공하기 위해, PostgreSQL을 3개 노드로 구성하고, streaming replication을 설정하고, 장애 시 리더를 자동 승격하는 등의 작업은 수동으로 하려면 매우 복잡합니다.</p>

<blockquote class="prompt-info">
  <p>CNPG는 단방향(‘Primary → Replica’)구조만 지원합니다.<br />
즉, 양방향(bi-directional replication, Multi master)환경을 지원하지 않습니다.<br />
(CNPG의 개발사인 EDB의 엔터프라이즈 솔루션에서는 지원한다고 합니다.)</p>
</blockquote>

<p>CloudNativePG는 이를 모두 자동으로 구성하고, 장애 발생 시 자동으로 복구할 수 있을정도로, DB를 추상화해줍니다.</p>

<h4 id="백업복구-및-pitr-지원">백업/복구 및 PITR 지원</h4>

<p>Kubernetes에서 PostgreSQL 백업을 주기적으로 S3 등에 저장하고, 시점 복구(PITR)를 제공하는 것은 쉽지 않습니다.<br />
CloudNativePG는 pgBackRest를 통합하여, <strong>YAML 설정만으로 백업과 복구를 자동화</strong> 구성할 수 있습니다</p>

<h4 id="롤링-업그레이드무중단-업그레이드-및-버전-관리">롤링 업그레이드(무중단 업그레이드) 및 버전 관리</h4>

<p>PostgreSQL의 minor 버전 업그레이드를 다운타임 없이 하려면, 데이터 동기화와 ‘리더 전환’이 필요합니다.<br />
CloudNativePG는 버전 변경 시 <strong>롤링 업그레이드 자동 수행</strong>하여, 버젼관리에 대한 부담을 줄여줍니다.</p>

<p><br /></p>

<h3 id="deployment-architecture">Deployment Architecture</h3>

<p><img src="/assets/img/for-post/What%20is%20CloudNativePG/image.png" alt="Deployment Architecture on GKE" />
<em>Deployment Architecture on GKE | from <a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/cloudnativepg">cloud.google.com</a></em></p>

<p>‘CNPG’를 사용하여 배포하면, 여러 AZ(Availability Zone)에 걸쳐 DB의 Instance를 구축합니다.<br />
각각의 Instance에 할당되는 Disk도 AZ에 분산되어 있습니다.</p>

<p><img src="/assets/img/for-post/What%20is%20CloudNativePG/image%201.png" alt="CNPG의 Primary구조와 Backup Storage" class="w-50" />
<em>CNPG의 Primary구조와 Backup Storage | from <a href="https://learn.microsoft.com/en-us/azure/aks/postgresql-ha-overview">learn.microsoft.com</a></em></p>

<p>CNPG로 구성한 환경에서는, 단방향 Replication을 지원하며, Client(여기서는 App)은 Primary에 접속하게 됩니다.</p>

<h4 id="cnpg의-service구조">CNPG의 Service구조</h4>

<p><img src="/assets/img/for-post/What%20is%20CloudNativePG/techblog-postgres-architecture.drawio.png" alt="Primary Service와 Replica Service로 구분하여 사용가능." />
<em>Primary Service와 Replica Service로 구분하여 사용가능.</em></p>

<p>CNPG로 Postrgesql을 구성하면, Primary와 Replica Service가 만들어지며, 해당 Service를 통해 Client에서 DB에 접속할 수 있습니다.</p>

<p>만약, ‘my-db’라는 이름의 클러스터를 구성했다고 할 때, 다음과 같은 Service가 만들어집니다.(CQRS패턴)</p>
<ul>
  <li>
    <p>my-db-rw<br />
항상 <strong>현재 Primary Pod</strong>를 가리킵니다.<br />
SELECT, INSERT, UPDATE, DELETE 등 쓰기 요청을 보낼 수 있는 엔드포인트를 제공합니다.</p>
  </li>
  <li>
    <p>my-db-ro<br />
Replica Pod로 라우팅합니다.<br />
읽기 전용 쿼리(예: 분석용)를 분산시켜 부하를 줄여줍니다.</p>
  </li>
  <li>
    <p>my-db-r(Headless Service)<br />
<code class="language-plaintext highlighter-rouge">my-db-0.my-db-r.default.svc.cluster.local</code> 등의 DNS 이름을 통해 개별 Pod에 접근 할 수 있게해줍니다.<br />
CNPG Operator나 내부 복제 로직, 또는 pgbouncer pooler가 사용하기도 합니다.(즉 DB Pod끼리 서로 인식할때 사용합니다.)</p>
  </li>
</ul>

<p><br /></p>

<h2 id="cloudnativepg-기능-소개">CloudNativePG 기능 소개</h2>

<ul>
  <li>
    <p>PostgreSQL 클러스터 생성 및 관리<br />
YAML 선언만으로 PostgreSQL 클러스터 생성할 수 있습니다.<br />
StatefulSet, PVC(Persistent Volume Claim) 등을 자동으로 구성해줍니다.</p>
  </li>
  <li>HA(High Availability) 지원<br />
기본적으로 <strong>3개의 인스턴스</strong>를 통해 HA를 구성합니다.<br />
리더-팔로워 구조를 자동으로 구성해줍니다.<br />
patroni 대신 <strong>native streaming replication</strong> 기반의 고가용성(HA)을 제공합니다.
    <blockquote class="prompt-info">
      <p>‘<strong>native streaming replication</strong>‘은<br />
PostgreSQL이 자체적으로 제공하는 Replication 기능으로, 기본 기능만으로 동작하며 별도의 외부 도구 없이도 실시간으로 데이터를 리더(Primary) → 팔로워(Standby) 노드로 복제할 수 있는 메커니즘입니다.</p>
    </blockquote>
  </li>
  <li>
    <p>자동 장애 복구<br />
리더 노드 장애 시 자동으로 팔로워 중 하나를 승격하여, ‘Fail over(장애 극복)’를 제공합니다.</p>
  </li>
  <li>
    <p>백업 및 PITR(Point In Time Recovery)<br />
‘pgBackRest’를 내장하여 백업/복구 지원합니다.<br />
S3 등 외부 스토리지 연동 가능합니다.</p>
  </li>
  <li>
    <p>롤링 업그레이드<br />
PostgreSQL minor 버전 업그레이드를 다운타임 없이 수행합니다.</p>
  </li>
  <li>Kubernetes 네이티브<br />
kubectl, CRD, Operator 패턴 등 K8s 생태계에 최적화되어 있습니다.<br />
Cluster, Backup, ScheduledBackup, Pooler 등의 CRD를 제공합니다.</li>
</ul>

<p><br /></p>

<h3 id="patroni와-비교">Patroni와 비교</h3>

<p>‘Patroni’는 CNPG와 마찬가지로, Postgesql의 HA를 위한 솔루션중 하나입니다. 다만, 철학과 구성 방식, 운영 환경이 다릅니다.</p>

<table>
  <thead>
    <tr>
      <th><strong>항목</strong></th>
      <th><strong>Patroni</strong></th>
      <th><strong>CloudNativePG</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>목적의 차이</strong></td>
      <td>PostgreSQL 고가용성 구성</td>
      <td>Kubernetes 네이티브 PostgreSQL 운영</td>
    </tr>
    <tr>
      <td><strong>배포 환경의 차이</strong></td>
      <td><strong>VM, Bare Metal, Kubernetes 등 범용</strong></td>
      <td><strong>Kubernetes 전용</strong></td>
    </tr>
    <tr>
      <td><strong>복제 방식</strong></td>
      <td>PostgreSQL native streaming replication</td>
      <td>PostgreSQL native streaming replication</td>
    </tr>
    <tr>
      <td><strong>리더 선출 방식</strong></td>
      <td>etcd, Consul, Zookeeper 등 외부 저장소(DCS, Distributed Configuration Store) 필요</td>
      <td>자체 리더 선출 로직 내장 (DCS 불필요)</td>
    </tr>
    <tr>
      <td><strong>클러스터 구성</strong></td>
      <td>수동 YAML 구성 + DCS 세팅 필요</td>
      <td>YAML CRD만 작성하면 자동 구성</td>
    </tr>
    <tr>
      <td><strong>자동화 수준</strong></td>
      <td>설치 및 설정 수동, 자동 failover는 가능</td>
      <td>설치부터 백업, 업그레이드까지 전면 자동화</td>
    </tr>
    <tr>
      <td><strong>백업 기능</strong></td>
      <td>직접 구성 필요 (pgBackRest 연동 등)</td>
      <td>pgBackRest 내장, ScheduledBackup CRD 제공</td>
    </tr>
    <tr>
      <td><strong>롤링 업그레이드</strong></td>
      <td>수동 처리.</td>
      <td>자동 지원 (클러스터 단위)</td>
    </tr>
    <tr>
      <td><strong>읽기 전용 리플리카</strong></td>
      <td>가능</td>
      <td>가능 (Pooler 연동)</td>
    </tr>
    <tr>
      <td><strong>운영 복잡도</strong></td>
      <td>중~고</td>
      <td>낮음 (Kubernetes 사용 시)</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="references">References</h2>

<dl>
  <dt>CloudNativePG | cloudnative-pg.org</dt>
  <dd><a href="https://cloudnative-pg.io/">CloudNativePG - PostgreSQL Operator for Kubernetes</a></dd>
  <dt>cloudnative-pg Git Repository | github.com/cloudnative-pg/cloudnative-pg</dt>
  <dd><a href="https://github.com/cloudnative-pg/cloudnative-pg">https://github.com/cloudnative-pg/cloudnative-pg</a></dd>
  <dt>Kubernetes Operator pattern | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Operator pattern</a></dd>
  <dt>Deploy PostgreSQL to GKE using CloudNativePG | cloud.google.com</dt>
  <dd><a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/cloudnativepg">Deploy PostgreSQL to GKE using CloudNativePG | Kubernetes Engine | Google Cloud</a></dd>
  <dt>Recommended architectures for PostgreSQL in Kubernetes | cncf.io</dt>
  <dd><a href="https://www.cncf.io/blog/2023/09/29/recommended-architectures-for-postgresql-in-kubernetes/">Recommended architectures for PostgreSQL in Kubernetes</a></dd>
  <dt>Overview of deploying a highly available PostgreSQL database on Azure Kubernetes Service (AKS) | learn.microsoft.com</dt>
  <dd><a href="https://learn.microsoft.com/en-us/azure/aks/postgresql-ha-overview">Deploying a PostgreSQL Database on AKS with CloudNativePG - Azure Kubernetes Service</a></dd>
  <dt>EDB and Partner OptimaData Discuss Postgres, Kubernetes and the Power of Cloud Native | enterprisedb.com</dt>
  <dd><a href="https://www.enterprisedb.com/blog/edb-and-partner-optimadata-discuss-postgres-kubernetes-and-power-cloud-native">EDB and Partner OptimaData Discuss Postgres, Kubernetes and the Power of Cloud Native</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="DevOps" /><category term="CloudNative" /><category term="aws" /><category term="kubernetes" /><category term="cncf" /><category term="k8s" /><category term="db" /><category term="postgresql" /><summary type="html"><![CDATA[CloudNativePG 소개]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/What%20is%20CloudNativePG/cnpg-cover.png" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/What%20is%20CloudNativePG/cnpg-cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Networking Essentials | Core Concepts - System Design Interview</title><link href="https://blog.devpour.net/posts/networking-essentials/" rel="alternate" type="text/html" title="Networking Essentials | Core Concepts - System Design Interview" /><published>2025-07-17T15:20:00+09:00</published><updated>2025-07-21T14:55:54+09:00</updated><id>https://blog.devpour.net/posts/networking%20essentials</id><content type="html" xml:base="https://blog.devpour.net/posts/networking-essentials/"><![CDATA[<p>시스템 디자인(System Design)에 있어, 네트워킹(Netwoking)은 고려해야하는 필수적인 부분중 하나입니다.<br />
이 Post에선, 네트워킹에서도 가장 중요한 부분만 정리하려고 합니다.</p>

<h2 id="네트워킹-기초networking-101">네트워킹 기초(Networking 101)</h2>
<h3 id="osiopen-systems-interconnection-model-7-layers">OSI(Open Systems Interconnection) Model 7 Layers</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/image.png" alt="OSI Model 7 Layers" />
<em>OSI Model 7 Layers</em></p>

<p>‘OSI 7 Layers’는 네트워크의 구조를 <strong>추상화(Abstract)</strong>하여 계층(Layer)으로 표현하는 모델입니다.</p>

<h4 id="osi의-배경">OSI의 배경</h4>

<p>초기 네트워크가 설계될 때부터, 물리적으로 구현된 Architecture는 아닙니다.<br />
이미 네트워크가 만들어진 이후, 네트워크를 정리(교육, 표준화, 문서화 목적으로)하기 위해, 네트워크 계층을 나누어 정리하기 시작했습니다.<br />
이후, <a href="https://www.iso.org/standard/20269.html">ISO/IEC 7498</a> 표준을 통해 OSI가 공식화 되었습니다.</p>

<blockquote class="prompt-info">
  <p>Q: OSI(Open Systems Interconnection)이라는 이름의 의미?<br />
A: OSI가 ‘서로 다른 벤더의 시스템들이, 개방된(open) 표준을 통해 상호연결(interconnect)되어야 한다’라는 목적을 담고 있어서, 지어졌다고 합니다.<br />
OSI가 만들어지던 시절, 각 벤더들은 자기만의 독자적인 프로토콜을 사용하여, 닫힌(Closed) 네트워크를 구축했다고 합니다.</p>
</blockquote>

<h4 id="osi-작동-flow">OSI 작동 Flow</h4>

<p>OSI 모델이 실제로 동작할 때, 데이터가 시스템 간에 오가는 흐름은 크게 송신 측(Send) 과 수신 측(Receive) 으로 나눠 볼 수 있습니다.</p>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%201.png" alt="간략하게 표현한 OSI Flow" />
<em>간략하게 표현한 OSI Flow</em>
<br />
<img src="/assets/img/for-post/Networking%20Essentials/image%202.png" alt="OSI 작동 Flow | from [bytebytego.com](https://bytebytego.com/guides/guides/what-is-osi-model/)" />
<em>OSI 작동 Flow | from <a href="https://bytebytego.com/guides/guides/what-is-osi-model/">bytebytego.com</a></em></p>

<p>각 계층은 자신에게 할당된 책임만 다루고, 상하위 계층은 인터페이스(헤더·트레일러, API 등)로만 통신하기 때문에 유연한 확장성과 호환성을 보장할 수 있습니다.</p>
<ul>
  <li>
    <p>Encapsulation<br />
상위 계층의 데이터를 받은 후, 각 계층이 자신만의 헤더(또는 트레일러)를 순서대로 “캡슐”처럼 씌워 물리 매체(Physical Layer)로 보낼 준비를 합니다.</p>
  </li>
  <li>
    <p>De-encapsulation<br />
물리 계층으로 들어온 <strong>비트</strong>를 받아, <strong>프레임 → 패킷 → 세그먼트 → 데이터</strong> 순서로 ‘껍질’을 벗기듯 떼어내며 최종 애플리케이션 데이터로 복원합니다.</p>
  </li>
</ul>

<p><br /></p>

<h3 id="web-request-예시example">Web Request 예시(example)</h3>

<p>‘Web Request’ 예시를 통해, ‘Request’가 처리되는 과정을 따라가 봅니다.</p>

<p><img src="/assets/img/for-post/Networking%20Essentials/web-request-flow.png" alt="Client → Server Request Example" />
<em>Client → Server Request Example</em></p>

<ol>
  <li>
    <p>DNS Resolution<br />
Client는 Domain name을 IP로 변환합니다.(DNS를 통해)</p>
  </li>
  <li>TCP Handshake<br />
Client는 TCP Connection을 생성하기 위한 작업(‘three-way handshake’)을 실행합니다.
    <ul>
      <li>SYN: Client가 보내는 ‘synchronize’ Packet입니다. Connection 생성을 요청하기 위해 보냅니다.</li>
      <li>SYN-ACK: Server는 SYN-ACK(synchronize-acknowledge) Packet으로 Client에게 응답합니다.</li>
      <li>ACK: Client는 ACK(acknowledge) Packet을 서버에 보내며, Connection이 생성(establish)됩니다.</li>
    </ul>
  </li>
  <li>
    <p>HTTP Request<br />
TCP Connection이 만들어지면, Client는 이를 통해 <code class="language-plaintext highlighter-rouge">HTTP GET요청</code>을 보냅니다.(서버로부터 웹페이지 가져오기)</p>
  </li>
  <li>
    <p>Server processing<br />
Server에서 Client의 Request를 처리하고, Web Page를 생성하여 HTTP Response를 수행할 준비를 합니다.</p>
  </li>
  <li>
    <p>HTTP Response<br />
Server가 만들어진 Response를 Client에게 전달합니다. 이 Response에는 생성한 Web Page도 포함되어 있습니다.</p>
  </li>
  <li>TCP Teardown<br />
전송(HTTP Response에 대한)이 완료되면, Client와 Server는 TCP Connection을 종료합니다.(four-way handshake)
    <ul>
      <li>FIN: Client가 FIN(finish) packet을 서버에 보냅니다.</li>
      <li>ACK: Server가 FIN을 받았다는 의미로 ACK를 보냅니다.</li>
      <li>FIN: Server또한 FIN Packet을 Client에 보냅니다.(Client와 Server 양쪽 모두 종료시키기 위함)</li>
      <li>ACK: Client는 Server가 보낸 FIN Packet을 잘 받았다는 의미로 ACK를 보냅니다.</li>
    </ul>
  </li>
</ol>

<p><br />
<br /></p>

<h2 id="network-layer-protocolslayer-3">Network Layer Protocols(Layer 3)</h2>

<p>OSI 모델에서 ‘패킷(Packet)’ 단위의 전송을 담당하며, 주로 논리적 주소 지정과 경로 설정(Routing) 기능을 수행합니다.</p>

<blockquote class="prompt-info">
  <p><strong>패킷(Packet)과 IP 데이터그램(IP Datagram)</strong><br />
패킷은 OSI의 네트워크 계층에서 사용하는 추상적이 데이터 단위입니다.<br />
IP 데이터그램은, IP 프로토콜에서 실제로 전송되는 단위를 말합니다.</p>
</blockquote>

<h3 id="ipinternet-protocol-address">IP(Internet Protocol) Address</h3>

<p>IP Address는 ‘전 지구적 라우팅을 위한 주소 체계’입니다.</p>

<dl>
  <dt>IP버젼에 따라,</dt>
  <dd>IPv4와 IPv6로 나뉩니다.</dd>
  <dt>IP의 역할에 따라,</dt>
  <dd>Public IP(공인 IP)와 Private IP(사설 IP)로 구분하여 설명합니다.</dd>
</dl>

<h4 id="ipv4와-ipv6">IPv4와 IPv6</h4>

<table>
  <thead>
    <tr>
      <th><strong>항목</strong></th>
      <th><strong>IPv4</strong></th>
      <th><strong>IPv6</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>주소 길이</td>
      <td>32비트</td>
      <td>128비트</td>
    </tr>
    <tr>
      <td>주소 공간</td>
      <td>약 42억 개 (2³²)</td>
      <td>약 3.4×10³⁸ (2¹²⁸)</td>
    </tr>
    <tr>
      <td>표기법</td>
      <td>점으로 구분된 10진수 (예: 192.0.2.1)</td>
      <td>콜론으로 구분된 16진수 (예: 2001:0db8:85a3::8a2e:0370:7334)</td>
    </tr>
  </tbody>
</table>

<h4 id="ipv6의-필요성">IPv6의 필요성</h4>

<p>IPv4의 32비트 공간은 인터넷 기기 폭발적 증가로 고갈되어, NAT(Network Address Translation) 같은 우회책(Private IP를 사용하는)이 필요해졌습니다.<br />
IPv6는 128비트 주소 덕분에 거의 무제한에 가까운 주소를 제공하며, 각 기기에 고유 글로벌 주소를 부여할 수 있습니다.</p>

<blockquote class="prompt-info">
  <p>IPv4와 IPv6를 모두 사용하는 것을 ‘Dual-stack(듀얼 스택)’이라고 합니다.</p>
</blockquote>

<h4 id="public-ip와-private-ip">Public IP와 Private IP</h4>

<p>IPv4의 주소가 고갈되면서, ‘IPv4’의 체계를 유지하면서 시스템 확장에 대응하기 위해, ‘Private IP(사설 IP)’가 만들어졌습니다.</p>

<p><br />
Public Network와 Private Network사이에서, NAT(Network Address Translator)를 통해 IP가 치환되어 전송됩니다.</p>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%203.png" alt="NAT의 역할을 보여주는 이미지. 출처: [https://en.wikipedia.org/wiki/Network_address_translation](https://en.wikipedia.org/wiki/Network_address_translation)" />
<em>NAT의 역할을 보여주는 이미지. 출처: <a href="https://en.wikipedia.org/wiki/Network_address_translation">https://en.wikipedia.org/wiki/Network_address_translation</a></em></p>

<p>Private IP는 <a href="https://datatracker.ietf.org/doc/html/rfc1918">RFC 1918</a>(사설 인터넷에 대한 주소 할당 표준)을 통해 별도의 IPv4의 영역으로 구분되어 있습니다.<br />
아래와 같은 주소 범위가 Private IP로 사용됩니다.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>10.0.0.0        -   10.255.255.255  (10/8 prefix)
172.16.0.0      -   172.31.255.255  (172.16/12 prefix)
192.168.0.0     -   192.168.255.255 (192.168/16 prefix)
</pre></td></tr></tbody></table></code></pre></div></div>

<blockquote class="prompt-info">
  <p>‘RFC 1918’에 따라, Public Network에서는 해당 IP대역이 라우팅되지 않도록 필터링 됩니다.</p>
</blockquote>

<h4 id="ipv4-header-구조">IPv4 Header 구조</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%204.png" alt="IPv4 Header 구조" />
<em>IPv4 Header 구조</em></p>

<p><br /></p>

<h3 id="icmpinternet-control-message-protocol">ICMP(Internet Control Message Protocol)</h3>

<p><code class="language-plaintext highlighter-rouge">ping www.google.com</code> 으로 대표되는 Protocol입니다.<br />
주로 네트워크 통신문제를 진달할 때 사용되며, 데이터가 ‘의도한 대상에게 도달하는지’를 확인하는데 사용됩니다.</p>

<p><br /></p>

<h3 id="ipsecinternet-protocol-security">IPSec(Internet Protocol Security)</h3>

<p>인터넷 프로토콜(IP)을 통해 전송되는 데이터를 안전하게 보호하기 위한 프로토콜 모음입니다.<br />
호스트 간, 게이트웨이(라우터/방화벽)간 터널링, VPN 등에 사용됩니다.</p>

<p><br />
IPSec은 보호 수준에 따라서, 2가지 모드를 제공합니다.</p>

<p><strong>전송 모드(Transport Mode)</strong></p>
<ul>
  <li>원본 IP 헤더는 그대로 두고, 페이로드(예: TCP/UDP 데이터)만 보호합니다.</li>
  <li>호스트 간(end-to-end) 보안에 주로 사용합니다.</li>
</ul>

<p><strong>터널 모드(Tunnel Mode)</strong></p>
<ul>
  <li>원본 IP 패킷 전체를 새로운 IP 헤더 뒤에 캡슐화(encapsulation)합니다.</li>
  <li>게이트웨이 간(site-to-site) VPN 터널에 주로 사용합니다.</li>
  <li>외부에서는 터널링 장비의 IP 두 개만 보이고, 내부 네트워크 구조는 은닉합니다.</li>
</ul>

<p><br />
<br /></p>

<h2 id="transport-layerlayer-4">Transport Layer(Layer 4)</h2>

<p>Transport Layer (4계층) 은 호스트 간(혹은 어플리케이션 간)의 종단간(end-to-end) 통신을 책임지며, 주로 세그먼트(segment) 단위로 데이터를 처리합니다.<br />
주요 Protocol로는 TCP, UDP, QUIC가 있습니다.</p>

<blockquote class="prompt-info">
  <p>대부분의 시스템 디자인 인터뷰에서는 TCP나 UDP(거의 TCP) 위주로 다룹니다.</p>
</blockquote>

<h3 id="tcptransmission-control-protocol-전송제어-프로토콜">TCP(Transmission Control Protocol, 전송제어 프로토콜)</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%205.png" alt="TCP Connection 생성과 종료과정의 Handshake" />
<em>TCP Connection 생성과 종료과정의 Handshake</em>
<br />
<img src="/assets/img/for-post/Networking%20Essentials/image%206.png" alt="How TCP manages a byte stream. | [tcpcc.systemapproach.org](https://tcpcc.systemsapproach.org/tcp_ip.html#reliable-and-ordered-delivery)" />
<em>How TCP manages a byte stream. | <a href="https://tcpcc.systemsapproach.org/tcp_ip.html#reliable-and-ordered-delivery">tcpcc.systemapproach.org</a></em></p>

<p>TCP는 연결 상태에 대한 <strong>강한 신뢰</strong>가 필요할때 사용합니다. TCP는 다음과 같은 특징을 갖고 있습니다.</p>

<blockquote class="prompt-info">
  <p>여기서 ‘신뢰’는 모든 Packet이 ‘결과적으로’ 정확하게 도착하는것을 말합니다.</p>
</blockquote>

<ul>
  <li>
    <p>Connection-oriented(연결형 서비스)<br />
통신을 시작하기 전에, 송·수신 간에 3-way Handshake로 논리적인 ‘Session(세션)’을 설정합니다.</p>
  </li>
  <li>
    <p>Reliability(신뢰성, 신뢰할 수 있습니다.)<br />
전송된 Segment(세그먼트, 조각)에 대해 ACK응답을 받고(받았는지 확인하는 신호), 누락 혹은 손상된 경우에는 재전송합니다.</p>
  </li>
  <li>
    <p>In-order Delivery(순서 보장)<br />
수신측에서 순서가 뒤바뀐 Segment를 버퍼링하여 원래 순서대로 어플리케이션에 전달합니다.</p>
  </li>
  <li>
    <p>Flow Control(흐름 제어)<br />
수신 측이 처리 가능한 만큼만 보내도록 송신 윈도우 크기(Rwnd, Receiver Window)를 조절합니다. (슬라이딩 윈도우 사용)</p>
  </li>
  <li>
    <p>Congestion Control(혼잡 제어)<br />
네트워크 과부하를 방지하도록 송신 속도를 조절합니다. (Slow Start, Congestion Avoidance, Fast Recovery 등 이용)</p>
  </li>
  <li>
    <p>Error Detection(오류 검출)<br />
헤더와 페이로드에 대해 Checksum을 계산하여 손상 여부를 검사합니다.</p>
  </li>
</ul>

<h4 id="사용-사례">사용 사례</h4>

<p>파일 전송(FTP/SFTP), 이메일(SMTP/IMAP), 데이터베이스 요청, 결제 트랜잭션, HTTP/HTTPS, SSH 등…</p>

<h4 id="tcp-segment-header-format">TCP Segment Header Format</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%207.png" alt="TCP header format | [tcpcc.systemapproach.org](https://tcpcc.systemsapproach.org/tcp_ip.html#reliable-and-ordered-delivery)" class="w-50" />
<em>TCP header format | <a href="https://tcpcc.systemsapproach.org/tcp_ip.html#reliable-and-ordered-delivery">tcpcc.systemapproach.org</a></em></p>

<ul>
  <li>Sequence Number: ‘이 세그먼트에 담긴 데이터 바이트 중 <strong>첫 번째 바이트가 전체 바이트 스트림에서 몇 번째 바이트인가</strong>‘를 나타냅니다. 전체 바이트 스트림중 세그먼트의 위치를 알 수 있게 해줍니다.
    <ul>
      <li>예를 들면, 다음과 같은 값이 설정되게 됩니다.<br />
첫 세그먼트에 바이트 #1~#500이 담겼다면 Sequence Number = 1<br />
두 번째 세그먼트에 바이트 #501~#1000이 담겼다면 Sequence Number = 501</li>
    </ul>
  </li>
  <li>Acknowledgment Number: 다음에 기대하는 바이트 위치(즉, 마지막으로 받은 바이트+1)</li>
  <li>Flags:<br />
각 플래그 값의 의미는 다음과 같습니다.
    <ul>
      <li>SYN: 연결 요청</li>
      <li>ACK: 응답 확인</li>
      <li>FIN: 연결 종료 요청</li>
      <li>PSH, RST, URG 등</li>
    </ul>
  </li>
  <li>Window Size: 수신자 버퍼 여유 공간(Rwnd, Receiver Window)을 나타냅니다.</li>
  <li>Checksum: 헤더+데이터 무결성 확인용으로 사용됩니다.</li>
</ul>

<h4 id="tcp-keepalive">TCP Keepalive</h4>

<dl>
  <dt>TCP Keepalive는</dt>
  <dd>TCP 연결이 유휴 상태(idle)가 된 후에도 피어(peer)가 살아 있는지 확인해 주기 위한 운영체제(OS) 수준의 기능입니다.</dd>
  <dd>‘half-open 연결(TCP를 구성하는 Host중 한쪽만 열린 상태)’을 정리하거나 방화벽·NAT 매핑 유지를 목적으로 사용합니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>HTTP Header의 <code class="language-plaintext highlighter-rouge">Connection: keep-alive</code> 는 동작하는 Layer가 다르며, HTTP KeepAlive가 반드시 TCP KeepAlive를 기반으로 작동하는것은 아닙니다.</p>
</blockquote>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%208.png" alt="TCP Keepalive가 connection이 닫히는걸 방지하는 Flow | from [aws.amazon.com](https://aws.amazon.com/ko/blogs/networking-and-content-delivery/implementing-long-running-tcp-connections-within-vpc-networking/)" />
<em>TCP Keepalive가 connection이 닫히는걸 방지하는 Flow | from <a href="https://aws.amazon.com/ko/blogs/networking-and-content-delivery/implementing-long-running-tcp-connections-within-vpc-networking/">aws.amazon.com</a></em></p>

<ol>
  <li>
    <p>Idle 타이머 시작(Start Keepalive Idle Timer)<br />
연결이 ESTABLISHED 상태로 일정 시간(예: 2시간) 동안 데이터 교환이 없으면, 커널은 <strong>Keepalive Probe</strong>를 보내기 위해 타이머를 시작합니다.</p>
  </li>
  <li>
    <p>Probe 전송(Keepalive Probe)<br />
상대방에게 “아직 살아 있나?”를 묻는 <strong>빈 ACK 세그먼트</strong>(페이로드 0) 또는 <strong>윈도우 프로브</strong>(window=0) 형태로 전송합니다.<br />
이때 SEQ 번호는 SND.UNA-1 (즉, 마지막으로 성공적으로 ACK된 바이트 이전)으로 설정해, 상대가 ACK를 답하도록 유도합니다.</p>
  </li>
  <li>
    <p>응답 대기 및 재전송(Keepalive Retransmit)<br />
첫 Probe 후 일정 시간(예: 75초) 동안 응답이 없으면 재전송 합니다.<br />
재전송 횟수(예: 9회)를 초과할 때까지 Probe를 반복합니다.</p>
  </li>
  <li>
    <p>연결 해제(Keepalive Timeout)<br />
지정된 횟수만큼 Probe에 답이 없으면 “peer unreachable”로 간주합니다.<br />
소켓을 강제 종료하고, 애플리케이션에 오류(ETIMEDOUT)를 알리게 됩니다.</p>
  </li>
</ol>

<h4 id="congestion-control">Congestion Control</h4>

<dl>
  <dt>TCP Congestion Control(혼잡 제어)는</dt>
  <dd>네트워크 혼잡(congestion) 상황에서 패킷 손실과 지연(latency) 폭발을 방지하기 위해, 송신(Sender) 측이 전송 속도를 동적으로 조절하는 메커니즘입니다.</dd>
</dl>

<p>다음과 같은 효과가 있습니다.</p>
<ul>
  <li>
    <p>버퍼 오버플로우 방지<br />
라우터·스위치의 큐(buffer)가 가득 차면 패킷 손실이 급증하게 되는데, 이를 예방합니다.</p>
  </li>
  <li>
    <p>네트워크 사용에 대한 공정성(Fairness) 제공<br />
여러 송신자가 하나의 링크(네트워크상의 경로를 의미)를 공유할 때, 서로 과도하게 대역폭을 가져가지 않도록 조절합니다.</p>
  </li>
  <li>
    <p>효율성(Efficiency) 증대<br />
네트워크 용량(capacity)을 최대한 활용하되, 손실 없이 안정적으로 작동하게 합니다.</p>
  </li>
</ul>

<p><img src="/assets/img/for-post/Networking%20Essentials/TCP_Slow-Start_and_Congestion_Avoidance.png" alt="주요 Algorithm Flow" />
<em>주요 Algorithm Flow</em></p>

<ol>
  <li>
    <p>Slow-Start<br />
적은 데이터를 전송하면서, 네트워크의 허용량을 가늠하며, 전송할 데이터의 양을 늘립니다.<br />
ACK(수신완료 신호)가 올 때마다 cwnd(Congestion Window)를 2배씩(지수적으로) 증가합니다.</p>
  </li>
  <li>
    <p>Congestion Avoidance<br />
패킷 손실이 발생하면, 해당 부분부터는 조금씩(1 MSS, 선형적으로)증가시킵니다.<br />
네트워크 용량 한계 근처에서 안정적으로 대역폭을 탐색하기 위한 과정입니다.</p>
  </li>
</ol>

<h4 id="tcp는-언제-선택하면-되나요">TCP는 언제 선택하면 되나요?</h4>

<ul>
  <li>신뢰성(신뢰할 수 있는 전송)이 필요할 때</li>
  <li>순서 보장(in-order delivery)이 필요할 때</li>
  <li>흐름 제어(flow control)와 혼잡 제어(congestion control)가 필요할 때</li>
  <li>연결 지향(connection-oriented) 통신이 자연스러운 프로토콜</li>
</ul>

<p><br /></p>

<h3 id="udpuser-datagram-protocol">UDP(User Datagram Protocol)</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/tcp-vs-udp.png" alt="TCP의 Handshake와 같은 과정이 없는 UDP." />
<em>TCP의 Handshake와 같은 과정이 없는 UDP.</em></p>

<p>비연결형, 비신뢰성 전송 프로토콜입니다. TCP와 달리 최소한의 헤더만 붙여 빠른 전송을 목표로 합니다.<br />
UDP는 다음과 같은 특징을 갖고 있습니다.</p>

<ul>
  <li>
    <p>Connectionless(비연결형)<br />
세션 설정(handshake) 과정 없이 곧바로 데이터를 전송합니다.<br />
각 데이터그램을 독립적으로 취급합니다.</p>
  </li>
  <li>
    <p>Unreliable(신뢰할 수 없는)<br />
UDP에서는 데이터 손실이 발생할 수 있습니다.<br />
ACK/Re-transmission 없어, → 손실·순서 뒤바뀜 감지·복구 기능을 제공하지 않습니다.<br />
애플리케이션이, 필요 시 자체 재전송·순서 보장 로직을 구현해야 합니다.</p>
  </li>
  <li>
    <p><strong>Low Overhead(낮은 오버헤드)</strong><br />
헤더 크기 아주 작습니다.(8바이트) → 실시간성이 중요한 서비스에 유리합니다.<br />
각종 기능들이 최소화 되어 있어, 지연(latency)과 처리 부하가 적습니다.</p>
  </li>
  <li>
    <p>애플리케이션 주도 제어<br />
흐름 제어나 혼잡 제어 기능 없어, → 개발자가 직접 조절하거나 라이브러리를 사용해야 합니다.</p>
  </li>
</ul>

<p>브라우져에서 Native로 지원하지 않아, Web App같은 경우 적합하지 않습니다.</p>

<h4 id="udp-header">UDP Header</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%209.png" alt="Format for UDP header" class="w-50" />
<em>Format for UDP header</em></p>

<h4 id="사용-사례-1">사용 사례</h4>

<ul>
  <li>실시간 멀티미디어가 필요한 경우: VoIP, 화상회의, 실시간 스트리밍(지연 최소화)</li>
  <li>
    <p><strong>DNS</strong>: 요청-응답이 작고, 그 과정이 빠르게 이루어져야 합니다. 재시도 로직은 클라이언트에서 처리합니다.<br />
<img src="/assets/img/for-post/Networking%20Essentials/image%2010.png" alt="Domain Resolve 과정" class="w-50" />
 <em>Domain Resolve 과정</em></p>
  </li>
  <li>DHCP, SNMP: 간단한 질의응답 프로토콜</li>
  <li>게임 서버: 일부 패킷 손실을 허용하지만, 높은 처리량이 요구될 때.</li>
</ul>

<h4 id="언제-udp를-사용하면-되나요">언제 UDP를 사용하면 되나요?</h4>

<ul>
  <li><strong>지연(Latency) 최소화</strong>가 우선일 때</li>
  <li><strong>일부 데이터 손실</strong>을 애플리케이션 레벨에서 허용·복구할 수 있을 때</li>
  <li><strong>연결 설정·유지</strong>에 드는 오버헤드를 줄이고 싶을 때</li>
</ul>

<p><br /></p>

<h3 id="quicquick-udp-internet-connections">QUIC(Quick UDP Internet Connections)</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2011.png" alt="TCP + TLS와 QUIC를 비교" />
<em>TCP + TLS와 QUIC를 비교</em></p>

<p>UDP위에 구현된 차세대 전송 프로토콜입니다.<br />
UDP를 기반으로, TCP처럼 연결 지향적(end-to-end) 스트림을 제공하고, 흐름·혼잡 제어(congestion control), 신뢰성 보장(reliability) 메커니즘을 포함하고 있습니다.<br />
이에 따라, OSI의 Transport Layer로 포함시키고 있습니다.</p>

<h4 id="주요-특징">주요 특징</h4>

<ul>
  <li>
    <p>TLS와 Handshake과정 통합<br />
TLS 1.3 암호화 핸드셰이크를 QUIC 연결 설정 과정과 결합하여, 0-RTT 또는 1-RTT로 빠른 보안 설정을 제공합니다.</p>
  </li>
  <li>
    <p>Multiplexing(멀티플렉싱)<br />
하나의 QUIC 연결(connection) 위에 다중 스트림(stream)을 생성해, 개별 스트림 지연이 다른 스트림에 영향을 주지 않음</p>
  </li>
  <li>
    <p>Connection Migration(연결 이주)<br />
클라이언트 IP나 포트가 바뀌어도(예: Wi-Fi→LTE 전환) 동일한 연결 ID를 유지해, 세션 끊김 없이 재개할 수 있습니다.</p>
  </li>
  <li>
    <p>경량 헤더와 확장성을 갖추고 있습니다.<br />
UDP 기반으로 작동하므로 가벼운 헤더를 갖고 있고, 헤더 압축 및 확장 헤더 디자인을 통해 미래 기능 추가에 유연하게 대응합니다.</p>
  </li>
  <li>
    <p><strong>User-Space에서 구현(OS 커널이 아닌, App으로 구현되어 있음)</strong><br />
전통적인 TCP/IP 스택처럼 운영체제 커널 내부가 아니라, 애플리케이션 라이브러리나 프로세스 레벨의 코드로 동작합니다.<br />
덕분에 플랫폼에 독립적이며,  빠른 배포 및 업데이트가 가능합니다.</p>
  </li>
</ul>

<h4 id="언제-quic를-사용하면-되나요">언제 QUIC를 사용하면 되나요?</h4>

<ul>
  <li>높은 지연 민감도를 갖고 있을때: 초기 페이지 로드나 모바일 환경 전환 시 0-RTT로 빠른 연결할 수 있습니다.</li>
  <li>멀티플렉싱이 필요할 때: HTTP/3으로 대량의 작은 리소스를 동시에 전송할 때 헤드-오브-라인 차단 없이 처리</li>
  <li>네트워크 이주 환경이 필요할 때: 모바일 클라이언트의 IP 변경에도 세션 유지가 필요할 때</li>
</ul>

<p><br /></p>

<h3 id="tcp와-udp-비교">TCP와 UDP 비교</h3>

<table>
  <thead>
    <tr>
      <th><strong>구분</strong></th>
      <th><strong>TCP</strong></th>
      <th><strong>UDP</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>연결</td>
      <td>연결형 (3-way handshake)</td>
      <td>비연결형</td>
    </tr>
    <tr>
      <td>신뢰성</td>
      <td>신뢰성 (ACK, 재전송, 순서 보장)</td>
      <td>비신뢰성 (손실·순서 뒤바뀜 가능)</td>
    </tr>
    <tr>
      <td>흐름·혼잡 제어</td>
      <td>있음 (슬라이딩 윈도우, 혼잡 제어)</td>
      <td>없음</td>
    </tr>
    <tr>
      <td>헤더 오버헤드</td>
      <td>최소 20바이트 (+옵션)</td>
      <td>8바이트</td>
    </tr>
    <tr>
      <td>전송 속도</td>
      <td>상대적으로 느림 (오버헤드·제어 기능)</td>
      <td>매우 빠르고 지연 낮음</td>
    </tr>
  </tbody>
</table>

<p><br />
<br /></p>

<h2 id="application-layerlayer-7">Application Layer(Layer 7)</h2>

<p>‘Application Layer’는 대부분의 개발자가 가장 많은 시간을 보내는 layer입니다.<br />
전송계층(Transport Layer)의 최상단에서, 어플리케이션이 통신하는 방법을 정의합니다.</p>

<blockquote class="prompt-info">
  <p>보통 Application Layer는 ‘User-space(사용자 공간)’에서 처리되는 반면에, 그 아래에 있는 Layer는 OS Kernel에서 처리됩니다.<br />
덕분에, Application Layer는 더 유연하고, 수정이 쉽습니다. 하위계층은 변경이 어렵지만, 그 처리능력이 매우 효율적입니다.</p>
</blockquote>

<h3 id="httphypertext-transfer-protocol">HTTP(Hypertext Transfer Protocol)</h3>

<p>Web에서 Data를 교환하는데 사용하는 표준 Protocol입니다.<br />
‘Stateless Protocol’로서, 각각의 request가 독립적(서버가 이전 request를 알 필요가 없다)으로 작동합니다.<br />
대부분의 System Design에서 유용하게 사용됩니다.<br />
이런 부분이, ‘System Design’관점에서는 System의 표면(surface area)를 최소화할 수 있는 장점이 있습니다.</p>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2012.png" alt="HTTP Message 예시" />
<em>HTTP Message 예시</em></p>

<h4 id="key-concepts">Key Concepts</h4>

<p>HTTP는 다음과 같은 요소들로 구성됩니다.</p>
<ul>
  <li><strong>Request methods</strong>: GET, POST, PUT, DELETE, etc.</li>
  <li><strong>Status codes</strong>: 200 OK, 404 Not Found, 500 Server Error, etc.</li>
  <li><strong>Headers</strong>: Request나 Response의 Metadata역할을 합니다.</li>
  <li><strong>Body</strong>: 전송되는 실제 데이터가 위치하는 곳 입니다.</li>
</ul>

<h4 id="http-content-negotiation">HTTP Content-negotiation</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2013.png" alt="Content-negotiation 메커니즘" />
<em>Content-negotiation 메커니즘</em></p>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2014.png" alt="Agent-driven negotiation 메커니즘" />
<em>Agent-driven negotiation 메커니즘</em></p>

<p>클라이언트가 원하는 표현 방식(미디어 타입·언어·인코딩·캐릭터 셋 등)을 명시하면, 서버가 그중 가장 적합한 표현을 골라 응답하는 메커니즘입니다.</p>

<p><br />
협상(negotiation)을 위해, HTTP Header특정 Field를 사용합니다.</p>

<table>
  <thead>
    <tr>
      <th><strong>협상 카테고리</strong></th>
      <th><strong>요청 헤더</strong></th>
      <th><strong>예시 값</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>미디어 타입</strong></td>
      <td>Accept</td>
      <td>text/html, application/json;q=0.8, <em>/</em>;q=0.1</td>
    </tr>
    <tr>
      <td><strong>언어</strong></td>
      <td>Accept-Language</td>
      <td>ko-KR, en-US;q=0.7, en;q=0.5</td>
    </tr>
    <tr>
      <td><strong>인코딩</strong></td>
      <td>Accept-Encoding</td>
      <td>gzip, deflate, br</td>
    </tr>
    <tr>
      <td><strong>캐릭터 셋</strong></td>
      <td>Accept-Charset</td>
      <td>utf-8, iso-8859-1;q=0.5</td>
    </tr>
  </tbody>
</table>

<h4 id="http-versions">HTTP Versions</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2015.png" alt="다양한 HTTP version과 그 구조." />
<em>다양한 HTTP version과 그 구조.</em></p>

<p><br /></p>

<h3 id="httpshypertext-transfer-protocol-secure">HTTPS(Hypertext Transfer Protocol Secure)</h3>

<p>HTTP의 Secure Protocol입니다. Client와 Server간 데이터를 암호화된 상태로 주고 받는 프로토콜 입니다.<br />
HTTPS는 ‘Handshake’과정을 통해 작동되며, 크게 <strong>2가지 방식의 암호화 방식</strong>을 각 목적에 따라 다르게 사용합니다.</p>

<ul>
  <li>‘Handshake 과정’: 비대칭키(Asymmetric encryption, 공개키) 암호화 방식을 사용하여, ‘데이터 교환’과정에서 사용할 대칭 Key를 만들어냅니다.</li>
  <li>‘데이터 교환 과정’: 대칭키(Symmetric encryption) 암호화 방식을 사용합니다.</li>
</ul>

<h4 id="tls-handshake">TLS Handshake</h4>

<p>![TLS Handshake full flow - from <a href="https://en.wikipedia.org/wiki/File:Full_TLS_1.2_Handshake.svg">Wikipedia</a>](/assets/img/for-post/Networking%20Essentials/Full_TLS_1.2<em>Handshake.png)
_TLS Handshake full flow - from <a href="https://en.wikipedia.org/wiki/File:Full_TLS_1.2_Handshake.svg">Wikipedia</a></em></p>

<p>TCP Connection을 생성한 후에, TLS Handshake가 추가로 이루어집니다.(HTTP/2 이하에서, HTTP/3에선 QUIC와 결합)</p>

<h4 id="http와-https의-용도-구분">HTTP와 HTTPS의 용도 구분</h4>

<ul>
  <li>
    <p>HTTP<br />
내부 네트워크, 성능 테스트, <strong>암호화가 필요 없는</strong> 간단 API 개발 등으로 사용됩니다.</p>
  </li>
  <li>
    <p>HTTPS<br />
사용자 개인정보, 로그인·결제 페이지, API 토큰 전송, SEO 최적화, 브라우저 보안 정책 준수하기 위해 사용됩니다.</p>
  </li>
</ul>

<h3 id="restrepresentational-state-transfer-api">REST(Representational State Transfer) API</h3>

<dl>
  <dt>REST는</dt>
  <dd><strong>웹의 특성(HTTP, URI, 미디어 타입 등)을 최대한 활용</strong>해 <strong>리소스 중심의 API</strong>를 설계하는 아키텍처 스타일입니다.</dd>
  <dd>API를 디자인하는 패러다임(Paradigm)중 하나입니다. 가장 흔하게 사용되는 패러다임입니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>REST의 핵심 원리는, ‘Client가 resource중심으로 작업을 수행하는 경우가 많다’는 것입니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>RESTful API Design에서, 가장 선행되어야 하는것은 ‘리소스(resource)와 그 작동(operation)을 모델링하는 것’입니다.</p>
</blockquote>

<blockquote class="prompt-info">
  <p>REST를 ‘RESTful HTTP’로서 HTTP기반의 아키텍쳐로 많이 쓰이지만, HTTP가 필수조건은 아닙니다.
REST의 제약들은 이론상 HTTP뿐 아니라 SMTP, CoAP, 심지어 메시지 큐 프로토콜 위에도 적용될 수 있습니다.</p>
</blockquote>

<h4 id="주요-개념과-설계-원칙">주요 개념과 설계 원칙</h4>

<h5 id="리소스resource-중심으로-설계합니다"><strong>리소스(Resource) 중심으로 설계합니다.</strong></h5>

<p>URI를 통해 모든 리소스를 고유하게 식별할 수 있어야 합니다.<br />
이때, 데이터 모델 객체(예: User, Order, Product 등)는 ‘명사’로 URI에 표현합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>GET   /users            → 전체 사용자 리소스 컬렉션
GET   /users/123        → ID=123 사용자의 리소스
POST  /users            → 새 사용자 생성
PUT   /users/123        → ID=123 사용자 전체 교체
PATCH /users/123        → ID=123 사용자 일부 수정
DELETE /users/123       → ID=123 사용자 삭제
</pre></td></tr></tbody></table></code></pre></div></div>

<h5 id="http-methodverb의-의미를-담고-있는에-따른-행위-표현을-사용합니다"><strong>HTTP Method(Verb의 의미를 담고 있는)에 따른 ‘행위 표현’을 사용합니다.</strong></h5>

<table>
  <thead>
    <tr>
      <th><strong>메서드</strong></th>
      <th><strong>의미</strong></th>
      <th><strong>멱등성(Idempotent)</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GET</td>
      <td>리소스 조회</td>
      <td>예 (여러 번 호출해도 결과 동일)</td>
    </tr>
    <tr>
      <td>POST</td>
      <td>리소스 생성 혹은 비멱등 작업 수행</td>
      <td>아니요</td>
    </tr>
    <tr>
      <td>PUT</td>
      <td>리소스 전체 갱신(교체, upsert)</td>
      <td>예</td>
    </tr>
    <tr>
      <td>PATCH</td>
      <td>리소스 일부 갱신</td>
      <td>아니요 (설계 따라 다름)</td>
    </tr>
    <tr>
      <td>DELETE</td>
      <td>리소스 삭제</td>
      <td>예</td>
    </tr>
    <tr>
      <td>OPTIONS</td>
      <td>지원 가능한 메서드 조회</td>
      <td>예</td>
    </tr>
    <tr>
      <td>HEAD</td>
      <td>GET과 동일하지만 응답 본문 없이 헤더만 반환</td>
      <td>예</td>
    </tr>
  </tbody>
</table>

<h5 id="statelessness상태를-저장하지-않습니다"><strong>Statelessness(상태를 저장하지 않습니다.)</strong></h5>

<p>Server는 Client의 상태를 저장하지 않습니다.<br />
각 요청(Request)는 독립적으로 처리되어야 하며, 각 요청에 필요한 모든 정보를 담아 보내야 합니다.</p>

<blockquote class="prompt-info">
  <p>이는 서버 확장성과 신뢰성을 높여주는 핵심적인 원칙입니다.
각 요청이 독립적이라 서비스 구조가 단순해지고, ‘서버 확장’을 고려할때 제약사항이 많이 줄어듭니다.</p>
</blockquote>

<p><br /></p>
<h4 id="언제-사용하나요">언제 사용하나요?</h4>
<p>REST는 인터뷰에서 기본적으로 사용하기에 좋습니다. 잘 알려져 있고, 확장가능한 시스템을 구축할때에 좋은 도구가 됩니다.</p>

<blockquote class="prompt-info">
  <p>하지만, REST에서 데이터 형식으로 사용하는 JSON은 Serialization관련 작업이 매우 비효율적입니다. 이는 대부분의 어플리케이션에서 문제가 없지만(bottleneck현상), 관련 문제가 있다면, 다른 도구(gRPC)를 고려하는게 좋습니다.</p>
</blockquote>

<p><br /></p>

<h3 id="graphql">GraphQL</h3>

<dl>
  <dt>GraphQL은</dt>
  <dd>Facebook에서 개발하여 2015년에 오픈소스로 공개한 API 쿼리 언어이자 런타임으로, 비교적 최신의 패러다임(paradigm)입니다.</dd>
  <dd>Client로 하여금, 정확하게 자신이 원하는 데이터를 요청할 수 있게 합니다.</dd>
</dl>

<p>기존 RESTful API가 가진 여러 한계를 극복하고, 클라이언트 개발자·서버 개발자 모두의 생산성을 높여 주기 위해 고안된 쿼리 언어이자 런타임입니다.</p>

<h4 id="rest의-under-fetching과-over-fetching-문제">REST의 Under-fetching과 Over-fetching 문제</h4>

<h5 id="under-fetching"><strong>Under-fetching</strong></h5>
<p><img src="/assets/img/for-post/Networking%20Essentials/image%2016.png" alt="REST의 under-fetching문제와 GraphQL 비교" />
<em>REST의 under-fetching문제와 GraphQL 비교</em></p>

<p>필요한 데이터가 여러 Endpoint에 흩어져 있어서, 여러 Endpoint를 호출해야하는 상황을 말합니다.<br />
예를 들면, /users/123로 받은 응답에는 포스트 목록이 없어서, /users/123/posts를 별도로 호출해야 할 때가 많습니다.<br />
GraphQL에서는, 하나의 요청에 여러 Entity에 대한 Query를 날릴 수 있습니다.</p>

<h5 id="over-fetching"><strong>Over-fetching</strong></h5>

<p><img src="/assets/img/for-post/Networking%20Essentials/over-fetching.png" alt="over-fetching 상황." />
<em>over-fetching 상황.</em></p>

<p>불필요한 데이터까지 포함하여, 데이터를 과다 수신하는 문제를 말합니다.<br />
만약 Under-fetching문제를 해결하기 위해, End-point를 통합하게 된다면 불필요한 데이터 포함될 가능성이 높아집니다.<br />
<br />
또다른 예시로,
REST API에서 /users/123 같은 엔드포인트가 항상 유저의 모든 속성(이메일, 주소, 프로필 사진 URL, 가입일 등)을 반환할 때, 화면에 이름·아이디만 필요해도 불필요한 데이터를 전부 내려받아야 합니다.</p>

<p><br />
GraphQL에서는, <strong>Client에서 주도적으로</strong> Response데이터의 Filtering을 요청하여, 정확하게 필요한 데이터만 받아낼 수 있습니다.</p>

<p><br /></p>

<h4 id="언제-사용하나요-1">언제 사용하나요?</h4>

<ul>
  <li>
    <p>Frontend팀이 빠르게 iteration할때에 적합합니다.<br />
Frontend가 주도적으로 쿼리를 할 수 있기 때문에.</p>
  </li>
  <li>
    <p>클라이언트마다 서로 다른 데이터 요구사항이 있을 때</p>
  </li>
</ul>

<dl>
  <dt>하지만, 이런 구조(GraphQL)는</dt>
  <dd>Backend에 Latency와 Complexity를 높이게 됩니다.</dd>
  <dd>Cache를 CDN Level에서 적극적으로 사용해야 할때는 적합하지지 않습니다.(RESTful 디자인이 Cache에 더 직관적)</dd>
</dl>

<p><br /></p>

<h3 id="grpcgoogle-remote-procedure-call">gRPC(google Remote Procedure Call)</h3>

<dl>
  <dt>gRPC는</dt>
  <dd>Google이 개발한 오픈소스 원격 프로시저 호출(Remote Procedure Call, RPC) 프레임워크입니다.</dd>
  <dd>특히 마이크로서비스 환경에서 <strong>높은 성능</strong>과 명확한 인터페이스 계약을 제공합니다.</dd>
  <dd>HTTP/2와 Protobuf(Protocol Buffers)를 사용합니다.</dd>
</dl>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2017.png" alt="gRPC를 하나의 이미지로 요약. | from [bytebytego.com](https://bytebytego.com/guides/what-is-grpc/)" />
<em>gRPC를 하나의 이미지로 요약. | from <a href="https://bytebytego.com/guides/what-is-grpc/">bytebytego.com</a></em></p>

<h4 id="protobufprotocol-buffers">Protobuf(Protocol Buffers)</h4>

<p>Protocol Buffers(이하 protobuf)는 JSON과 같이 데이터를 Serializing(직렬화)하는 방법중에 하나입니다.</p>

<div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kd">message</span> <span class="nc">User</span> <span class="p">{</span>
  <span class="kt">string</span> <span class="na">id</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// 0A 03 31 32 33 12 08 6A 6F 68 6E 20 64 6F 65 // binary encoding된 데이터</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>JSON은 사람이 이해하기 쉽지만, 컴퓨터 입장에서는 연산(Serializing에 관련된)에 비효율이 있습니다.<br />
Protobuf는 데이터를 binary로 encoding하여 외부 시스템과 교환합니다.</p>

<h4 id="rest와-grpc-작동방식-비교">REST와 gRPC 작동방식 비교</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2018.png" alt="REST방식과 gRPC방식의 차이 | from [https://refine.dev/blog/grpc-vs-rest/#step-3-1](https://refine.dev/blog/grpc-vs-rest/#step-3-1)" />
<em>REST방식과 gRPC방식의 차이 | from <a href="https://refine.dev/blog/grpc-vs-rest/#step-3-1">https://refine.dev/blog/grpc-vs-rest/#step-3-1</a></em></p>

<p>gRPC는 REST와 달리 함수를 호출하듯 인터페이스를 설계·사용합니다.</p>

<h5 id="http를-기반으로하는-rest와-grpc비교">HTTP를 기반으로하는 REST와 gRPC비교</h5>
<p>gRPC도 HTTP/2를 기반으로 작동하는 프로토콜이기 때문에, REST와 비교하며, 내부적으로 어떤 HTTP요청을 보내는지 비교하고자 합니다.</p>

<ul>
  <li>REST Over HTTP
    <div class="language-http highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="nf">POST</span> <span class="nn">/users</span> <span class="k">HTTP</span><span class="o">/</span><span class="m">1.1</span>
<span class="na">Content-Type</span><span class="p">:</span> <span class="s">application/json</span>

<span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Alice"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
  <li>gRPC
    <div class="language-http highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="nf">POST</span> <span class="nn">/myapp.UserService/CreateUser</span> <span class="k">HTTP</span><span class="o">/</span><span class="m">2</span>
<span class="na">Content-Type</span><span class="p">:</span> <span class="s">application/grpc</span>
<span class="s">  </span>
<span class="s">(binary payload - protobuf encoded)</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ul>

<h4 id="grpc의-내장기능-client-side-load-balancingcslb">gRPC의 내장기능. Client-side Load Balancing(CSLB)</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2019.png" alt="Client Load Balancing 구조 | from [grpc.io](https://grpc.io/blog/grpc-load-balancing/)" />
<em>Client Load Balancing 구조 | from <a href="https://grpc.io/blog/grpc-load-balancing/">grpc.io</a></em></p>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2020.png" alt="Server에 대한 정보를 미리 Discovery하고 저장하는 Loolaside LB가 있는 CSLB구조." />
<em>Server에 대한 정보를 미리 Discovery하고 저장하는 Loolaside LB가 있는 CSLB구조.</em></p>

<p><strong>클라이언트가 호출할 서버 인스턴스를 직접 선택</strong>해 분산시키는 메커니즘입니다.<br />
서버 풀을 중앙에서 관리하는 대신, 클라이언트가 가져온 엔드포인트 목록을 기반으로 <strong>round-robin, pick_first</strong> 같은 정책을 적용하거나, Envoy xDS·Google grpclb 같은 외부 컨트롤 플레인과 연동할 수 있습니다.</p>

<p><br /></p>

<h4 id="언제-사용하나요-2">언제 사용하나요?</h4>

<dl>
  <dt>gRPC는</dt>
  <dd>Strong type(강타입)특성을 통해, 에러를 사전에 검출하고</dd>
  <dd>Binary protocol을 통해 ‘JSON over HTTP’보다 훨씬 더 나은(어쩔때는 10배 이상)의 성능을 제공합니다.</dd>
  <dd>서버와 클라이언트가 교환하는 데이터가 Binary라, 디버깅이 어렵습니다.</dd>
</dl>

<p>때문에, gRPC는 <strong>MSA(Micro Service Architecture)에서 내부(internal) 서비스간 통신</strong>에서 큰 힘을 발휘합니다.</p>

<p><img src="/assets/img/for-post/Networking%20Essentials/grpc.png" alt="Example of Using gRPC for Internal APIs, and REST and HTTP for External" />
<em>Example of Using gRPC for Internal APIs, and REST and HTTP for External</em></p>

<p>Internal Service간에는 gRPC로 구성하여 성능을 높이고, public-facing APIs(외부로 노출되는 API)는 REST를 사용해 범용성 및 호환성을 높이는 구성으로 사용합니다.</p>

<blockquote class="prompt-info">
  <p>System Design Interview에서는 내·외부모두 REST로 구성해도 괜찮습니다. 다만, Interviewer나 과제 자체가 특별한 요구사항이 있다면 고려해봐야 합니다.</p>
</blockquote>

<p><br /></p>

<h3 id="sseserver-sents-events">SSE(Server-Sents Events)</h3>

<p>보통의 request/response 스타일의 API와는 다르게, Server가 Client에 데이터를 ‘push’하는 프로토콜 입니다.<br />
서버가 클라이언트(브라우저)로 <strong>단방향(Stream)</strong>으로 실시간 이벤트(데이터)를 푸시하는 기술이며,<br />
<strong>TCP연결을 전제</strong>로 하기 때문에, HTTP/1.1 혹은 HTTP/2 위에서 동작합니다.</p>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2021.png" alt="SSE Flow with Gateway" />
<em>SSE Flow with Gateway</em></p>

<h4 id="작동-방식">작동 방식</h4>

<p>Client에서 ‘Event Stream’을 생성하기 위한 요청을 보내고, 서버에서는 해당 Stream을 계속 유지하여 Response를 보냅니다.</p>

<ol>
  <li>클라이언트 연결(EventSource 생성)<br />
브라우져(혹은 JS환경)에서 EventSource 객체를 만듭니다.
    <div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre> <span class="kd">const</span> <span class="nx">es</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">EventSource</span><span class="p">(</span><span class="dl">'</span><span class="s1">/events</span><span class="dl">'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>내부적으로는, 아래의 과정을 거치게 됩니다.</p>
    <ul>
      <li>GET /events HTTP/1.1 요청</li>
      <li>헤더에 Accept: text/event-stream 자동 추가</li>
      <li>TCP 3-way 핸드셰이크 → TLS(HTTPS인 경우) → HTTP 요청</li>
    </ul>
  </li>
  <li>서버 응답(스트리밍 시작)<br />
서버는 HTTP Response를 끊임없이 유지합니다.
    <div class="language-http highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="k">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="m">200</span> <span class="ne">OK</span>
<span class="na">Content-Type</span><span class="p">:</span> <span class="s">text/event-stream</span>
<span class="na">Cache-Control</span><span class="p">:</span> <span class="s">no-cache</span>
<span class="na">Connection</span><span class="p">:</span> <span class="s">keep-alive</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>이어서 텍스트 프레임을 차례로 전송합니다.</p>
    <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>data: 첫 번째 메시지 내용\n
\n
data: 두 번째 메시지 내용\n
id: 42\n
event: customEvent\n
\n
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <ul>
      <li>data: 한 줄에 메시지 내용을 기록</li>
      <li>빈 줄(\n\n)이 하나의 <strong>이벤트 단위(Record)</strong> 를 구분</li>
      <li>id: 로 이벤트 식별자 지정 → 재연결 시 Last-Event-ID 헤더로 이어받기</li>
      <li>event: 로 커스텀 이벤트 이름 지정(클라이언트 addEventListener(‘customEvent’,…))</li>
      <li>retry: 으로 재연결 대기(ms) 권고 가능</li>
    </ul>
  </li>
  <li>클라이언트에서 이벤트 처리
    <div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre> <span class="c1">// 기본 Event 처리 방식</span>
 <span class="nx">es</span><span class="p">.</span><span class="nx">onmessage</span> <span class="o">=</span> <span class="nx">e</span> <span class="o">=&gt;</span> <span class="p">{</span>
   <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">이벤트 데이터:</span><span class="dl">"</span><span class="p">,</span> <span class="nx">e</span><span class="p">.</span><span class="nx">data</span><span class="p">);</span>
 <span class="p">};</span>
    
 <span class="c1">// 혹은 커스텀 이벤트 Lister를 통해 처리</span>
 <span class="nx">es</span><span class="p">.</span><span class="nf">addEventListener</span><span class="p">(</span><span class="dl">'</span><span class="s1">customEvent</span><span class="dl">'</span><span class="p">,</span> <span class="nx">e</span> <span class="o">=&gt;</span> <span class="p">{</span>
   <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">customEvent:</span><span class="dl">"</span><span class="p">,</span> <span class="nx">e</span><span class="p">.</span><span class="nx">data</span><span class="p">);</span>
 <span class="p">});</span>
    
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ol>

<p><br /></p>

<h4 id="언제-사용하나요-3">언제 사용하나요?</h4>

<p>Client에게 Notification이나 Event를 보내야 할때 사용합니다.</p>
<ul>
  <li>주식·환율 시세 업데이트</li>
  <li>라이브 스포츠 스코어보드</li>
  <li>로그 스트리밍 및 모니터링</li>
  <li>실시간 피드(뉴스·소셜 피드)</li>
  <li>실시간 알림(Notifications)</li>
</ul>

<p><br /></p>

<h3 id="wswebsockets-wsswebsockets-secure">WS(WebSockets), WSS(WebSockets Secure)</h3>

<dl>
  <dt>WS는</dt>
  <dd>웹 환경에서 <strong>양방향(full-duplex) 실시간 통신</strong>을 가능하게 해 주는 프로토콜로, HTTP의 한계를 넘어 클라이언트와 서버가 자유롭게 메시지를 주고받을 수 있도록 설계되었습니다.</dd>
  <dd>TCP 기반으로 동작합니다.</dd>
  <dd>연결을 계속 유지(Keep-alive)하여 <strong>실시간 이벤트</strong>를 전달할 때에 적합합니다.</dd>
</dl>

<h4 id="연결-과정">연결 과정</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2022.png" alt="WebSocket over TCP Sequence diagram | from researchgate.net" class="w-50" />
<em>WebSocket over TCP Sequence diagram | from <a href="https://www.researchgate.net/figure/Websocket-over-TCP-sequence-diagram_fig2_340258624">researchgate.net</a></em></p>

<ol>
  <li>Client 요청<br />
Client에서 TCP연결을 생성하고, HTTP요청을 통해, HTTP연결을 Websocket으로  전환하는 요청을 보냅니다.
    <div class="language-http highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="nf">GET</span> <span class="nn">/chat</span> <span class="k">HTTP</span><span class="o">/</span><span class="m">1.1</span>
<span class="na">Host</span><span class="p">:</span> <span class="s">example.com</span>
<span class="na">Upgrade</span><span class="p">:</span> <span class="s">websocket</span>
<span class="na">Connection</span><span class="p">:</span> <span class="s">Upgrade</span>
<span class="na">Sec-WebSocket-Key</span><span class="p">:</span> <span class="s">&lt;base64-encoded nonce&gt;</span>
<span class="na">Sec-WebSocket-Version</span><span class="p">:</span> <span class="s">13</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
  <li>Server 응답<br />
HTTP를 WebSocket으로 전환하는 것을 승인합니다.
    <div class="language-http highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="k">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="m">101</span> <span class="ne">Switching Protocols</span>
<span class="na">Upgrade</span><span class="p">:</span> <span class="s">websocket</span>
<span class="na">Connection</span><span class="p">:</span> <span class="s">Upgrade</span>
<span class="na">Sec-WebSocket-Accept</span><span class="p">:</span> <span class="s">&lt;SHA1&amp;base64 of nonce&gt;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ol>

<h4 id="언제-사용하나요-4">언제 사용하나요?</h4>

<p><strong>high-frequency</strong>, <strong>persistent</strong>, <strong>bi-directional</strong> communication이 필요할 때 사용합니다.</p>
<ul>
  <li>채팅 어플리케이션</li>
  <li>실시간 협업 툴</li>
  <li>온라인 멀티플레이 게임</li>
  <li>라이브 대시보드</li>
  <li>경매(옥션), 스포츠 베팅 등 실시간 입찰/배팅</li>
</ul>

<blockquote class="prompt-info">
  <p>System Design Interview에서는, WebSocket을 사용하기 전에 반드시 “왜 필요한지”설명해야 합니다.<br />
WebSocket은 강력한 기능을 제공하지만, 이를 지원하고 유지하는 인프라 비용이 많이 들고, 특히 대규모 연결 시 상태 저장(stateful) 연결의 오버헤드로 인해 설계에 상당한 조정이 필요할 수 있습니다.</p>
</blockquote>

<p><br /></p>

<h3 id="webrtcweb-real-time-communication">WebRTC(Web Real-Time Communication)</h3>

<dl>
  <dt>WebRTC는</dt>
  <dd>브라우저나 애플리케이션 간에 <strong>네이티브 P2P(peer-to-peer) 오디오·비디오·데이터 스트림</strong>을 전달하기 위한 오픈 웹 표준입니다.</dd>
  <dd>별도의 플러그인 없이, 자바스크립트 API와 브라우저 내장 기능만으로 실시간 통신 기능을 구현할 수 있습니다.</dd>
  <dd>다른 프로토콜과 다르게 UDP기반입니다.</dd>
</dl>

<h4 id="webrtc의-구조-및-컴포넌트">WebRTC의 구조 및 컴포넌트</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2023.png" alt="WebRTC 구조" />
<em>WebRTC 구조</em></p>

<p>대부분의 Client는 inbound connection을 허용하지 않고(보안 문제로), NAT뒤에 있기 때문에, Client끼리 직접 연결하는것은 어려운 과제입니다.<br />
WebRTC는 여러 컴포넌트를 통해 이를 극복합니다.</p>

<ul>
  <li>
    <p>Signaling Server (신호 중계 서버)<br />
피어 간에 SDP(Session Description Protocol) 와 ICE 후보(candidate) 를 교환하기 위한 임시 채널입니다. 즉, 두 Client사이의 연결을 만드는 <strong>초기 협상단계에 사용</strong>합니다.<br />
단, 미디어·데이터 자체는 여기서 통과하지 않습니다.</p>
  </li>
  <li>
    <p>STUN(Session Traversal Utilities for NAT)<br />
“내가 밖에서 어떻게 보이는지”(public IP, NAT 매핑)를 알려줍니다.<br />
이를 통해, Peer-to-peer 연결을 가능하게 합니다.</p>
  </li>
  <li>
    <p>TURN(Traversal Using Relays around NAT)<br />
피어 간 직접 연결(hole punching) 실패 시, 중계(relay)로 동작합니다.</p>
  </li>
</ul>

<h4 id="webrtc-connection-수립-4단계">WebRTC Connection 수립 4단계</h4>

<ol>
  <li>Client는 Signaling Server에 연결하여 peer에 대한 정보를 얻습니다.</li>
  <li>Client는 STUN 서버에 접속하여 공용 IP 주소와 포트를 얻습니다.</li>
  <li>Client는 Signaling Server를 통해 이 정보를 서로 공유합니다.</li>
  <li>Client는 직접 P2P 연결을 설정하고 데이터 전송을 시작합니다.</li>
</ol>

<h4 id="webrtc의-기반이-되는-udp-hole-punching">WebRTC의 기반이 되는 UDP Hole Punching</h4>
<p>UDP 홀 펀칭(UDP Hole Punching)은 양쪽이 NAT 뒤에 있어도 서로 직접 UDP 패킷을 주고받을 수 있게 해 주는 대표적인 NAT 트래버설(Traversal) 기법입니다.<br />
<img src="/assets/img/for-post/Networking%20Essentials/image%2024.png" alt="UDP Multi Hole Punching" />
<em>UDP Multi Hole Punching</em></p>

<blockquote class="prompt-info">
  <p>Q: 필요한 이유?<br />
A: 대부분의 홈 라우터나 기업 방화벽은 <strong>사설 IP → 인터넷</strong> 방향(아웃바운드) 패킷만 허용하고,<br />
<strong>인터넷 → 사설 IP</strong>(인바운드)는 기본적으로 차단합니다.<br />
따라서 NAT 뒤에 있는 두 호스트 A, B가 서로 통신하려면, NAT 장비의 “구멍(hole)”을 동시에 뚫어줘야 합니다.</p>
</blockquote>

<h4 id="언제-사용하나요-5">언제 사용하나요?</h4>

<p>WebRTC는 <strong>브라우저나 애플리케이션 간에 플러그인 없이</strong> 직접 P2P(피어투피어)로 오디오·비디오·데이터를 주고받아야 할 때 사용합니다.</p>

<ul>
  <li>실시간 화상·음성 통화(Voice/Video Chat)</li>
  <li>라이브 스트리밍 &amp; 브로드캐스트</li>
  <li>파일 전송 &amp; 데이터 교환</li>
  <li>스크린 공유 &amp; 원격 제어</li>
  <li>실시간 협업 도구
    <blockquote class="prompt-info">
      <p>실시간 협업 도구의 결과물을 저장하기 위해, CRDT(Conflict-free replicated data type)을 함께 사용하기도 합니다.</p>
    </blockquote>
  </li>
  <li>실시간 게임 &amp; IoT 제어</li>
</ul>

<p><br />
<br /></p>

<h2 id="load-balancing">Load Balancing</h2>

<p>서비스가 성장하고 트래픽이 기하급수적으로 늘어나면서, System을 설계할때, ‘Scalability를 고려한 설계’가 중요한 부분중 하나가 되었습니다.<br />
여기서는 Scaling의 종류와 동적으로 생성된 서버와 연결하기 위한 ‘Load Balancer’에 대해서 알아봅니다.</p>

<h3 id="scaling">Scaling</h3>

<p>Scaling에는 2가지 옵션이 있습니다.</p>
<ul>
  <li>더 성능이 좋은 서버(Vertical Scaling)</li>
  <li>더 많은 서버(Horizontal Scaling)</li>
</ul>

<p><img src="/assets/img/for-post/Networking%20Essentials/scale-option.png" alt="Vertical vs Horizontal Scaling" />
<em>Vertical vs Horizontal Scaling</em></p>

<p>항상 ‘Horizontal Scaling’만이 답이 아니며, 상황에 따라 적절하게 예측하여 Scaling하는게 중요합니다.</p>

<blockquote class="prompt-info">
  <p>실제 Interview에서는 ‘Horizontal Scaling’에 대한 시나리오가 더 자주 나옵니다.</p>
</blockquote>

<p><br /></p>

<h3 id="client-side-load-balancingcslb">Client-Side Load Balancing(CSLB)</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2020.png" alt="Server에 대한 정보를 미리 Discovery하고 저장하는 Loolaside LB가 있는 CSLB구조." />
<em>Server에 대한 정보를 미리 Discovery하고 저장하는 Loolaside LB가 있는 CSLB구조.</em></p>

<dl>
  <dt><strong>CSLB에서</strong> Client는</dt>
  <dd>스스로 어떤 서버에 연결할지 선택하게 됩니다.</dd>
  <dd>보통, 서버 목록을 요청해서 Client에서 들고 있다가, 목록의 서버중 하나에 요청을 보냅니다.</dd>
  <dd>Server에 변화가 있는경우를 대비해, 주기적으로 polling하거나 update를 Push 받아야 합니다.</dd>
  <dt>CSLB는</dt>
  <dd>Client입장에서 가장 빠른 서버를 선택할 수 있습니다.</dd>
  <dd>중간에 요청 자체를 라우팅해주는 역할이 없다보니, 매우 효율적이고 빠릅니다.</dd>
</dl>

<h4 id="cslb의-examples">CSLB의 Examples</h4>

<ul>
  <li>
    <p>Redis Cluster<br />
<img src="/assets/img/for-post/Networking%20Essentials/image%2025.png" alt="Redis Cluster에서 Key에 따른 Client Load Balancing | from [aeraki.net](https://www.aeraki.net/docs/v1.x/tutorials/redis/cluster/)" />
<em>Redis Cluster에서 Key에 따른 Client Load Balancing | from <a href="https://www.aeraki.net/docs/v1.x/tutorials/redis/cluster/">aeraki.net</a></em><br />
Redis Cluster에서는 Key를 Hash한 결과를 통해, 어떤 shard(여러 Node에 분산되어 있는 데이터)에 데이터가 포함되어 있는지 구분하고, 해당 노드로 요청을 보냅니다.</p>
  </li>
  <li>
    <p>DNS<br />
Domain을 Resolve하는 과정에서, ‘DNS Resolver’가 CSLB를 수행합니다.<br />
만약 아래와 같이 <code class="language-plaintext highlighter-rouge">service.example.com</code> 에 대한 A/AAAA 다중 레코드를 구성한다면,</p>
    <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>  service.example.com. 300 IN A 10.0.0.1
  service.example.com. 300 IN A 10.0.0.2
  service.example.com. 300 IN A 10.0.0.3
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>아래와 같이. 요청마다 Record List의 순서가 바뀌어서 응답하게 됩니다.</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>  <span class="nv">$ </span>dig +short service.example.com
  10.0.0.2
  10.0.0.3
  10.0.0.1
    
  <span class="nv">$ </span>dig +short service.example.com
  10.0.0.1
  10.0.0.2
  10.0.0.3
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>이렇게 IP 목록의 순서가 변하면서, CSLB가 작동하게 됩니다.</p>
  </li>
</ul>

<h4 id="언제-사용하나요-6">언제 사용하나요?</h4>

<ul>
  <li>우리가 통제해야하는 Client가 적을때.</li>
  <li>많은 수의 Client가 있어도, ‘slow update(e.g. DNS)’를 견딜 수 있을때.</li>
  <li>주로 internal service간의 통신에서 사용됩니다.</li>
</ul>

<p><br /></p>

<h3 id="dedicated-load-balancers">Dedicated Load Balancers</h3>

<dl>
  <dt>Dedicated Load Balancer는</dt>
  <dd>애플리케이션 서버가 아니라 <strong>오직 트래픽 분산(로드 밸런싱)</strong> 목적으로 배포되는 전용 장치나 서비스입니다.</dd>
  <dd>크게 <strong>하드웨어 어플라이언스</strong>와 <strong>소프트웨어(HAProxy, nginx, Apache)/클라우드 서비스</strong> 형태가 있습니다.</dd>
</dl>

<h4 id="layer-4-load-balancer-이하-nlb-network-load-balancer">Layer 4 Load Balancer (이하 NLB, Network Load Balancer)</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/nlb-diagram.png" alt="Simple HTTP Request with L4 Load Balancer" />
<em>Simple HTTP Request with L4 Load Balancer</em></p>

<dl>
  <dt>NLB는</dt>
  <dd>Transport Layer(TCP/UDP, Layer 4)에서 작동하는 Load Balancer입니다.</dd>
  <dd>Packet의 컨텐츠에 해당하는 부분을 보지 않고, Transport Layer의 도구(IP Addr, Port Num)으로만 Routing을 수행합니다.</dd>
</dl>

<h5 id="특징"><strong>특징</strong></h5>

<ul>
  <li>Client와 Server사이에서 ‘Persistent TCP Connection(KeepAlive으로 알려진 connection)’을 유지합니다.(WebSocket같은)</li>
  <li>빠르고 효율적입니다.(Packet을 최소한으로만 살펴보기 때문에)</li>
  <li>Application Data를 기준으로 Routing 하지 못합니다.</li>
  <li>빠른 성능이 목적일때 사용합니다.</li>
  <li>TCP를 사용하기 때문에, 이를 사용하여 상위 계층에서 통신할 수 있습니다.</li>
</ul>

<blockquote class="prompt-info">
  <p>NLB를 통해, TCP연결이 생성된다면, 해당 연결을 재사용한다면, balancing이 발생하지 않습니다.<br />
즉, Connection을 생성하는 시점에만 Balancing이 발생합니다.</p>

  <p>생성된 Connection을 통해, 요청을 보내면 Connection과 연결된 Server에서만 처리합니다.</p>
</blockquote>

<h5 id="언제-사용하나요-7"><strong>언제 사용하나요?</strong></h5>

<ul>
  <li>WebSocket Connection을 사용할때.</li>
  <li>‘persistent connection’이 필요할때.</li>
  <li>Layer 7 LB보다 너 높은 성능이 필요할때.</li>
</ul>

<blockquote class="prompt-info">
  <p>Interview에서는 WebSocket을 써야하는 경우가 아니라면, L7 LB가 대부분 더 적합합니다.</p>
</blockquote>

<h4 id="layer-7-load-balancer-이하-alb-application-load-balancer">Layer 7 Load Balancer (이하 ALB, Application Load Balancer)</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/alb-diagram.png" alt="Simple HTTP Request with L7 Load Balancer" />
<em>Simple HTTP Request with L7 Load Balancer</em></p>

<dl>
  <dt>ALB는</dt>
  <dd>Application Layer에서 작동하는 load balancer입니다.</dd>
  <dd>Layer 7의 컨텐츠를 직접 들여다 보고, Routing할 수 있습니다.</dd>
</dl>

<h5 id="특징-1"><strong>특징</strong></h5>

<ul>
  <li>
    <p>‘incoming(들어오는) connection’을 끊고, Backend Server와 새로운 connection을 생성합니다.<br />
(Client와의 Connection과 Server의 Connection을 끊어서 작동한다는 뜻)<br />
이를 기반으로 ALB를 통해 통합된 TLS설정을 할 수 있습니다.</p>
  </li>
  <li>request의 내부 컨텐츠(URL, headers, cookies, etcd)를 기반으로 라우팅할 수 있습니다.</li>
  <li>NLB보다 더 CPU Intensive합니다. (Packet에 대해 더 많이 들여다 보기 때문에)</li>
  <li>더 많은 기능과 유연성(라우팅 방법에 대한)을 제공합니다.
    <blockquote class="prompt-info">
      <p>cookie데이터를 기반으로 특정유저가 같은 서버에만 연결되도록 할 수 있습니다.</p>
    </blockquote>
  </li>
  <li>HTTP기반의 traffic에 적합합니다.</li>
</ul>

<h5 id="언제-사용하나요-8"><strong>언제 사용하나요?</strong></h5>

<p>HTTP 기반의 traffic을 다룰때, 대부분의 경우에 적합합니다.(WebSocket은 제외)</p>

<p><br /></p>

<h3 id="health-checks-and-fault-tolerance">Health Checks and Fault Tolerance</h3>
<p>Load Balancer(이하 LB)가 traffic을 분배할때, 이를 완벽하게 수행하기 위해, Backend Server에 대한 모니터링 책임을 갖고 있습니다.</p>

<p><br />
LB는 ‘<strong>Health Check</strong>‘를 통해, 자동화된 failover를 구현합니다.(High availability 제공)<br />
‘Health Check’는 다양한 프로토콜(Protocol)을 사용하여 구성할 수 있으며, 일반적으로는 TCP나 HTTP요청을 사용합니다.</p>

<blockquote class="prompt-info">
  <p>TCP보다는 HTTP가 Application수준의 Availability를 측정하기에 적합하기 때문에, HTTP를 더 많이 사용합니다.</p>
</blockquote>

<p>아래는, AWS ALB Health Check Configuration 예시입니다.</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>Protocol:   HTTP
Path:       /healthz
Port:       traffic port
Interval:   10 seconds
Timeout:    5 seconds
Healthy Threshold:   3
Unhealthy Threshold: 2
Matcher:    200–399
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h3 id="load-balancing-algorithms">Load Balancing Algorithms</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2026.png" alt="Load Balancing 알고리즘 종류 | from blog.bytebytego.com" />
<em>Load Balancing 알고리즘 종류 | from <a href="https://blog.bytebytego.com/p/ep47-common-load-balancing-algorithms">blog.bytebytego.com</a></em></p>

<h3 id="적용">적용</h3>

<p>실제로 적용할때는, 다음과 같은 LB들을 사용합니다.</p>
<ul>
  <li>Hardware Load Balancers: Physical devices like F5 Networks BIG-IP</li>
  <li>Software Load Balancers: HAProxy, NGINX, Envoy</li>
  <li>Cloud Load Balancers: AWS ELB/ALB/NLB, Google Cloud Load Balancing, Azure Load Balancer</li>
</ul>

<blockquote class="prompt-info">
  <p>Interview에서, 만약 LB의 처리량(throughput)이 매우 크다면, Hardware LB를 사용하는걸 언급하는게 좋습니다.</p>
</blockquote>

<p><br />
<br /></p>

<h2 id="regionalization-and-latency">Regionalization and Latency</h2>

<p>글로벌 서비스에서, 서버는 전 세계에 걸쳐 분포되어 있습니다.<br />
이때 필연적으로 발생하는 ‘물리적 거리’ 때문에, 많은 Latency가 만들어집니다.</p>

<blockquote class="prompt-info">
  <p>빛은 진공 상태에서 광속의 약 2/3인 약 200,000km/s로 광섬유 케이블을 통해 이동합니다.
즉, 뉴욕과 런던(약 5,600km)을 왕복하는 데는 신호 전파의 물리적 특성만으로 이론적으로 약 56ms의 지연 시간이 발생합니다. 이러한 물리적 제약 때문에 저지연 애플리케이션에는 지리적 분포가 필수적입니다.</p>
</blockquote>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2027.png" alt="전세계 해저케이블 분포 | from submarinecablemap.com" />
<em>전세계 해저케이블 분포 | from <a href="https://www.submarinecablemap.com/">submarinecablemap.com</a></em></p>

<p>이런 물리적인 한계 때문에, 어느정도의 Latency를 피할 수 없는 부분이 있습니다.<br />
하지만, 이를 개선하기 위한, 몇가지 최적화(optimization) 전략을 소개합니다.</p>

<p><br /></p>

<h3 id="cdnscontent-delivery-networks">CDNs(Content Delivery Networks)</h3>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2028.png" alt="CDN Description | from cloudns.net" />
<em>CDN Description | from <a href="https://www.cloudns.net/blog/cdn-content-delivery-network/">cloudns.net</a></em></p>

<p>전 세계에 분산된 캐시 서버(Edge 서버)를 이용해 사용자에게 가장 가까운 위치에서 정적·동적 콘텐츠를 빠르게 전달하는 서비스입니다.<br />
이는 캐싱(Caching) 덕분에 가능합니다.<br />
데이터가 많이 변경되지 않거나 자주 업데이트될 필요가 없다면, 엣지 서버에 캐싱하여 반환할 수 있습니다. 이는 이미지, 비디오 및 기타 자산과 같은 정적(static) 콘텐츠에 특히 효과적입니다.</p>

<blockquote class="prompt-info">
  <p>Interview에서, CDNs을 자주 보게 될것입니다. 데이터가 Cacheable하고 글로벌 환경에서 빠르게 응답해야 한다면, CDN은 좋은 전략이 될겁니다.</p>
</blockquote>

<p><br /></p>

<h3 id="regional-partitioning">Regional Partitioning</h3>

<p>서비스에 따라, 모든 데이터가 Global단위로 공유될 필요가 없을 수 있습니다. 즉, Regional하게 사용될 수 있습니다.<br />
이를 이용하여, 지역(Region, 인접 도시들을 묶은 개념)별로 데이터센터를 분리하여 운영하면, Latency를 줄이면서 데이터 유지비용도 줄일 수 있습니다.</p>

<p><br />
<br /></p>

<h2 id="handling-failures-and-fault-modes">Handling Failures and Fault Modes</h2>

<p>우리는 종종 ‘server crash’, ‘solar flares(태양 표면 폭발의 전자기파로 인해 디지털 장비가 쟁이를 일으키곤 합니다.)’로 인해, 시스템 장애를 겪곤 합니다.<br />
여기서는, 우리는 이런 ‘시스템의 실패(Failures)를 어떻게 다뤄야 하는지’를 다룹니다.</p>

<blockquote class="prompt-info">
  <p>“네트워크는 안정적이다”라는 생각은, 분산 시스템에서 가장 위험한 가정입니다.<br />
네트워크는 언제든 실패할 수 있으며, 이를 염두해두어야 합니다.</p>
</blockquote>

<p><br /></p>

<h3 id="timeouts-and-retries-with-backoff">Timeouts and Retries with Backoff</h3>

<p>실패를 처리하는 가장 기본적인 방법은, Timeout과 Retry를 사용하는 것입니다.<br />
만약, 요청을 처리하는데에 너무 오래 걸린다면, Timeout처리 이후에 Retry로 다시 시도하면, 이 요청은 성공할 수 있습니다.(특히, 분산처리 환경이라면 더욱)<br />
이 과정에서 <strong>API의 Idempotency(멱등성)</strong>이 필요합니다.</p>

<h4 id="idempotency멱등성">Idempotency(멱등성)</h4>

<p>같은 요청을 여러 번 처리해도 “결과가 처음 한번 처리한 것과 동일” 하도록 보장하는 특성을 말합니다.<br />
즉, Retry를 수행해도, <strong>처음 요청한것과 동일하다는 것이 보장된다</strong>는 얘기입니다.<br />
장애가 발생해도, 이 요청을 다시 보냄으로서 처음 요청과 동일한 결과를 얻을 수 있습니다.</p>

<ul>
  <li>
    <p>읽기 요청(GET)의 Idempotency<br />
GET 요청은 서버 상태를 변경하지 않으므로, 몇 번 호출해도 문제가 없습니다. 이 경우, ‘Idempotency’를 구현하기 쉽습니다.</p>
  </li>
  <li>
    <p>쓰기 요청(POST 등)에서의 Idempotency 보장<br />
‘쓰기 요청’의 Idempotency를 보장하는 것은 더 어렵고 복잡합니다.</p>
    <ul>
      <li>
        <p><strong>Idempotency Key</strong>를 도입합니다.<br />
클라이언트가 요청에 고유 키(예: 사용자ID+날짜)를 함께 전송하여, Server가 요청을 구분할 수 있게 합니다. 이를 통해, 서버가 중복 요청을 인식할 수 있습니다.</p>
      </li>
      <li>
        <p><strong>서버 처리 로직</strong><br />
같은 키의 요청이 이미 처리(혹은 처리 중)된 경우 <strong>재실행 없이</strong> 결과만 반환합니다.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="backoff-strategy">Backoff Strategy</h4>

<p><img src="/assets/img/for-post/Networking%20Essentials/image%2029.png" alt="'Exponential'과 'Exponential and random incremental(Jitter의 한 종류)'비교." />
<em>‘Exponential’과 ‘Exponential and random incremental(Jitter의 한 종류)’비교.</em></p>

<p>‘Backoff’는 재시도(Retry)시 “실패 후 다음 시도를 언제 할지”를 결정하기 위한 전략입니다.<br />
주로 네트워크 요청, 메시지 전송, 분산 잠금(Lock) 획득 등에서 일시적 오류를 처리할 때 사용합니다.<br />
‘Backoff’ 전략중에, 대표적으로 ‘Exponential’과 ‘Jitter’가 있습니다.</p>

<ul>
  <li>Exponential<br />
재시도(retry) 간의 <strong>지연 시간을 지수 함수적으로 늘려가는 전략</strong>입니다.<br />
주로 네트워크 요청, 분산 락 획득, 메시지 큐 소비 등 일시적 실패를 견디기 위해 사용됩니다.
    <blockquote class="prompt-info">
      <p>만약 동시 요청수가 많아서 Fail이 발생하고 있다면, Exponential 방식은 여전히 문제가 있을 수 있습니다.</p>
    </blockquote>
  </li>
  <li>Jitter<br />
백오프(backoff) 전략에서 여러 클라이언트가 동시에 재시도하면서 발생하는 “동시 재시도 폭주(thundering herd)”를 막기 위해, <strong>지연 시간(delay)에 임의성(randomness)</strong>을 섞는 기법입니다.</li>
</ul>

<blockquote class="prompt-info">
  <p>System Design Interview에서, Backoff전략으로 ‘Exponential’만 얘기하곤 하지만, 시니어 인터뷰인 경우, Jitter를 알아두는게 좋습니다.</p>
</blockquote>

<p><br /></p>

<h3 id="circuit-breakers">Circuit Breakers</h3>

<p>여기서는, <strong>‘cascading failures in a system(시스템 장애가 전파되는것)’</strong>에 대해서 다루며, 이를 해결하기 위한 <strong>‘Circuit Breakers’</strong>를 다룹니다.</p>

<blockquote class="prompt-info">
  <p>이 ‘cascading failures’에 대한 질문은 Interview에서 많이 나오는 좋은 질문중 하나입니다.</p>

  <p>가장 치명적인 문제들을 예방하는 방법을 아는 지원자를 찾는 데 도움이 될 뿐만 아니라, 많은 면접관이 원하는 경험을 평가하는 좋은 기준이기도 합니다.<br />
면접 준비의 핵심은 하나의 실패가 새로운 실패로 이어질 수 있는 시나리오, 즉 연쇄적인 실패에 익숙해지는 것입니다.</p>

  <p>이러한 패턴을 파악하고 이를 완화하는 방법을 아는 것은 면접에서 돋보일 수 있는 좋은 방법입니다.</p>
</blockquote>

<p><strong>‘cascading failures’사례</strong><br />
데이터베이스가 갑자기 다운되어 한 번에 한 인스턴스씩 부팅해야 하는 경우, 수많은 Retry와 사용자들의 과격한 반응으로 인해 인스턴스가 시작조차 되지 않을 수 있습니다(“thundering herd”라고 불리는 현상).<br />
첫 번째 인스턴스를 다시 시작할 수 없으니 전체 데이터베이스를 다시 온라인 상태로 만들 수 없습니다.<br />
이대로, 교착상태에 빠져버리게 됩니다.</p>

<dl>
  <dt>‘Circuit Breakers’는</dt>
  <dd>분산 시스템이나 마이크로서비스 환경에서, 하나의 서비스 실패가 시스템 전체로 전파되는 것을 방지하기 위해 고안된 안정화 기법입니다.</dd>
  <dd>네트워크 호출(원격 API, DB, 캐시 등)에 적용해, <strong>일정 수준 이상의 오류가 감지되면 빠르게 실패 응답</strong>을 반환하고, 이후 정상 복구 여부를 검사해 다시 호출을 허용합니다.</dd>
  <dd>네트워크 통신에 직접적인 영향을 미치는, <strong>견고한 시스템 설계를 위한 중요한 패턴</strong>입니다.</dd>
</dl>

<h4 id="동작-흐름">동작 흐름</h4>

<ol>
  <li>
    <p><strong>Closed 상태</strong><br />
기본 상태로, 모든 요청을 정상적으로 외부 서비스(또는 리소스)로 전달하는 상태입니다.<br />
오류가 연속해서 일정 개수(failureThreshold) 이상 발생하면 → <strong>Open 상태</strong>로 전환합니다.</p>
  </li>
  <li>
    <p><strong>Open 상태</strong><br />
외부 호출을 <strong>즉시 차단(Short-circuit)</strong> 하고, 사전에 정의된 실패 응답(예: 503, fallback 데이터)을 바로 반환합니다.<br />
일정 시간(resetTimeout)이 지나면 → <strong>Half-Open 상태</strong>로 전환합니다.</p>
  </li>
  <li>
    <p><strong>Half-Open 상태</strong><br />
외부 호출을 “시험 삼아” 소수(maxTrialRequests)만큼 허용하는 상태입니다.<br />
이 중 <strong>성공</strong>이 연속 successThreshold 개수만큼 발생하면 → <strong>Closed</strong>로 복귀합니다.<br />
<strong>실패</strong> 발생 시 다시 <strong>Open</strong>으로 돌아가고, resetTimeout 카운트 재시작합니다.</p>
  </li>
</ol>

<h4 id="circuite-breakers가-주는-이점advantages">Circuite Breakers가 주는 이점(advantages)</h4>

<ul>
  <li>
    <p><strong>빠른 실패(fail fast)</strong><br />
이미 불안정한 리소스에 대량 요청을 보내지 않아 시스템 부하가 감소합니다.</p>
  </li>
  <li>
    <p><strong>장애 격리</strong><br />
특정 서비스 문제를 빠르게 감지해, caller(상위 서비스)로 에러가 전파되는것을 제어합니다.</p>
  </li>
  <li>
    <p><strong>회복 탄력성</strong><br />
Half-Open 상태로 복귀 검사를 통해, 외부 서비스가 정상화되면 자동 복귀할 수 있습니다.</p>
  </li>
</ul>

<h4 id="언제-사용하나요-9">언제 사용하나요?</h4>

<ul>
  <li>
    <p><strong>HTTP 클라이언트</strong><br />
외부 API 호출 라이브러리에 회로 차단기가 내장되어 있습니다. (Netflix Hystrix, Resilience4j 등)</p>
  </li>
  <li>
    <p><strong>데이터베이스 연결</strong><br />
DB 커넥션 풀에서 일정 실패율 초과 시 새 연결 시도를 차단합니다.</p>
  </li>
  <li>
    <p><strong>마이크로서비스 간 RPC</strong><br />
gRPC나 REST 호출 시 Circuit Breaker 미들웨어를 삽입하여 사용합니다.</p>
  </li>
</ul>

<p><br />
<br /></p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>매우 많은 내용을 다루었지만, 아래의 Key Area를 기반으로 ‘System Design Interview’를 준비하는게 좋습니다.</p>

<ol>
  <li><strong>Understand the basics</strong>: IP addressing, DNS, and the TCP/IP model</li>
  <li><strong>Know your protocols</strong>: TCP vs. UDP, HTTP/HTTPS, WebSockets, and gRPC</li>
  <li><strong>Master load balancing</strong>: Client-side load balancing and dedicated load balancers</li>
  <li><strong>Plan for practical realities</strong>: Regionalization and patterns for handling failures</li>
</ol>

<p>네트워킹 관련 결정은, 지연 시간, 처리량, 안정성, 보안 등 시스템의 모든 측면에 영향을 미친다는 점을 기억해야 합니다.<br />
네트워킹 구성 요소와 ‘패턴에 대한 정보’에 기반한 선택을 통해, 기능적(functional)일 뿐만 아니라 견고하고(robust) 확장 가능한(scalable) 시스템을 설계할 수 있습니다.</p>

<p><br />
<br /></p>

<h2 id="references">References</h2>

<dl>
  <dt>Networking Essentials | hellointerview.com</dt>
  <dd><a href="https://www.hellointerview.com/learn/system-design/core-concepts/networking-essentials">Hello Interview | System Design in a Hurry</a></dd>
  <dt>OSI 7 Layers | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/what-is/osi-model/">OSI 모델이란 무엇인가요?- OSI 7계층 설명 - AWS</a></dd>
  <dt>OSI Model Tutorial | 9tut.com</dt>
  <dd><a href="https://www.9tut.com/osi-model-tutorial">CCNA Training » OSI Model Tutorial</a></dd>
  <dt>What is the OSI Model? | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/">What is the OSI Model? | Cloudflare</a></dd>
  <dt>OSI Model Explained | bytebytego.com</dt>
  <dd><a href="https://bytebytego.com/guides/guides/what-is-osi-model/">ByteByteGo | OSI Model Explained</a></dd>
  <dt>The Network Layer I | Addressing | utsa.pressbooks.pub</dt>
  <dd><a href="https://utsa.pressbooks.pub/networking/chapter/the-network-layer-addressing/">6. The Network Layer I | Addressing – Telecommunications and Networking</a></dd>
  <dt>ICMP | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/ko-kr/learning/ddos/glossary/internet-control-message-protocol-icmp/">ICMP란?| 네트워크레이어 프로토콜 | Cloudflare</a></dd>
  <dt>IPsec | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/ko-kr/learning/network-layer/what-is-ipsec/">IPsec이란? | VPN 작동 방식 | Cloudflare</a></dd>
  <dt>IPSec이란 무엇인가요? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/what-is/ipsec/">IPSec란 무엇인가요? - IPSec 프로토콜 설명 - AWS</a></dd>
  <dt>Transmission Control Protocol | ibm.com</dt>
  <dd><a href="https://www.ibm.com/docs/en/aix/7.3.0?topic=management-transmission-control-protocolinternet-protocol">Transmission Control Protocol/Internet Protocol</a></dd>
  <dt>TCP Segment Format | tcpcc.systemapproach.org</dt>
  <dd><a href="https://tcpcc.systemsapproach.org/tcp_ip.html#segment-format">Chapter 2: Background — TCP Congestion Control: A Systems Approach Version 1.1-dev documentation</a></dd>
  <dt>TCP KeepAlive | tldp.org</dt>
  <dd><a href="https://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html">TCP keepalive overview</a></dd>
  <dt>Implementing long-running TCP Connections within VPC networking | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/blogs/networking-and-content-delivery/implementing-long-running-tcp-connections-within-vpc-networking/">Implementing long-running TCP Connections within VPC networking | Amazon Web Services</a></dd>
  <dt>What is UDP? | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/learning/ddos/glossary/user-datagram-protocol-udp/">What is the User Datagram Protocol (UDP)? | Cloudflare</a></dd>
  <dt>UDP | systemsapproach.org</dt>
  <dd><a href="https://book.systemsapproach.org/e2e/udp.html">5.1 Simple Demultiplexor (UDP) — Computer Networks: A Systems Approach Version 6.2-dev documentation</a></dd>
  <dt>HTTP Content negotiation | developer.mozilla.org</dt>
  <dd><a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Content_negotiation">Content negotiation - HTTP | MDN</a></dd>
  <dt>RFC 2295: Transparent Content Negotiation in HTTP | datatracker.ietf.org</dt>
  <dd><a href="https://datatracker.ietf.org/doc/html/rfc2295">RFC 2295: Transparent Content Negotiation in HTTP</a></dd>
  <dt>HTTP Content negotiation | http.dev</dt>
  <dd><a href="https://http.dev/content-negotiation">HTTP Content Negotiation</a></dd>
  <dt>GraphQL | graphql.org</dt>
  <dd><a href="https://graphql.org/">GraphQL | A query language for your API</a></dd>
  <dt>RESTful API | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/what-is/restful-api/">RESTful API란 무엇인가요? - RESTful API 설명 - AWS</a></dd>
  <dt>Best practices for RESTful web API design | learn.microsoft.com</dt>
  <dd><a href="https://learn.microsoft.com/en-us/azure/architecture/best-practices/api-design">Web API Design Best Practices - Azure Architecture Center</a></dd>
  <dt>gRPC load balancing | grpc.io</dt>
  <dd><a href="https://grpc.io/blog/grpc-load-balancing/">gRPC Load Balancing</a></dd>
  <dt>Server-sent events(SSE) | learn.microsoft.com</dt>
  <dd><a href="https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/server-sent-events?tabs=server-sent-events-gateway-api">Server-sent events and Application Gateway for Containers</a></dd>
  <dt>WebRTC Hompage | webrtc.org</dt>
  <dd><a href="https://webrtc.org/?hl=ko">WebRTC</a></dd>
  <dt>What is WebRTC and how does it work? | antmedia.io</dt>
  <dd><a href="https://antmedia.io/what-is-webrtc-and-how-webrtc-works/">WebRTC Tutorial: What Is WebRTC and How It Works? - Ant Media</a></dd>
  <dt>UDP hole punching | en.wikipedia.org</dt>
  <dd><a href="https://en.wikipedia.org/wiki/UDP_hole_punching">UDP hole punching</a></dd>
  <dt>What is Load balancing? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/what-is/load-balancing/?nc1=h_ls">What is Load Balancing? - Load Balancing Algorithm Explained - AWS</a></dd>
  <dt>What’s the difference between application, network, and gateway load balancing? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/compare/the-difference-between-the-difference-between-application-network-and-gateway-load-balancing/?nc1=h_ls">Application, Network, and Gateway Load Balancing - Difference Between Load Balancing Types - AWS</a></dd>
  <dt>Conflict-free replicated data type(CRDT) | en.wikipedia.org</dt>
  <dd><a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">Conflict-free replicated data type</a></dd>
  <dt>What is DNS-based load balancing? | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/learning/performance/what-is-dns-load-balancing/">What is DNS-based load balancing? | DNS load balancing | Cloudflare</a></dd>
  <dt>CDN이란 무엇인가요? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/what-is/cdn/">CDN이란 무엇인가요? - 콘텐츠 전송 네트워크 설명 - AWS</a></dd>
  <dt>Exponential Backoff And Jitter | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/">Exponential Backoff And Jitter | Amazon Web Services</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="System Design Interview" /><category term="Core Concepts" /><category term="networking" /><category term="System Design" /><category term="interview" /><category term="alb" /><category term="tcp" /><category term="udp" /><category term="ois" /><category term="http" /><category term="https" /><category term="restapi" /><category term="Computer Science" /><summary type="html"><![CDATA[시스템 디자인(System Design)에 있어, 네트워킹(Netwoking)은 고려해야하는 필수적인 부분중 하나입니다. 이 Post에선, 네트워킹에서도 가장 중요한 부분만 정리하려고 합니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/Networking%20Essentials/networking-essentials-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/Networking%20Essentials/networking-essentials-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Workloads | Kubernetes Deep Dive - 4</title><link href="https://blog.devpour.net/posts/k8s-workloads/" rel="alternate" type="text/html" title="Workloads | Kubernetes Deep Dive - 4" /><published>2025-07-09T13:50:00+09:00</published><updated>2025-07-09T13:50:00+09:00</updated><id>https://blog.devpour.net/posts/k8s%20workloads</id><content type="html" xml:base="https://blog.devpour.net/posts/k8s-workloads/"><![CDATA[<p>Kubernetes는 Infra에 대한 추상화를 제공하는 Framework입니다. 이때, 가장 기본이 되는 추상화 단위가 ‘Pod(파드)’입니다.<br />
이 Pod를 어떻게 다루느냐(Workload Management)에 따라, 한 단계 더 추상화된, ‘Deployments’,  ‘ReplicaSet’, ‘DaemonSet’등의 Workload Object가 있습니다.<br />
이 Post에서는 Kubernetes에서의 각 Wokrload를 살펴보고, 이해하려고 합니다.</p>

<h2 id="pod파드">Pod(파드)</h2>

<blockquote>
  <p>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.<br />
- from <a href="https://kubernetes.io/docs/concepts/workloads/pods/">kubernetes.io</a></p>
</blockquote>

<dl>
  <dt>Pod는</dt>
  <dd>Kubernetes의 가장 작은 단위의 배포가능한(deployable) 컴퓨팅 단위입니다.</dd>
  <dd>하나 혹은 여러 Container를 포함하는 Group입니다.</dd>
  <dd>Pod안에서, Container들 끼리 Storage와 Network 자원을 공유합니다.</dd>
</dl>

<h3 id="pod-사용-방법-2가지">Pod 사용 방법 2가지</h3>

<h4 id="하나의-container를-돌리는-pod">하나의 Container를 돌리는 Pod</h4>

<p>Pod당 1개의 container를 포함하여 운영합니다. 이 경우, Kubernetes가 Container를 직접관리하지 못하니, Pod로 Wrapping하여 관리합니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.14.2</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="여러개의-container를-돌리는-pod">여러개의 Container를 돌리는 Pod</h4>

<p>Container간 서로 강하게 엮여있는(tightly coupled) 경우, Pod안에 여러 Container를 포함시켜 운영할 수 있습니다.</p>

<blockquote class="prompt-info">
  <p>만약, 동일한 Container를 여러개 돌리고 싶은 경우라면, 이 방식이 아니라, 뒤에 나올 ‘ReplicaSet’을 고려해야 합니다.</p>
</blockquote>

<p><br /></p>
<h3 id="pod안에서의-자원resource-공유-원리">Pod안에서의 자원(Resource) 공유 원리</h3>

<p><img src="/assets/img/for-post/k8s%20workloads/image.png" alt="Linux technologies that contribute to containers" />
<em>Linux technologies that contribute to containers</em></p>

<blockquote>
  <p>The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container.<br />
- from kubernetes.io</p>
</blockquote>

<p>Pod안의 Container들끼리는 Storage(Volume), Network, 컴퓨팅 자원을 공유합니다.<br />
이런 Pod 내부 Container들끼리의 Context공유(Resource 공유)는, Linux의 namespaces와 cgroups(Control Groups)과 다른 격리 기술의 집합(set)으로 이루어져 있습니다.</p>

<dl>
  <dt>Namespaces로</dt>
  <dd>Pod별로 Process 격리(Isolation)를 구현하고,</dd>
  <dt>cgroups로</dt>
  <dd>Container별로(즉, Process별로) 자원(Resource)을 제어합니다.</dd>
</dl>

<h4 id="linux의-namespaces">Linux의 namespaces</h4>

<p><img src="/assets/img/for-post/k8s%20workloads/image%201.png" alt="Pod와 linux namespace의 관계" class="w-50" />
<em>Pod와 linux namespace의 관계</em></p>

<p>‘Namespaces’는 Linux kernel 기능중 하나입니다.<br />
프로세스별로 ‘가상화된’ 시스템 리소스를 제공하기 위해 도입한 기능입니다. 네임스페이스 덕분에 서로 격리된 프로세스 그룹이 각자 독립적인 환경(파일 시스템, 네트워크, 프로세스 ID 등)을 갖고 동작할 수 있습니다.</p>

<blockquote class="prompt-info">
  <p>여기서, ‘시스템 리소스’는 ‘파일 시스템 마운트 정보’, ‘프로세스 식별자(PID) 공간’, ‘네트워크 스택’, ‘cgroup 계층 구조’등이 해당됩니다.</p>
</blockquote>

<p>Kubernetes에서, <strong>하나의 Pod는 실제로 “네임스페이스 집합(namespace bundle, process나 network namespace의 집합을 얘기함)” 위에서 돌아갑니다.</strong></p>

<p>Pod에 사용하는 namespace의 컨셉을 이해하기 위해, 그 일부를 함께 정리하고자 합니다.</p>
<ul>
  <li>
    <p>Process isolation(PID namespace)<br />
OS의 ‘Process’단위의 격리(isolation)를 위한 namespace입니다.<br />
Pod는 각각의 PID namespace를 가지며, 같은 Host머신에서도 Process가 격리(isolation)되어 작동합니다.</p>
  </li>
  <li>
    <p>Network interfaces(net namespace)
‘net namespace’는 ‘Process’에 새로운 IP(Virtual IP)를 부여하여, 독립적인 Port운영을 가능하게 해줍니다.<br />
예를 들면, 메일서버를 운영한다고 했을때, 해당 메일 서버가 25 Port를 요구하기 때문에, Host당 하나만 띄울 수 있습니다(PID 격리가 되어있다 하더라도).<br />
‘net namespace’는 IP레벨의 격리를 제공하여, <strong>같은 Host에서도 Network interface를 Pod단위로 분리시켜 줍니다.</strong></p>
  </li>
  <li>
    <p>Cgroups<br />
Linux안에서 시스템의 리소스를 조정하는 메커니즘(mechanism) 입니다.<br />
밑에서 더 자세히 다룹니다.</p>
  </li>
  <li>
    <p>등등…</p>
  </li>
</ul>

<h4 id="linux의-cgroupscontrol-groups">Linux의 cgroups(Control Groups)</h4>

<p>‘cgroups’은 Process단위로 Resource(CPU, memory, disk I/O 등)을 격리(isolate)하고, 계산하고(accounts), 제한(limit)하는 Kernel 기능입니다.</p>

<p>cgroups의 기능은 다음과 같습니다.</p>
<ul>
  <li>
    <p>리소스 제한(Resource limiting)<br />
메모리 사용량(파일 시스템 캐시 포함), 디스크 I/O 대역폭, CPU quota, CPU set, 프로세스당 최대 열린 파일 수 등 그룹 전체에 대한 상한선을 설정할 수 있습니다.</p>
  </li>
  <li>
    <p>우선순위 조절(Prioritization)<br />
CPU 스케줄링 비중(cpu.shares)이나 블록 디바이스 I/O 우선순위(blkio.weight)를 조절해, 특정 그룹이 더 많은 리소스를 확보하도록 할 수 있습니다 .</p>
  </li>
  <li>
    <p>리소스 사용량 측정 및 계산(Accounting)<br />
그룹 단위로 CPU 사용량, 메모리 사용량, I/O 활동 등을 계측하여, 과금(billing)·모니터링·로그 수집 등에 활용할 수 있습니다.</p>
  </li>
  <li>
    <p>프로세스 제어(Control)<br />
프로세스 그룹 전체를 freeze/unfreeze(일시 중단/재개)하거나, checkpoint &amp; restore(검사점 생성 후 재시작) 기능을 통해 상태를 보존·복원할 수 있습니다</p>
  </li>
</ul>

<blockquote class="prompt-info">
  <p>‘cgroups v1’에서는 다중 계층을 사용하여, 각각의 cgroup 정책에 따라 별도의 Controller를 사용할 수 있지만, 복잡성이 높아지는 원인이 되었습니다.<br />
최신버전인 ‘cgroups v2’에서는 변경되어, 단일 계층에서 Controller(리소스 관리를 위한)를 관리합니다.</p>
</blockquote>

<p>아래는 Kubenetes에서 Pod를 정의하는 부분에서, cgroup과 관련이 있는 부분을 표시하고 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cgroup-demo-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-server</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:latest</span>
    <span class="c1"># ───────────────────────────────────────────────────────────────</span>
    <span class="c1"># 이 부분이 cgroup으로 구현되는, 리소스 격리 설정입니다.</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="c1"># 최소 보장(request)와 최대 제한(limit)을 지정합니다.</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">250m"</span>        <span class="c1"># 이 컨테이너에 최소 0.25 CPU 코어를 보장.</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">128Mi"</span>    <span class="c1"># 최소 128MiB 메모리를 보장.</span>
      <span class="na">limits</span><span class="pi">:</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">500m"</span>        <span class="c1"># 최대 0.5 CPU 코어까지만 사용 가능.</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">256Mi"</span>    <span class="c1"># 최대 256MiB 메모리까지만 사용 가능.</span>
    <span class="c1"># ───────────────────────────────────────────────────────────────</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h3 id="pod-lifecycle">Pod Lifecycle</h3>

<blockquote>
  <p>Like individual application containers, Pods are considered to be relatively ephemeral (rather than durable) entities.<br />
- from kubernetes.io</p>
</blockquote>

<p>Pod는 영속적(durable)이라기 보다, 일시적인(ephemeral) entity로 여겨집니다.<br />
Kubernetes에서는 Pod의 Lifecycle를 통해, 일시적으로 사용되는 Pod의 상태를 추상화하였습니다.</p>

<blockquote class="prompt-info">
  <p>반복적이고 빠르게, 제거되고 생성되기 때문에, Lifecycle이 필요하다는 뜻</p>
</blockquote>

<h4 id="pod와-fault-recoveryself-healing">Pod와 Fault Recovery(Self-Healing)</h4>

<p>Pod는 포함하고 있는 Container중 하나가 ‘fail’상태에 빠지면, 해당 Container를 재시작하려고 시도합니다.<br />
하지만, Pod을 복구할 수 없는 상태에 빠진 경우, <strong>Kubernetes는 더 이상 Pod자체를 복구하려고 하지 않습니다.</strong><br />
문제가 있는 Pod을 제거하고, 다른 구성요소(Control plane의 <a href="https://blog.devpour.net/posts/k8s-control-plane/#kube-controller-manager">Control Manager</a>)를 통해 <strong>Pod을 재생성하여 복구 합니다.</strong></p>

<blockquote>
  <p>A given Pod (as defined by a UID) is never “rescheduled” to a different node; instead, that Pod can be replaced by a new, near-identical Pod.
- from <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-fault-recovery">kubernetes.io</a></p>
</blockquote>

<p>이미 생성된 Pod는 <strong>절대로 다른 노드에 재배치 되지 않습니다</strong>. 대신에, <strong>새로운 Pod으로 대체됩니다.</strong>(Pod의 <code class="language-plaintext highlighter-rouge">.metadata.uid</code> 가 바뀝니다. 즉, 새로 생성됩니다.)</p>

<blockquote class="prompt-info">
  <p>이 경우, 새롭게 생성된 Pod이 같은 Node에 배치되는것을 보장하지 않습니다.
(Pod를 새로 생성하는거기 때문에, 당연한 얘기일 수도.. filtering을 통해 같은 Node로 배치되게 유도할 수는 있습니다.)</p>
</blockquote>

<h4 id="pod-phase">Pod phase</h4>

<p>Pod의 status는 여러 정보를 포함한 Object Field로 구성되어 있습니다.<br />
여기에는 <code class="language-plaintext highlighter-rouge">phase</code> 라는 field가 있는데, 이는 추상화된 lifecycle의 ‘high-level summary’입니다.<br />
Pod의 phase는 아래의 List로 제한되어 있으며, 이외의 다른 어떤 값도 존재해선 안됩니다.</p>

<ul>
  <li>
    <p>Pending<br />
클러스터가 Pod ‘정의’를 수용했으나, 아직 스케줄링이 완료되지 않았거나 컨테이너 이미지 다운로드·생성 과정이 진행 중인 상태입니다.</p>
  </li>
  <li>
    <p>Running<br />
최소 하나의 “주요(main)” 컨테이너가 정상적으로 시작된 상태. 모든 컨테이너가 생성되어 Running 상태가 되면 이 Phase로 진입.</p>
  </li>
  <li>
    <p>Succeeded<br />
모든 컨테이너가 정상 종료(Exit 0)하고, 재시작 정책에 따라 재시작되지 않을 때. 주로 일회성 작업(Job/CronJob이 생성한 Pod)에서 볼 수 있습니다.</p>
  </li>
  <li>
    <p>Failed<br />
적어도 하나의 컨테이너가 비정상 종료(Non-zero Exit)하거나, 시스템(OOM 등)에 의해 강제 종료된 상태.이 Phase로 들어가면 다시 다른 Phase로 전환되지 않습니다.</p>
  </li>
  <li>
    <p>Unknown<br />
kube-apiserver가 해당 Pod의 상태를 확인할 수 없을 때. 네트워크 단절이나 kubelet 오류 등으로 인해 노드와의 통신이 끊긴 경우에 주로 발생합니다.</p>
  </li>
</ul>

<h4 id="crashloopbackoff이-phase라고-혼동하지-마세요">CrashLoopBackOff이 phase라고 혼동하지 마세요</h4>

<p>Pod가 반복적으로 ‘시작 실패’를 겪을 때, <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> 값이 kubectl명령어로 나오는 status field에 노출 될 수 있습니다.(아래 예시 참고)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>kubectl get pods <span class="nt">--namespace</span><span class="o">=</span>alessandras-namespace

 NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE
alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h
</pre></td></tr></tbody></table></code></pre></div></div>

<p>마찬가지로, <code class="language-plaintext highlighter-rouge">Terminating</code> 값이 Status에 표시될 때도 있습니다.<br />
이때, 이 <code class="language-plaintext highlighter-rouge">CrashLoopBackoff</code> 와 <code class="language-plaintext highlighter-rouge">Terminating</code>  값이 Pod의 Phase값은 아닙니다.<br />
kubectl에서 표시되는 Status는 유저 친화적인(직관적인) 값일 뿐이고, 반드시 Pod의 Phase값이 위치하는건 아닙니다.</p>

<p><br /></p>

<h3 id="init-containers">Init Containers</h3>

<dl>
  <dt>‘Init Containers’는</dt>
  <dd>Pod initialization 과정에서, Main App container가 실행되기전에 실행되어, initialization작업을 하는 Container입니다.</dd>
  <dd>항상 Container 작업이 성공적으로 완료(complete successfully)되어야 합니다.</dd>
  <dd>작업이 실패하게 되면, 성공할때까지 재실행 합니다.(<code class="language-plaintext highlighter-rouge">restartPolicy</code> 가 있다면, 해당 정책에 따라 ‘Pod fail’로 다루게 됩니다.)</dd>
</dl>

<h4 id="일반-container와의-차이점">일반 Container와의 차이점</h4>

<ul>
  <li>‘lifecycle’, ‘livenessProbe’, ‘readinessProbe’, or ‘startupProbe’ fields를 지원하지 않습니다.</li>
  <li>Pod이 ‘Ready’상태에 들어가기전에 모든 작업을 완료하고, 종료되어야 합니다.</li>
  <li>만약, 여러개의 init Container를 정의하였다면, 정의한 순서대로 실행되며, 반드시 이전의 container 작업이 성공적으로 완료되어야 합니다.</li>
  <li>Main App Container와 같은 시스템 Resource를 사용하지만, 상호작용하지 않습니다.
    <blockquote>
      <p>Init containers share the same resources (CPU, memory, network) with the main application containers but do not interact directly with them. They can, however, use shared volumes for data exchange.<br />
- from <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#differences-from-sidecar-containers">kubernetes.io</a></p>
    </blockquote>
  </li>
</ul>

<p>아래는 Init Container의 예시입니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">myapp-pod</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">MyApp</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">myapp-container</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox:1.28</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">echo</span><span class="nv"> </span><span class="s">The</span><span class="nv"> </span><span class="s">app</span><span class="nv"> </span><span class="s">is</span><span class="nv"> </span><span class="s">running!</span><span class="nv"> </span><span class="s">&amp;&amp;</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">3600'</span><span class="pi">]</span>
  <span class="na">initContainers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">init-myservice</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox:1.28</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s2">"</span><span class="s">until</span><span class="nv"> </span><span class="s">nslookup</span><span class="nv"> </span><span class="s">myservice.$(cat</span><span class="nv"> </span><span class="s">/var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local;</span><span class="nv"> </span><span class="s">do</span><span class="nv"> </span><span class="s">echo</span><span class="nv"> </span><span class="s">waiting</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">myservice;</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">2;</span><span class="nv"> </span><span class="s">done"</span><span class="pi">]</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">init-mydb</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox:1.28</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s2">"</span><span class="s">until</span><span class="nv"> </span><span class="s">nslookup</span><span class="nv"> </span><span class="s">mydb.$(cat</span><span class="nv"> </span><span class="s">/var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local;</span><span class="nv"> </span><span class="s">do</span><span class="nv"> </span><span class="s">echo</span><span class="nv"> </span><span class="s">waiting</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">mydb;</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">2;</span><span class="nv"> </span><span class="s">done"</span><span class="pi">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h3 id="sidecar-containers">Sidecar Containers</h3>

<dl>
  <dt>‘Sidecar Containers’는</dt>
  <dd>Pod 안에서, Main Container와 함께 계속 실행되는 Container입니다.</dd>
  <dd>Main Application Container가 시작되기 전에, 먼저 시작합니다.</dd>
  <dd>App Container의 기능을 확장하거나 강화하기 위해 사용됩니다(<strong>App code변경 없이</strong>).</dd>
  <dd>예를 들면, 로깅, 모니터링, 보안, 데이터 동기화 같은것이 있습니다.</dd>
</dl>

<h4 id="kubernetes에서의-sidecar-containers">Kubernetes에서의 Sidecar containers</h4>

<ul>
  <li>
    <p>‘init container’의 special case 입니다.(즉, init container의 한 종류입니다.)<br />
Kubernetes에서, ‘Sidecar container’는 ‘init container’의 special case 입니다.<br />
‘Sidecar container’는 Pod의 부팅(startup)이후에도 계속 실행된 상태로 있습니다. (보통의 ‘init container’는 Pod의 부팅이후에는 종료됩니다.)<br />
‘init container’와 다르게, sidecar container가 ‘running’상태라면, 종료되지 않고 다음 container가 실행됩니다.</p>
  </li>
  <li>
    <p>동일 Pod의 네임스페이스(namespaces)와 볼륨(volumes), 네트워크를 공유합니다.</p>
  </li>
  <li>Cluster단위에서 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature</a>를 enable해줘야 합니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>  <span class="nt">--feature-gates</span><span class="o">=</span>..,SidecarContainers<span class="o">=</span><span class="nb">true</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p>위와 같이 Sidecar를 사용하도록 설정하면, <code class="language-plaintext highlighter-rouge">initContainers.restartPolicy</code> 를 설정할 수 있게 됩니다.(재실행 가능하도록 설정)</p>
  </li>
  <li>종료시엔, Main Container가 완전히 종료되고, Sidecar의 shutdown이 실행됩니다.(Pod에 정의된 순서의 역순으로 실행)
    <blockquote class="prompt-info">
      <p>이는 종료시에도, Sidecar가 다른 Container를 보조하는 역할이 빈틈없이 수행되도록 합니다.</p>
    </blockquote>
  </li>
</ul>

<p>아래는 Sidecar 정의 예시입니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">myapp</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">myapp</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">myapp</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">myapp</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">myapp</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">alpine:latest</span>
          <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">while</span><span class="nv"> </span><span class="s">true;</span><span class="nv"> </span><span class="s">do</span><span class="nv"> </span><span class="s">echo</span><span class="nv"> </span><span class="s">"logging"</span><span class="nv"> </span><span class="s">&gt;&gt;</span><span class="nv"> </span><span class="s">/opt/logs.txt;</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">1;</span><span class="nv"> </span><span class="s">done'</span><span class="pi">]</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
              <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/opt</span>
      <span class="na">initContainers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">logshipper</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">alpine:latest</span>
          <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Always</span>
          <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">tail</span><span class="nv"> </span><span class="s">-F</span><span class="nv"> </span><span class="s">/opt/logs.txt'</span><span class="pi">]</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
              <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/opt</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
          <span class="na">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h3 id="pod의-리소스-요청-기준">Pod의 리소스 요청 기준</h3>

<p>Pod에는 여러 Container가 포함될 수 있습니다. 이때, Container의 리소스(CPU같은) 정의는 어떻게 처리되어 Pod로서 스케쥴링 되는지 알아봅니다.<br />
<br /></p>

<p>컨테이너들은 Pod 단위로 네임스페이스(namespaces)와 볼륨을 공유하지만, 리소스(CPU·메모리·I/O) 관리는 컨테이너별 cgroup을 통해 개별적으로 격리・제어됩니다.</p>

<ul>
  <li>requests 합산<br />
스케줄러는 Pod 스펙의 각 컨테이너 <code class="language-plaintext highlighter-rouge">resources.requests</code> 값을 모두 더한 만큼의 CPU/메모리 리소스가 노드에 남아 있는지 보고, Pod를 배치합니다.<br />
예: 컨테이너 A가 cpu: 200m, memory: 100Mi, 컨테이너 B가 cpu: 100m, memory: 50Mi 라면, 스케줄러는 “이 Pod는 최소 300m CPU와 150Mi 메모리를 필요로 한다”고 판단합니다.</li>
</ul>

<p><br /></p>

<p>만약 Pod으로만 Application을 운영한다면, Pod자체로는 HA(High Availability) 관련 기능을 제공하지 않기때문에, 관리하는데에 많은 노력이 필요합니다.<br />
때문에 별도의 ‘Workload Management(Deployment, Daemonset 등)’를 통해 Pod을 관리합니다.</p>

<h2 id="deployment와-replicaset">Deployment와 ReplicaSet</h2>

<h3 id="deployment">Deployment</h3>

<p>Kubernetes에서, ‘Deployment’는 선언적(Declarative)으로 애플리케이션의 Pod 복제본Set(ReplicaSet)을 관리하고, 롤링 업데이트·롤백 같은 배포 전략을 자동화해 주는 상위 리소스입니다.<br />
<br /><br />
아래는 ‘Deployment’의 예시입니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
 <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-deployment</span>
 <span class="na">labels</span><span class="pi">:</span>
   <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
 <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
 <span class="na">selector</span><span class="pi">:</span>
   <span class="na">matchLabels</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
 <span class="na">template</span><span class="pi">:</span>
   <span class="na">metadata</span><span class="pi">:</span>
     <span class="na">labels</span><span class="pi">:</span>
       <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
   <span class="na">spec</span><span class="pi">:</span>
     <span class="na">containers</span><span class="pi">:</span>
     <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
       <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.14.2</span>
       <span class="na">ports</span><span class="pi">:</span>
       <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
 <span class="na">strategy</span><span class="pi">:</span>
   <span class="na">type</span><span class="pi">:</span> <span class="s">RollingUpdate</span>
   <span class="na">rollingUpdate</span><span class="pi">:</span>
     <span class="na">maxSurge</span><span class="pi">:</span> <span class="m">1</span>
     <span class="na">maxUnavailable</span><span class="pi">:</span> <span class="m">1</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="주요-기능">주요 기능</h4>

<ul>
  <li>
    <p><strong>ReplicaSet 관리</strong><br />
Deployment는 내부적으로 ReplicaSet을 생성·관리하여, 지정한 수(spec.replicas)만큼 Pod이 항상 가동되어 있도록 보장합니다.</p>
  </li>
  <li>
    <p><strong>롤링 업데이트(RollingUpdate)</strong><br />
새로운 버전의 컨테이너 이미지로 점진 교체하면서 가용성(Availability)을 유지합니다.<br />
기본값은 최대 25% 오버프로비저닝, 최대 25% 미달 허용(maxSurge: 25%, maxUnavailable: 25%).</p>
  </li>
  <li>
    <p><strong>롤백(Rollback)</strong><br />
문제가 생기면 이전 버전의 ReplicaSet으로 자동 혹은 수동 복귀가 가능합니다.</p>
  </li>
  <li>
    <p><strong>버전 관리(Revision)</strong><br />
각 변경은 Revision 번호를 부여받아, <code class="language-plaintext highlighter-rouge">kubectl rollout history</code>로 추적할 수 있습니다.</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>  <span class="c1"># deployment의 history를 확인합니다</span>
  <span class="s">$ kubectl rollout history deployment/nginx-deployment</span>
    
  <span class="s">deployments "nginx-deployment"</span>
  <span class="s">REVISION    CHANGE-CAUSE</span>
  <span class="s">1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml</span>
  <span class="s">2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1</span>
  <span class="s">3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ul>

<h4 id="배포전략strategy들">배포전략(Strategy)들</h4>

<p><code class="language-plaintext highlighter-rouge">.spec.strategy</code> 를 통해, Deployment의 배포 전략을 선택할 수 있습니다.</p>

<ul>
  <li>
    <p>RollingUpdate(기본값)<br />
점진적으로 새로운 ReplicaSet의 Pod를 늘리고, 구 버전 Pod를 줄이며 교체합니다.<br />
maxSurge·maxUnavailable로 스피드·가용성 균형 조정할 수 있습니다.</p>
  </li>
  <li>
    <p>Recreate<br />
먼저 모든 기존 Pod를 삭제한 뒤(Zero-downtime 없이) 새 버전 Pod를 생성하는 방식입니다.<br />
Stateless 애플리케이션에서 단순하게 사용할때만 사용합니다.</p>
    <blockquote class="prompt-info">
      <p>This will only guarantee Pod termination previous to creation for upgrades.
‘Recreate’은 새로운 버젼이 생성되기 전에, 반드시 그 이전버전이 제거되는것만 보장합니다. 즉, zero downtime을 보장하지 않습니다.</p>
    </blockquote>
  </li>
</ul>

<h4 id="운영관련-팁tip들">운영관련 팁(Tip)들</h4>
<ul>
  <li><strong>컨테이너 이미지 벡터 태그</strong> 대신 <strong>SHA digest</strong>(my-app@sha256:…)를 쓰면, 동일 버전 재배포 시에도 불필요한 롤아웃을 방지할 수 있습니다.</li>
  <li><strong>Blue–Green</strong> 혹은 <strong>Canary</strong> 배포: Deployment를 여러 개 만들고, Deployment사이에서 서비스(Service) 라우팅을 전환하거나, ‘Argo Rollouts, Flagger’ 같은 툴을 활용하여 구현합니다.</li>
</ul>

<p><br /></p>

<h3 id="replicaset">ReplicaSet</h3>

<blockquote>
  <p>A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.<br />
- from kubernetes.io</p>
</blockquote>

<p>‘ReplicaSet’은 Pod의 복제본이 어느때든(즉, 항상) 지정한 수 만큼 가동되어 있도록 보장하는 역할을 합니다.</p>

<h4 id="replicaset의-작동-방식">ReplicaSet의 작동 방식</h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ReplicaSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">guestbook</span>
    <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="c1"># modify replicas according to your case</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">php-redis</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li>여러 필드를 통해, 특정 ReplicaSet으로 관리되고 있는 Pod을 구분합니다.
    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">spec.selector</code> (Label Selector)<br />
ReplicaSet 컨트롤러가 ‘지금 클러스터에 매칭되는 Pod이 몇 개인지’ 세고, 부족하면 생성, 초과면 삭제하기 위해 사용하는 핵심 필터입니다.<br />
스케줄러가 ‘이 ReplicaSet의 Pod’를 노드에 스케줄링하거나, Service가 ‘어떤 Pod로 트래픽을 보낼지’ 결정할 때도 이 Selector를 활용합니다.<br />
처음부터 <code class="language-plaintext highlighter-rouge">.spec.template.metadata.labels</code>(Pod의 Template에 있는 label정보)와 <strong>일치하도록 정의</strong>해야 하며, 라벨 구조를 바꾸면 스케일링 대상이 달라집니다.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">metadata.ownerReferences</code> (Owner Reference)<br />
‘이 Pod는 이 ReplicaSet의 자식’이라는 관계 정보로, ReplicaSet이 삭제될 때 자동으로 Pod를 정리(garbage collect)하도록 Kubernetes에 알려 줍니다.</p>
      </li>
    </ul>
  </li>
  <li>ReplicaSet이 유지해야 하는 상태를 정의하고, 이 상태를 유지합니다.
    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">spec.selector</code><br />
여러 Pod들 사이에서, ReplicaSet에 포함된 Pod을 식별하는데 사용하는 field입니다. 이를 통해 현재 개수와 Desired 개수를 확인하여, 일치하도록 조정합니다.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">spec.replicas</code><br />
유지되어야할 Pod의 수(a number of replicas)를 표현합니다.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">spec.template</code><br />
Pod를 생성할때 사용하는 Template입니다.</p>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="deployments와-replicaset의-관계">Deployments와 ReplicaSet의 관계</h3>

<dl>
  <dt>ReplicaSet은</dt>
  <dd>‘원하는 개수의 Pod가 항상 구동되도록 보장’하는 역할에 집중한 리소스인 반면,</dd>
  <dt>Deployment는</dt>
  <dd>ReplicaSet 위에 ‘업데이트 관리’, ‘버전 관리’, ‘롤백’ 같은 추가 기능을 제공하는 상위 추상화입니다.</dd>
</dl>

<blockquote class="prompt-info">
  <p>때문에, ReplicaSet을 직접 쓰는것보다, Deployment를 사용하는것을 추천합니다.</p>
</blockquote>

<p><br /></p>

<h2 id="statefulset">StatefulSet</h2>

<blockquote>
  <p>A StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.<br />
- from <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">kubernetes.io</a></p>
</blockquote>

<p>‘StatefulSet’은 쿠버네티스에서 상태(stateful)를 갖는 애플리케이션을 안정적으로 배포·확장·업데이트하기 위해 설계된 Workload 객체입니다. Deployment와 달리 각 Pod에 고정된 네트워크 ID와 영속 스토리지(Persistent Volume)를 보장하며, 생성·삭제·업데이트 시에도 순서(order)와 안정성(stability) 을 제공합니다.</p>

<h3 id="사용-사례">사용 사례</h3>

<ul>
  <li>데이터베이스 클러스터 <em>**</em>(예: Cassandra, MongoDB, MySQL Replication)</li>
  <li>분산 캐시 (예: Redis Sentinel, ZooKeeper)</li>
  <li>메시지 브로커 (예: Kafka, RabbitMQ)</li>
  <li>상태 저장 애플리케이션 (예: Elasticsearch, Etcd)</li>
</ul>

<blockquote class="prompt-info">
  <p>반드시 Stateful이 필요한 APP이 아니라면, 되도록 Deployment를 사용하길 추천합니다.<br />
Scalable을 고려해야하는 시스템에서, Stateful 시스템으로 App을 만든다면, Pod이 영속적인 개념이 아니기 때문에, Stateful을 위해 많은 작업이 추가로 필요해집니다.</p>
</blockquote>

<p><br /></p>

<h3 id="statefulset과-headless-service">StatefulSet과 Headless Service</h3>

<p><img src="/assets/img/for-post/k8s%20workloads/image%202.png" alt="StatefulSet과 Headless Service의 관계. mysql CQRS패턴을 예시로 사용. | [alibabacloud.com](https://www.alibabacloud.com/blog/kubernetes-application-management-stateful-services_595087)" />
<em>StatefulSet과 Headless Service의 관계. mysql CQRS패턴을 예시로 사용. | <a href="https://www.alibabacloud.com/blog/kubernetes-application-management-stateful-services_595087">alibabacloud.com</a></em></p>

<h4 id="statefulset에-service가-아닌-별도의-headless-service가-필요한-이유">StatefulSet에 ‘Service’가 아닌, 별도의 ‘Headless Service’가 필요한 이유?</h4>

<p>StatefulSet이 ‘상태 저장(stateful)’ 애플리케이션을 다루기 위해 제공하는 핵심 기능 중 하나가 <strong>각 Pod에 고정된 네트워크 ID</strong>를 부여하는 것입니다.<br />
그런데 쿠버네티스의 기본 ‘Service’는 <strong>클러스터 IP</strong>와 <strong>로드밸런싱(LB)</strong>을 전제로 동작하기 때문에, StatefulSet이 원하는 ‘Pod별로 고정된 DNS 이름’을 제공해 주지 않습니다.</p>

<dl>
  <dt>정리하면,</dt>
  <dd>Kubernetes에서는 보통, Pod가 아닌 ‘Service’를 통해 Pod에 접속합니다. 이 ‘Service’를 이용하면, LB를 통해 Pod에 random하게 접속하게 됩니다.</dd>
  <dd>하지만, DB와 같은 App들은 구분을 위해, Pod에 대한 고정된 DNS주소가 필요할 수 있습니다.</dd>
  <dd>StatefulSet은 이를 위해, 고정된 네크워크 ID(대표적으로 IP)를 제공하고,</dd>
  <dd>이를 Headless Service를 통해, Pod에 접속하기 위한 개별 IP를 조회할 수 있게 합니다.(DNS에 개별 Pod에 접속하기 위한 Domain을 등록하는 방식.)</dd>
</dl>

<h4 id="headless-service의-역할">Headless Service의 역할</h4>

<ul>
  <li>Service에 속한 Pod별로 A 레코드를 생성합니다
    <ul>
      <li>
        <p>일반 Service(clusterIP가 할당됩니다)<br />
동일한 Service 이름에 대해 하나의 VIP(가상 IP)만 DNS에 등록되는 방식입니다. 이후, LB와 kube-proxy를 통해 Pod로 연결됩니다.</p>
      </li>
      <li>
        <p>Headless Service(clusterIP가 부여되지 않습니다.)<br />
selector에 걸리는 각 Pod의 IP를 개별 A 레코드로 DNS에 등록하여, Service Discovery가 되도록 합니다.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Stable Network Identity 보장<br />
statefulSet이 생성하는 Pod 이름(mysql-0, mysql-1…)과 Headless Service 이름(headless-mysql-svc)을 조합해<br />
<code class="language-plaintext highlighter-rouge">mysql-0.headless-mysql-svc</code>, <code class="language-plaintext highlighter-rouge">mysql-1.headless-svc</code> 와 같은 영구적인 DNS 이름을 제공합니다.<br />
<strong>Pod IP가 변경되더라도 DNS 이름은 그대로 유지</strong>되어, 애플리케이션은 항상 같은 호스트명으로 자신(또는 다른 노드)을 참조할 수 있습니다.</p>
  </li>
  <li>
    <p>Service Discover(서비스 디스커버리) 지원<br />
ZooKeeper, Cassandra 같은 분산 시스템은 클러스터 토폴로지를 구성할 때 피어(peer) 노드의 정확한 주소가 필요합니다.<br />
Headless Service를 통해 “내 토폴로지 멤버 리스트”를 DNS 기반으로 조회할 수 있게 해 줍니다.</p>
  </li>
  <li>로드밸런싱이 아니라 직접 연결<br />
Headless Service는 프록시나 로드밸런싱 기능을 제공하지 않습니다. DNS 조회 결과로 얻은 Pod IP 리스트를 클라이언트가 직접 사용하게 됩니다.<br />
이 방식이 StatefulSet이 요구하는 ‘개별 Pod에 대한 직접 연결’하는 시나리오에 딱 맞습니다.</li>
</ul>

<p><br /></p>

<h2 id="daemonset">DaemonSet</h2>

<blockquote>
  <p>A DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.
- from <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">kubernetes.io</a></p>
</blockquote>

<p>DaemonSet은 클러스터의 모든(또는 지정한) <strong>노드에 ‘하나씩’</strong> Pod를 배포·유지하도록 보장하는 컨트롤러입니다. 주로 노드별 에이전트(로그 수집기, 모니터링 에이전트, 네트워크 플러그인 등)를 배포할 때 사용합니다.<br />
노드가 추가되면, 해당 노드에 대한 Pod이 추가로 생성됩니다.</p>

<h3 id="사용-사례-1">사용 사례</h3>

<ul>
  <li>로그 수집 &amp; 모니터링 에이전트: Fluentd, Filebeat, Telegraf, Datadog Agent</li>
  <li>네트워크 플러그인: Calico, Cilium, Weave Net</li>
  <li>스토리지 드라이버: CSI 플러그인 데몬</li>
  <li>보안 에이전트: Falco, Istio 데몬 서비스</li>
</ul>

<h3 id="daemonset-spec-예시">DaemonSet Spec 예시</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">DaemonSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fluentd-elasticsearch</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">k8s-app</span><span class="pi">:</span> <span class="s">fluentd-logging</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">fluentd-elasticsearch</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">fluentd-elasticsearch</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">tolerations</span><span class="pi">:</span>
      <span class="c1"># these tolerations are to have the daemonset runnable on control plane nodes</span>
      <span class="c1"># remove them if your control plane nodes should not run pods</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">node-role.kubernetes.io/control-plane</span>
        <span class="na">operator</span><span class="pi">:</span> <span class="s">Exists</span>
        <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">node-role.kubernetes.io/master</span>
        <span class="na">operator</span><span class="pi">:</span> <span class="s">Exists</span>
        <span class="na">effect</span><span class="pi">:</span> <span class="s">NoSchedule</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">fluentd-elasticsearch</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/fluentd_elasticsearch/fluentd:v2.5.2</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">200Mi</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s">100m</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">200Mi</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">varlog</span>
          <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/log</span>
      <span class="c1"># it may be desirable to set a high priority class to ensure that a DaemonSet Pod</span>
      <span class="c1"># preempts running Pods</span>
      <span class="c1"># priorityClassName: important</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">varlog</span>
        <span class="na">hostPath</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/var/log</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="jobs">Jobs</h2>

<blockquote>
  <p>Jobs represent one-off tasks that run to completion and then stop.
- from <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">kubernetes.io</a></p>
</blockquote>

<p>‘Jobs’는 1회성 작업(실행을 완료하고 멈추는)을 안전하게 실행하기 위해 사용합니다.<br />
‘Job’은 작업의 완료를 위해, Pod를 1개이상 생성할 수 있습니다. 이때, <strong>‘작업의 완료’기준은, 성공적으로 종료된(successfully terminate) Pod의 갯수</strong>입니다.<br />
이를 정리하면, ‘Jobs’는 <strong>지정한 개수의 완료된 Pod을 보장해주는 컨트롤러</strong> 입니다.<br />
반복적이거나 장기 실행 서비스인 ‘Deployment’와 달리, 특정 작업이 한 번만 또는 정해진 횟수만큼 실행되어야 할 때 사용합니다.</p>

<h3 id="jobs-spec-예시">Jobs Spec 예시</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">batch/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Job</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-job</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">completions</span><span class="pi">:</span> <span class="m">3</span>           <span class="c1"># 총 3개 Pod가 성공 종료되어야 Job이 완료됩니다.</span>
  <span class="na">parallelism</span><span class="pi">:</span> <span class="m">2</span>           <span class="c1"># 동시에 최대 2개의 Pod을 실행합니다.</span>
  <span class="na">backoffLimit</span><span class="pi">:</span> <span class="m">4</span>          <span class="c1"># 실패 시, 최대 4회 재시도합니다.</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">OnFailure</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">task</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
        <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">sh"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">-c"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">echo</span><span class="nv"> </span><span class="s">Hello;</span><span class="nv"> </span><span class="s">exit</span><span class="nv"> </span><span class="s">0"</span><span class="pi">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="응용하기">응용하기</h3>

<ul>
  <li>
    <p><a href="https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/">Indexed Jobs</a> (.spec.completionMode: “Indexed”)<br />
각 Pod에 JOB_COMPLETION_INDEX 환경 변수를 주어, 인덱스별 작업(partition으로 )을 분리하여 처리할 수 있습니다.</p>
  </li>
  <li>
    <p><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">Work Queue 패턴</a><br />
메시지 큐(RabbitMQ, Kafka)와 연동해, parallelism 을 높여 대량 데이터를 분산 처리합니다.</p>
  </li>
  <li>
    <p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/">TTL(Time-to-Live) Controller</a><br />
Job 완료 후 일정 시간(.spec.ttlSecondsAfterFinished)이 지나면 자동 삭제되게 할 수 있습니다.</p>
  </li>
</ul>

<h3 id="사용-사례-2">사용 사례</h3>
<ul>
  <li>데이터 마이그레이션: 데이터를 일괄로 로드하거나 변환할때 사용합니다.</li>
  <li>백업/정리 스크립트: 주기적인 백업작업이나 로그 아카이빙 작업에 사용합니다.</li>
  <li>머신러닝 학습: 단일 배치(1회 실행) 훈련 작업용으로 사용합니다.</li>
  <li>크론작업 대체: CronJob과 조합해 주기적 Batch 실행할 때 사용합니다.</li>
</ul>

<h2 id="references">References</h2>

<dl>
  <dt>Pods | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/pods/">Pods</a></dd>
  <dt>Workload Management | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/controllers/">Workload Management</a></dd>
  <dt>Deployments | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a></dd>
  <dt>ReplicaSet | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a></dd>
  <dt>StatefulSet | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a></dd>
  <dt>Headless Service | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Service</a></dd>
  <dt>DaemonSet | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a></dd>
  <dt>Linux namespaces | en.wikipedia.org</dt>
  <dd><a href="https://en.wikipedia.org/wiki/Linux_namespaces">Linux namespaces</a></dd>
  <dt>The 7 most used Linux namespaces | redhat.com</dt>
  <dd><a href="https://www.redhat.com/en/blog/7-linux-namespaces">The 7 most used Linux namespaces</a></dd>
  <dt>Linux cgroups(Control Groups) | docs.redhat.com</dt>
  <dd><a href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/resource_management_guide/ch01">Chapter1.Introduction to Control Groups (Cgroups) | Resource Management Guide | Red Hat Enterprise Linux | 6 | Red Hat Documentation</a></dd>
  <dt>A Linux sysadmin’s introduction to cgroups | redhat.com</dt>
  <dd><a href="https://www.redhat.com/en/blog/cgroups-part-one">A Linux sysadmin’s introduction to cgroups</a></dd>
  <dt>4 Linux technologies fundamental to containers | opensource.com</dt>
  <dd><a href="https://opensource.com/article/21/8/container-linux-technology">4 Linux technologies fundamental to containers</a></dd>
  <dt>Evolution of Docker from Linux Containers | baeldung.com</dt>
  <dd><a href="https://www.baeldung.com/linux/docker-containers-evolution">Evolution of Docker from Linux Containers | Baeldung on Linux</a></dd>
  <dt>Building a Linux container by hand using namespaces | redhat.com</dt>
  <dd><a href="https://www.redhat.com/en/blog/building-container-namespaces">Building a Linux container by hand using namespaces</a></dd>
  <dt>Indexed Jobs | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/">Introducing Indexed Jobs</a></dd>
  <dt>Work Queue pattern with Jobs | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">Coarse Parallel Processing Using a Work Queue</a></dd>
  <dd><a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">Fine Parallel Processing Using a Work Queue</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="DevOps" /><category term="kubernetes" /><category term="aws" /><category term="kubernetes" /><category term="cncf" /><category term="k8s" /><summary type="html"><![CDATA[Kubernetes는 Infra에 대한 추상화를 제공하는 Framework입니다. 이때, 가장 기본이 되는 추상화 단위가 ‘Pod(파드)’입니다. 이 Pod를 어떻게 다루느냐(Workload Management)에 따라, 한 단계 더 추상화된, ‘Deployments’, ‘ReplicaSet’, ‘DaemonSet’등의 Workload Object가 있습니다. 이 Post에서는 Kubernetes에서의 각 Wokrload를 살펴보고, 이해하려고 합니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/k8s%20workloads/k8s-workloads-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/k8s%20workloads/k8s-workloads-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">HTTPS 이해하기</title><link href="https://blog.devpour.net/posts/Understanding-https-certificates/" rel="alternate" type="text/html" title="HTTPS 이해하기" /><published>2025-07-07T18:47:00+09:00</published><updated>2025-07-07T22:29:23+09:00</updated><id>https://blog.devpour.net/posts/Understanding%20https%20certificates</id><content type="html" xml:base="https://blog.devpour.net/posts/Understanding-https-certificates/"><![CDATA[<p>Production 레벨에서 사용되는 HTTP통신. 이때 보안의 기본이 되고 있는 <strong>HTTPS(Hyper Text Transfer Protocol Secure)</strong>에 대해서 알아보고자 합니다.</p>

<h2 id="http와-https의-차이">HTTP와 HTTPS의 차이</h2>

<dl>
  <dt>HTTP는</dt>
  <dd>Client ↔ Server간 암호화 없이 평문(Plain Text)로 데이터를 주고 받습니다.(암호화 되어 있지 않다는 뜻)</dd>
  <dd>때문에, 패킷을 감청하여 조합하면, 그 데이터를 쉽게 읽을 수 있습니다.</dd>
  <dt>HTTPS는</dt>
  <dd>HTTP의 스펙을 지키면서,</dd>
  <dd>SSL(Secure Socket Layer)와 TLS(Transport Layer Security)를 통해, 데이터를 암호화 합니다.</dd>
</dl>

<h2 id="httpsover-tls-작동-원리">HTTPS(over TLS) 작동 원리</h2>

<p>HTTPS는 ‘Handshake’과정을 통해 작동되며, 크게 2가지 방식의 암호화 방식을 각 목적에 따라 다르게 사용합니다.</p>
<ul>
  <li>‘Handshake 과정’: 비대칭키(Asymmetric encryption, 공개키) 암호화 방식을 사용하여, ‘데이터 교환’과정에서 사용할 대칭 Key를 만들어냅니다.</li>
  <li>‘데이터 교환 과정’: 대칭키(Symmetric encryption) 암호화 방식</li>
</ul>

<blockquote class="prompt-info">
  <p>여기서는 범용적으로 사용되는 TLS기반으로 알아봅니다. 세부사항에는 차이가 있지만, Flow는 TLS와 SSL 모두 비슷합니다.</p>
</blockquote>

<h3 id="tls-handshake">TLS Handshake</h3>

<dl>
  <dt>TLS Handshake과정중에는</dt>
  <dd>사용할 TLS버전(TLS 1.0, 1.2, 1.3 등)을 지정합니다.</dd>
  <dd>통신에 사용할 암호(cipher)를 선택합니다.</dd>
  <dd>서버의 Public key와 CA(Certificate Authority)정보를 기반으로 서버를 신뢰 할 수 있는지 검증합니다.</dd>
  <dd>데이터 전송때 사용할 대칭키(symmetric-key) 암호화에 사용하기 위해, ‘세션키(session key)’를 생성합니다.</dd>
</dl>

<p><img src="/assets/img/for-post/Understanding%20https%20certificates/Full_TLS_1.2_Handshake.png" alt="TLS Handshake full flow - from Wikipedia" />
<em>TLS Handshake full flow - from <a href="https://en.wikipedia.org/wiki/File:Full_TLS_1.2_Handshake.svg">Wikipedia</a></em></p>

<p>TCP 연결이 생성된 후(TCP Handshake후)에 TLS Handshake가 이루어집니다.</p>

<ol>
  <li>
    <p>Client Hello<br />
지원 가능한 TLS 버전, 암호 스위트(cipher suite) 목록, 랜덤값(Client Random)을 전송합니다.</p>
  </li>
  <li>
    <p>Server Hello
Client의 spec에 따라, 선택된 TLS 버전, 암호 스위트, 서버 랜덤값(Server Random)을 전송합니다.</p>
  </li>
  <li>서버 인증 &amp; 키 교환<br />
서버가 인증서(X.509)를 보내어 자신을 증명합니다.
    <blockquote class="prompt-info">
      <p>‘X.509’는 공개키(Public key)를 포함한 ‘인증서 구조’에 대한 표준을 말합니다.</p>
    </blockquote>

    <p>(TLS 1.2 이하) 서버 키 교환 메시지로 공개키 파라미터 전달.</p>
  </li>
  <li>
    <p>클라이언트가 서버의 인증서(X.509) 검증<br />
클라이언트가 인증서 체인을 검증하고 인증 기관(CA)의 신뢰를 확인합니다.<br />
이 과정은, 서버가 ‘누구’이고, 도메인의 실제 소유자인지 확인합니다.</p>
  </li>
  <li>
    <p>클라이언트에서 프리마스터 시크릿(premaster secret) 생성 및 전송<br />
클라이언트 측에서 ‘premaster secret’이라는 Random string을 하나 더 보냅니다.<br />
이 ‘premaster secret’은 서버의 Public key로 암호화(encrypt) 되어있어, 서버에서만 복호화(decrypt)가 가능합니다.</p>

    <blockquote class="prompt-info">
      <p>서버의 Public key는 ‘X.509’안에 포함되어 있는걸 사용합니다.</p>
    </blockquote>
  </li>
  <li>서버에서 프리마스터 시크릿 복호화(decrypt)</li>
  <li>세션 키 생성<br />
클라이언트, 서버 양쪽의 랜덤값과 프리마스터 시크릿(pre-master secret)을 기반으로 세션 키를 결정합니다.
    <blockquote class="prompt-info">
      <p>이때, 이 세션키(Session Key)는 Client와 Server가 각각 별도로 생성합니다. 하지만, 동일한 Session key를 얻게 됩니다(seed와 대상과 알고리즘이 동일하니).<br />
즉, 생성된 Session Key를 서로 공유하지 않습니다.</p>
    </blockquote>
  </li>
  <li>Finished 메시지 교환
    <ol>
      <li>
        <p>클라이언트 → 서버<br />
암호화된 ‘Finished’ 레코드를 전송합니다.(내부에 verify_data_client 포함)</p>
      </li>
      <li>
        <p>서버 수신 및 검증<br />
수신한 레코드(‘Finished’)를 복호화합니다.<br />
자신이 계산한 verify_data_client와 일치하는지 비교합니다.<br />
일치하지 않으면 핸드셰이크 실패로 처리하고, 세션을 종료합니다.</p>
      </li>
      <li>
        <p>서버 → 클라이언트<br />
서버도 마찬가지로, 암호화된 ‘Finished’ 레코드를 보냅니다.(내부에 verify_data_server 포함)</p>
      </li>
      <li>
        <p>클라이언트 수신 및 검증<br />
클라이언트에서, 서버의 verify_data_server를 복호화하고 검증합니다.<br />
성공하면 핸드셰이크 전 과정이 안전히 완료됨을 상호 확인합니다.</p>
      </li>
    </ol>
  </li>
  <li>암호화된 데이터 전송<br />
이후 모든 HTTP 메시지는 <strong>세션 키를 이용한 대칭키 알고리즘(Symmetric-key algorithm)으로 암호화</strong> 하여 보호합니다.</li>
</ol>

<h3 id="데이터-교환">데이터 교환</h3>

<h4 id="tls-12-이전">TLS 1.2 이전</h4>

<p>‘TLS Handshake’과정에서 도출된 ‘Session Key’를 ‘Master Secret’으로 부르고, 이 ‘Master Secret’을 기반으로, 암호화 Key 와 MAC Key(무결성 검증용)을 생성합니다.</p>

<blockquote class="prompt-info">
  <p>암호화와 무결성 검증을 별도의 알고리즘으로 수행합니다.</p>
</blockquote>

<h4 id="tls-13">TLS 1.3</h4>

<p>TLS 1.3부터, 데이터 교환시에는 AEAD(Authenticated Encryption with Associated Data)모드만을 사용합니다.<br />
때문에, ‘AEAD’라는 하나의 알고리즘으로 암호화와 무결성 검증을 모두 수행하며, ‘TLS Handshake’를 통해 만들어진 Session Key(정확하게는 Session key를 기반으로 만들어진 Secret)가  ‘AEAD’에 주입됩니다.</p>

<h4 id="데이터-교환시에는-왜-대칭키-암호화symmetric-key-algorithm-for-cryptography-방식을-사용할까">데이터 교환시에는 왜 대칭키 암호화(Symmetric-key algorithm for cryptography) 방식을 사용할까?</h4>

<dl>
  <dt>성능(Performance)의 이점</dt>
  <dd>대칭키 암호화는 비대칭키(RSA, ECDHE 등)보다 수백 배 빠르고, 하드웨어 가속(AES-NI) 지원도 풍부합니다.</dd>
  <dd>비대칭키(RSA, ECC)는 복잡한 연산(‘Modular Exponentiation’과 같은)으로 이루어져 있고, 키의 길이가 길어야 안정성을 보장하는 부분때문에, 상대적으로 성능이 느립니다.</dd>
  <dt>확장성(Throughput) 이점</dt>
  <dd>HTTP/2, HTTP/3 같이 대량의 바이트를 빠르게 처리해야 할 때, 대칭키는 CPU·메모리 비용이 낮아 효율적입니다.</dd>
  <dt>리소스(Resource) 제약에서의 이점</dt>
  <dd>모바일,IoT 기기처럼 계산 능력이 제한적인 환경에서도 충분히 빠르게 암호·복호화가 가능합니다.</dd>
  <dt>보안 관점(Security)에서의 이점</dt>
  <dd>AEAD 모드(AES-GCM, ChaCha20-Poly1305)는 암호화와 무결성 검증을 한 번에 제공해, 암호화·무결성 결합 보안(authenticated encryption)을 구현합니다.</dd>
</dl>

<h3 id="x509">X.509</h3>

<p>공개키 기반구조(PKI, Public Key Infrastructure)의 ‘표준 인증서 형식’으로, TLS(및 SSL)에서 서버(또는 클라이언트) 인증을 위해 사용됩니다.</p>

<h4 id="x509의-역할">X.509의 역할</h4>

<ul>
  <li>
    <p>신원 증명 역할<br />
인증서에 담긴 도메인 이름(Subject)과 발급자(Issuer) 정보를 통해 서버 신원을 검증</p>
  </li>
  <li>
    <p>공개키 배포 수단<br />
인증서 내부의 ‘Subject Public Key Info’ 필드로 서버의 공개키를 안전하게 전달합니다.</p>
  </li>
  <li>
    <p>무결성 확보<br />
발급자(CA, Certification Authority)의 디지털 서명으로 인증서 위·변조 가능성을 방지합니다.</p>
  </li>
</ul>

<h4 id="x509-예시-및-주요-필드">X.509 예시 및 주요 필드</h4>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="rouge-code"><pre>Certificate:
   Data:
       Version: 3 (0x2)
       Serial Number:
           10:e6:fc:62:b7:41:8a:d5:00:5e:45:b6
       Signature Algorithm: sha256WithRSAEncryption
       Issuer: C=BE, O=GlobalSign nv-sa, CN=GlobalSign Organization Validation CA - SHA256 - G2
       Validity
           Not Before: Nov 21 08:00:00 2016 GMT
           Not After : Nov 22 07:59:59 2017 GMT
       Subject: C=US, ST=California, L=San Francisco, O=Wikimedia Foundation, Inc., CN=*.wikipedia.org
       Subject Public Key Info:
           Public Key Algorithm: id-ecPublicKey
               Public-Key: (256 bit)
           pub: 
                   00:c9:22:69:31:8a:d6:6c:ea:da:c3:7f:2c:ac:a5:
                   af:c0:02:ea:81:cb:65:b9:fd:0c:6d:46:5b:c9:1e:
                   9d:3b:ef
               ASN1 OID: prime256v1
               NIST CURVE: P-256
       X509v3 extensions:
           X509v3 Key Usage: critical
               Digital Signature, Key Agreement
           Authority Information Access: 
               CA Issuers - URI:http://secure.globalsign.com/cacert/gsorganizationvalsha2g2r1.crt
               OCSP - URI:http://ocsp2.globalsign.com/gsorganizationvalsha2g2
           X509v3 Certificate Policies: 
               Policy: 1.3.6.1.4.1.4146.1.20
                 CPS: https://www.globalsign.com/repository/
               Policy: 2.23.140.1.2.2
           X509v3 Basic Constraints: 
               CA:FALSE
           X509v3 CRL Distribution Points: 
               Full Name:
                 URI:http://crl.globalsign.com/gs/gsorganizationvalsha2g2.crl
           X509v3 Subject Alternative Name: 
               DNS:*.wikipedia.org, DNS:*.m.mediawiki.org, DNS:*.m.wikibooks.org, DNS:*.m.wikidata.org, DNS:*.m.wikimedia.org, DNS:*.m.wikimediafoundation.org, DNS:*.m.wikinews.org, DNS:*.m.wikipedia.org, DNS:*.m.wikiquote.org, DNS:*.m.wikisource.org, DNS:*.m.wikiversity.org, DNS:*.m.wikivoyage.org, DNS:*.m.wiktionary.org, DNS:*.mediawiki.org, DNS:*.planet.wikimedia.org, DNS:*.wikibooks.org, DNS:*.wikidata.org, DNS:*.wikimedia.org, DNS:*.wikimediafoundation.org, DNS:*.wikinews.org, DNS:*.wikiquote.org, DNS:*.wikisource.org, DNS:*.wikiversity.org, DNS:*.wikivoyage.org, DNS:*.wiktionary.org, DNS:*.wmfusercontent.org, DNS:*.zero.wikipedia.org, DNS:mediawiki.org, DNS:w.wiki, DNS:wikibooks.org, DNS:wikidata.org, DNS:wikimedia.org, DNS:wikimediafoundation.org, DNS:wikinews.org, DNS:wikiquote.org, DNS:wikisource.org, DNS:wikiversity.org, DNS:wikivoyage.org, DNS:wiktionary.org, DNS:wmfusercontent.org, DNS:wikipedia.org
           X509v3 Extended Key Usage: 
               TLS Web Server Authentication, TLS Web Client Authentication
           X509v3 Subject Key Identifier: 
               28:2A:26:2A:57:8B:3B:CE:B4:D6:AB:54:EF:D7:38:21:2C:49:5C:36
           X509v3 Authority Key Identifier: 
               keyid:96:DE:61:F1:BD:1C:16:29:53:1C:C0:CC:7D:3B:83:00:40:E6:1A:7C
    Signature Algorithm: sha256WithRSAEncryption
      8b:c3:ed:d1:9d:39:6f:af:40:72:bd:1e:18:5e:30:54:23:35:
      ...
</pre></td></tr></tbody></table></code></pre></div></div>

<ol>
  <li>
    <p>버전(Version)<br />
X.509v1, v2, v3 중 대개 ‘v3’를 사용</p>
  </li>
  <li>
    <p>일련번호(Serial Number)<br />
CA(Certification Authority)가 고유하게 부여하는 숫자</p>
  </li>
  <li>
    <p>서명 알고리즘(Signature Algorithm)<br />
인증서 자체에 대한 서명에 사용된 해시·암호 알고리즘 (예: sha256WithRSAEncryption)</p>
  </li>
  <li>
    <p>발급자(Issuer)<br />
인증서를 발급한 CA의 DN(Distinguished Name)</p>
  </li>
  <li>
    <p>유효기간(Validity)<br />
Not Before, Not After로 표현되는 시작일·종료일</p>
  </li>
  <li>
    <p>주체(Subject)<br />
인증 대상(일반적으로 서버)의 DN, 도메인 이름(CN 또는 SAN)에 담김</p>
  </li>
  <li>
    <p>주체 공개키 정보(Subject Public Key Info)<br />
공개키 알고리즘·키 값 (예: RSA 2048-bit 공개키)</p>
  </li>
  <li>
    <p>확장 필드(Extensions) (X.509v3)<br />
Subject Alternative Name (SAN): 도메인, IP 주소 등<br />
Key Usage: 인증서 사용 목적(예: digitalSignature, keyEncipherment)<br />
Extended Key Usage: TLS 서버 인증, 클라이언트 인증 등<br />
Basic Constraints: CA 인증서 여부 및 경로 길이 제약<br />
기타 CRL 분산 포인트, Authority Key Identifier 등</p>
  </li>
  <li>
    <p>CA 서명(Signature)<br />
발급자 CA가 위 필드 전체에 대해 생성한 디지털 서명</p>
  </li>
</ol>

<h3 id="tls의-데이터-암호화에-사용되는-session-key">TLS의 데이터 암호화에 사용되는 Session Key</h3>

<blockquote>
  <p>A session key is any symmetric cryptographic key used to encrypt one communication session only. In other words, it’s a temporary key that is only used once, during one stretch of time, for encrypting and decrypting datasent between two parties; future conversations between the two would be encrypted with different session keys.<br />
- From <a href="https://www.cloudflare.com/learning/ssl/what-is-a-session-key/">Cloudflare</a></p>
</blockquote>

<p>‘세션키(Session Key)’는 한번의 커뮤니케이션에 사용되는, 암호화(encrypt)에 사용되는 Key입니다.<br />
세션키는 <strong>임시키</strong>이며, 특정 시간동안 2개의 대화 구성원 사이에서 암호화(encrypt)하고 복호화(decrypt)합니다.<br />
미래의 대화는 새롭게 생성된, 또 다른 세션키로 암호화, 복호화 됩니다.</p>

<p><img src="/assets/img/for-post/Understanding%20https%20certificates/image.png" alt="각 순간에 임시로 생성되어 사용되는 Session key - from Cloudflare" />
<em>각 순간에 임시로 생성되어 사용되는 Session key - from <a href="https://www.cloudflare.com/learning/ssl/what-is-a-session-key/">Cloudflare</a></em></p>

<h3 id="데이터-example">데이터 Example</h3>

<h4 id="평문-http">평문 HTTP</h4>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>GET /index.html HTTP/1.1
Host: example.com
User-Agent: curl/7.68.0
Accept: */*

(빈 줄)
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="tls-12로-암호화-했을때">TLS 1.2로 암호화 했을때</h4>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>16 03 03 00 5a    ← TLS 레코드 헤더 (type=Application Data, TLS1.2, length=0x005a=90바이트)
8b 4e a3 5f 2d ...  ← 실제 암호화된 페이로드 (총 90바이트)
...              ← (중략)
00 1d 9c b4 7e    ← MAC + 패딩 포함
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="tls-12와-13의-주요-변경점">TLS 1.2와 1.3의 주요 변경점</h3>

<p><a href="https://datatracker.ietf.org/doc/html/rfc8446#section-1.2">표준 문서</a>에 따르면, 다음과 같은 주요 변경점이 있습니다.</p>

<ul>
  <li>암호 스위트(Cipher Suite)를 AEAD’만’ 허용하여 단순화</li>
  <li>
    <p>핸드셰이크 지연 최소화 (1-RTT + 0-RTT)<br />
TLS 1.2는 완전한 핸드셰이크에 2회 왕복(RTT)이 필요했지만,<br />
TLS 1.3은 1-RTT만으로 완료되며, 이전 세션 재개 시 일부 데이터는 0-RTT로 즉시 전송할 수 있게 지원해 지연을 크게 줄였습니다.</p>
  </li>
  <li>Static RSA/DH(Diffie-Hellman) 제거하고, Forward Secrecy(FS, 세션키가 나중에 유출되도 과거의 기록을 안전하게 보호하는 속성을 말합니다.)를 기본으로 지원합니다.
    <blockquote class="prompt-info">
      <p>Q: Static RSA/DH(Diffie-Hellman)을 왜 제거 했나요?<br />
A: ‘Forward Secrecy’를 지원하기 위해, ‘임시(Ephemeral) 키 교환방식’만 지원하기 위해 제거되었습니다.</p>
    </blockquote>
  </li>
</ul>

<h2 id="ssl과-tls">SSL과 TLS</h2>

<p>SSL과 TLS모두 비대칭 키와 대칭키 방식을 이용하여, 암호화된 데이터 교환방식을 제공합니다.<br />
‘SSL’은 처음 Netscape가 만든 사설 규격이었습니다. 이후, SSL기능을 재설계하고, 표준화하면서 IETF(Internet Engineering Task Force)에 의해 ‘TLS’로 명명되었습니다.(비영리 표준화 과정을 거치며 투명한 검증과 호환성 보장)</p>

<h3 id="sslsecure-socket-layer의-결함">SSL(Secure Socket Layer)의 결함</h3>

<p>SSL에는 다음과 같은 결함이 있었습니다.</p>

<ul>
  <li>SSLv2의 구조적 결함<br />
암호 스위트(Cipher suite) 협상 과정의 취약점으로 중간자 공격(MITM)에 취약합니다.
    <blockquote class="prompt-info">
      <p>Cipher Suite는 TLS(또는 SSL) 연결에서 사용할 암호화 알고리즘들의 조합(combination)을 의미합니다.</p>
    </blockquote>

    <p>복수의 암호 모드가 혼재되어 프로토콜 복잡도와 취약성이 높습니다.</p>
  </li>
  <li>SSLv3의 한계<br />
MAC 계산에 MD5를 사용 → 충돌 공격에 노출되기 쉽습니다.(동일한 Hash값을 갖는 다른 데이터를 쉽게 생성 가능)
    <blockquote class="prompt-info">
      <p>MD5는 Hash알고리즘으로, 출력데이터 크기 자체가 작아(128비트 출력), Hash 충돌이 일어날 가능성이 높습니다.(+ 여러가지 다른 이유들 포함)</p>
    </blockquote>

    <p>핸드셰이크 메시지 무결성 검증 불완전<br />
나중에 발견된 <a href="https://spri.kr/posts/view/19827?code=industry_trend">POODLE 공격</a>으로 더 위험해짐</p>
  </li>
</ul>

<p>이런 여러 이유 때문에, SSL은 deprecated(더 이상 사용되지 않음)되었습니다.(<a href="https://www.rfc-editor.org/rfc/rfc7568.html">RFC 7568</a>, Deprecating Secure Sockets Layer Version 3.0)</p>

<h3 id="tlstransport-layer-security">TLS(Transport Layer Security)</h3>

<p>SSL의 결함으로 인해, 보안 연결을 재설계하고 표준화 하여 TLS가 되었습니다.</p>

<ul>
  <li>
    <p>표준화된 HMAC 도입<br />
TLS는 MD5 기반 MAC 대신 HMAC-SHA1·SHA256 등 표준화된 HMAC 사용 → 무결성 검증 강화하였습니다.</p>
  </li>
  <li>
    <p>키 교환·난수 처리 개선<br />
프리마스터 시크릿(pre-master secret) 교환 과정을 간소화하고 안전성을 증가시켰습니다.<br />
랜덤 넘버(zero-byte padding 등)의 처리 방식 보완하였습니다.</p>
  </li>
  <li>
    <p>확장(extension) 프레임워크를 지원합니다.<br />
암호 스위트, 압축 방법, 인증 방식 등을 메시지 교환 중에 동적으로 협상 가능합니다.<br />
이 확장성 덕분에, 새로운 알고리즘을 추가하기가 수훨합니다.</p>
    <blockquote class="prompt-info">
      <p>새로운 알고리즘이 추가된다면, extension field를 이용해 해당 알고리즘에서 필요한 field를 추가할 수 있습니다.(프로토콜 변경 없이!)</p>
    </blockquote>
  </li>
</ul>

<h2 id="references">References</h2>

<dl>
  <dt>HTTPS란 무엇입니까? | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/ko-kr/learning/ssl/what-is-https/">HTTPS란 무엇입니까? | Cloudflare</a></dd>
  <dt>HTTPS를 사용하는 이유는 무엇입니까? | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/ko-kr/learning/ssl/why-use-https/">HTTPS를 사용하는 이유는 무엇입니까? | Cloudflare</a></dd>
  <dt>HTTP와 HTTPS의 차이점은 무엇인가요? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/compare/the-difference-between-https-and-http/">HTTP와 HTTPS 비교 - 전송 프로토콜 간의 차이점 - AWS</a></dd>
  <dt>DNS over TLS와 DNS over HTTPS의 비교 | 안전한 DNS | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/ko-kr/learning/dns/dns-over-tls/">DNS 보안과 TLS HTTPS 비교 | Cloudflare</a></dd>
  <dt>SSL과 TLS의 차이점은 무엇인가요? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/compare/the-difference-between-ssl-and-tls/">SSL과 TLS 비교 - 통신 프로토콜 간의 차이점 - AWS</a></dd>
  <dt>SSL/TLS 인증서란 무엇인가요? | aws.amazon.com</dt>
  <dd><a href="https://aws.amazon.com/ko/what-is/ssl-certificate/">SSL 인증서란 무엇인가요? - SSL/TLS 인증서 설명 - AWS</a></dd>
  <dt>TLS Handshake | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/ko-kr/learning/ssl/what-happens-in-a-tls-handshake/">TLS 핸드셰이크란? | 세션키 교환 | Cloudflare</a></dd>
  <dt>RFC 5280. X.509에 대한 표준 문서 | datatracker.ietf.org</dt>
  <dd><a href="https://datatracker.ietf.org/doc/html/rfc5280">RFC 5280: Internet X.509 Public Key Infrastructure Certificate and Certificate Revocation List (CRL) Profile</a></dd>
  <dt>RFC 8446. TLS Protocol 1.3 | datatracker.ietf.org</dt>
  <dd><a href="https://datatracker.ietf.org/doc/html/rfc8446">RFC 8446: The Transport Layer Security (TLS) Protocol Version 1.3</a></dd>
  <dt>RFC 5246. TLS Protocol 1.2 | datatracker.ietf.org</dt>
  <dd><a href="https://datatracker.ietf.org/doc/html/rfc5246">RFC 5246: The Transport Layer Security (TLS) Protocol Version 1.2</a></dd>
  <dt>What is a session key? | cloudflare.com</dt>
  <dd><a href="https://www.cloudflare.com/learning/ssl/what-is-a-session-key/">What is a session key? | Session keys and TLS handshakes | Cloudflare</a></dd>
  <dt>Cipher suite | developer.mozilla.org</dt>
  <dd><a href="https://developer.mozilla.org/ko/docs/Glossary/Cipher_suite">암호화 스위트 (Cipher suite) - MDN Web Docs 용어 사전: 웹 용어 정의 | MDN</a></dd>
  <dt>SSL/TLS 주요 보안 이슈 | spri.kr</dt>
  <dd><a href="https://spri.kr/posts/view/19827?code=industry_trend">SSL/TLS 주요 보안 이슈 - SPRi</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="Programming" /><category term="networking" /><category term="networking" /><category term="https" /><category term="http" /><category term="web" /><category term="security" /><category term="protocol" /><summary type="html"><![CDATA[Production 레벨에서 사용되는 HTTP통신. 이때 보안의 기본이 되고 있는 HTTPS(Hyper Text Transfer Protocol Secure)에 대해서 알아보고자 합니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/Understanding%20https%20certificates/https-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/Understanding%20https%20certificates/https-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Components - Data Plane(Node) | Kubernetes Deep Dive - 3</title><link href="https://blog.devpour.net/posts/k8s-data-plane/" rel="alternate" type="text/html" title="Components - Data Plane(Node) | Kubernetes Deep Dive - 3" /><published>2025-07-04T21:35:00+09:00</published><updated>2025-07-10T18:06:59+09:00</updated><id>https://blog.devpour.net/posts/k8s%20data%20plane</id><content type="html" xml:base="https://blog.devpour.net/posts/k8s-data-plane/"><![CDATA[<p>이번에는 Kubernetes에서 사용자의 Application이 돌아가는 ‘Data Plane(Node)’에서, Kubernetes 시스템을 위해 돌아가는 컴포넌트(Components)들을 알아보고자 합니다.</p>

<p><img src="/assets/img/for-post/k8s%20data%20plane/image.png" alt="Node와 Node의 컴포넌트들" class="w-50" />
<em>Node와 Node의 컴포넌트들</em></p>

<h2 id="node에-대해서">Node에 대해서</h2>

<p>컴포넌트들에 대해 이해하기에 앞서, Kubernetes에서 Node의 의미를 짚고 가고자 합니다.<br />
Kubernetes에서 ‘Node’는,  사용자의 Pod(container로 이루어진)이 실제로 돌아가는 머신(machine)을 의미합니다.<br />
이 머신은, VM(Virtual Machine, virtualbox와 같은)일수도 있고, 물리(physical) 머신일 수도 있습니다.</p>

<blockquote class="prompt-info">
  <p>Kubernetes에서 Node로서 머신을 인식하기 위해서는, Network interface와 함께, ‘kubelet’이라는 컴포넌트가 중요한 역할을 합니다. 즉, OS자체라기 보다는 OS위에서 구동되는 kubelet이 중요합니다.</p>
</blockquote>

<p>이 Node에는 Kubernetes의 구성요소로서 역할하기 위해, 필수적(necessary)으로 필요한 서비스(컴포넌트)들을 포함하고 있습니다.(뒤에 나올 kubelet과 같은 컴포넌트들을 말합니다)<br />
이 컴포넌트들을 통해, ‘Control Plane’과 계속 통신하며 Node로서의 지위를 유지하고, Pod이 실제로 Node위에서 동작(run)하기 위한 일련의 과정을 수행합니다.</p>

<h3 id="node-관리하기추가하기">Node 관리하기(추가하기)</h3>

<p>노드를 kubernetes 클러스터(cluster)에 등록하기 위해, 주로 2가지 방법을 사용합니다.</p>

<h4 id="노드-스스로-control-plane에-등록하는-방법self-registration-of-nodes"><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#self-registration-of-nodes">노드 스스로 Control plane에 등록하는 방법(Self-registration of Nodes)</a></h4>
<p>노드를 관리하는 ‘kubelet’의 Flag중 <code class="language-plaintext highlighter-rouge">--register-node</code> 가 <code class="language-plaintext highlighter-rouge">true</code>(default값이 true) 라면, ‘kubelet’이 스스로 API Server를 통해 자신의 노드를 등록합니다.</p>

<h4 id="사용자가-수동으로-직접-node-object를-생성하는-방법"><a href="https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration">사용자가 수동으로 직접 ‘Node Object’를 생성하는 방법</a></h4>

<p>Kubernetes를 조작하기 위한 Client인 <code class="language-plaintext highlighter-rouge">kubectl</code>을 통해, Node Object를 수동으로 생성할 수 있습니다.<br />
만약 Node를 수동으로 등록하고자 한다면, ‘kubelet’의 <code class="language-plaintext highlighter-rouge">--register-node=false</code> 로 설정해야 합니다.</p>

<blockquote class="prompt-info">
  <p>노드를 등록하는 과정에서, 노드의 이름을 유니크(unique)하게 관리하는게 중요합니다.<br />
Cluster에서는, 노드의 이름이 동일하다면, 동일한 ‘Node Object’로 인식합니다.<br />
이 부분을 주의하지 않으면, 이름이 동일한 다른 머신으로 인해 Cluster의 장애를 유발할 수 있습니다.</p>
</blockquote>

<h2 id="kubelet">kubelet</h2>

<p><img src="/assets/img/for-post/k8s%20data%20plane/image%201.png" alt="image.png" class="w-50" /></p>

<p>‘kubelet’은 Node에서 돌아가는 Agent(사용자를 대신하여 자율적으로 작업을 수행하는 소프트웨어)입니다.<br />
‘Pod’안에 있는 container들이 계속해서 동작하도록(running상태 이도록) 하며, <strong>노드를 운영하는 핵심적인 역할</strong>을 수행합니다.</p>

<blockquote class="prompt-info">
  <p>kubelet은 kubernetes를 통해 생성된 container만 관리합니다.<br />
container의 label / tag를 이용하여, cluster에 있는 container인지 구분합니다.<br />
(해당 머신에 접속하면, kubelet과 공유하는 Container runtime을 통해 다른 container를 실행할 수 있습니다)</p>
</blockquote>

<p>kubelet이 노드에서 하는 역할은 아래와 같습니다.</p>

<h3 id="podspec을-동기화-합니다">PodSpec을 동기화 합니다.</h3>

<p>‘kubelet’은 API Server(kube-api-server)로 부터 자신의 노드에 할당된 PodSpec을 계속 감시합니다.<br />
Pod에 변경사항(생성되거나 수정됨)이 있다면, 노드의 Container Runtime을 통해 container를 실행하거나 종료합니다.<br />
아래는, PodSpec의 예시입니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pod</span>
<span class="na">spec</span><span class="pi">:</span>            <span class="c1"># PodSpec</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.25.0</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">nodeSelector</span><span class="pi">:</span>
    <span class="na">disktype</span><span class="pi">:</span> <span class="s">ssd</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
    <span class="na">configMap</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">web-config</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="컨테이너-런타임-인터페이스container-runtime-interface-cri과-연동되어-작동합니다">컨테이너 런타임 인터페이스(Container Runtime Interface, CRI)과 연동되어 작동합니다.</h3>

<p>여러 컨테이너 런타임(Docker, containerd, CRI-O)에 호환성이 있어, gRPC를 통해 컨테이터 런타임을 작동시킵니다.<br />
컨테이너 런타임을 통해, 외부에서 이미지를 가져오거나(pull), container를 생성, 삭제하며, 메트릭과 로그를 수집합니다.</p>

<h3 id="상태-보고status-reporting">상태 보고(Status Reporting)</h3>

<p>‘kubelet’은 Node(노드)와 Pod(파드)의 상태를 실시간으로 파악하여 보고합니다. 이러한 보고는, ‘kube-apiserver’를 통해, etcd에 최종 기록됩니다.</p>

<h4 id="node-상태-보고">Node 상태 보고</h4>

<ul>
  <li>
    <p>capacity / allocatable<br />
CPU, 메모리, 디스크와 같은 리소스 할당량 및 한계량을 API Server를 통해 Control Plane에 보고합니다.</p>
  </li>
  <li>
    <p>Node의 현재상태<br />
Ready(Node가 Pod을 받을 수 있는 상태)인지,<br />
DiskPressure, MemoryPressure, PIDPressure와 같은 리소스 압박 상태인지<br />
NetworkUnavailable과 같이 네트워크 문제가 있는지<br />
확인하여 보고합니다.</p>
  </li>
  <li>
    <p>Internal IP, Hostname, External IP등의 address를 보고합니다.</p>
  </li>
  <li>
    <p>Node에서 돌아가는 daemon의 endpoints(daemonEndpoints)를 보고합니다.<br />
kubelet과 같은 Daemon에 대한 포트(port)정보를 보고합니다.</p>
    <blockquote class="prompt-info">
      <p>이 Port정보는, Control plane과 통신하기 위한 gRPC와 HTTPS 포트입니다.</p>
    </blockquote>

    <blockquote class="prompt-info">
      <p>과거에는 dockershim에 대한 정보도 함께 공유됬지만, dockershim이 deprecated되면서 제거되었습니다.</p>
    </blockquote>
  </li>
</ul>

<h4 id="pod-상태-보고">Pod 상태 보고</h4>

<ul>
  <li>
    <p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">Pod의 phase</a>를 보고합니다.<br />
‘phase’는 Pod의 ‘상태’를 말합니다. 다만, status와 다른것은, Pod Lifecycle로 추상화된 ‘high-level summary’입니다.<br />
‘Pending’, ‘Running’, ‘Succeeded’, ‘Failed’, ‘Unknown’과 같은 값이 있습니다.</p>
  </li>
  <li>
    <p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions">Pod의 Conditions</a>를 보고합니다.<br />
Pod은 ‘PodStatus’를 가지는데, 이는 여러 condition으로 이루어져 있습니다.<br />
‘PodScheduled’, ‘PodReadyToStartContainers’, ‘Ready’등의 값이 있습니다.</p>
  </li>
  <li>
    <p>Pod에 포함된 Container상태 보고<br />
Pod의 Container상태를 보고합니다.<br />
ready, restartCount(재시작 횟수), state와 함께, 상세사유를 보고합니다.</p>
  </li>
  <li>Host IP와 Pod IP를 보고합니다.</li>
  <li>startTime을 보고합니다.</li>
</ul>

<h3 id="pod에-대한-헬스체크-및-라이프사이클lifecycle-관리">Pod에 대한 헬스체크 및 라이프사이클(Lifecycle) 관리</h3>

<p>‘kubelet’은 Pod의 현재상태를 체크하며, 라이프 사이클에 따라 적당한 작업을 수행합니다.</p>

<h4 id="pod-상태-확인">Pod 상태 확인</h4>
<p>개별 Container에 대한, ‘Liveness’, ‘Readiness’, ‘Startup Probe’를 실행해 헬스(health) 상태를 판단합니다.</p>

<h4 id="lifecycle-관리">Lifecycle 관리</h4>
<p>실패한 Container를 재시작하거나, Ready상태를 해제해, 서비스 트래픽에서 제외시킵니다.</p>

<h3 id="볼륨-마운트-관리">볼륨 마운트 관리</h3>
<p>Pod에 정의된 PV(PersistentVolume), ConfigMap, Secret 볼륨을 마운트(mount)하거나 언마운트(unmount) 합니다.</p>

<h3 id="노트-상태-관리">노트 상태 관리</h3>
<p>노드의 리소스 사용량(CPU, 메모리, 디스크)과 헬스(Ready/NotReady)를 판단해 API Server에 보고합니다.<br />
리소스 부족 시에는 evict(축출) 정책을 실행합니다.</p>

<h3 id="kubelet의-구성요소">kubelet의 구성요소</h3>
<ul>
  <li>
    <p>Pod Manager<br />
PodSpec을 해석하여, runtime 명령어로 변환하고 실행합니다.</p>
  </li>
  <li>
    <p>Probe Manager<br />
Liveness / Readiness / Startup Probe 을 스케쥴링 하고 실행합니다.</p>
  </li>
  <li>Volume Manager<br />
CSI(Container Storage Interface) 플러그인을 연동하고, 볼륨을 마운트 / 언마운트 합니다.
    <blockquote class="prompt-info">
      <p>CSI(Container Storage Interface):<br />
Container Runtime을 위한 Storage Interface를 말 합니다.<br />
AWS나 GCP와 같이 kubernetes환경마다 다양한 storage를 지원하기 때문에, 이런 다양한 storage를 지원하기 위해 만들어진 interface입니다.</p>
    </blockquote>
  </li>
  <li>
    <p>Eviction Manager<br />
Node의 자원(CPU, RAM, Disk 같은) 압박(pressure) 상황에서 Pod을 축출(eviction)합니다</p>
  </li>
  <li>Status Manager<br />
API Server를 통해 Pod/Node의 상태를 업데이트 합니다.</li>
</ul>

<h3 id="확장-가능한-부분">확장 가능한 부분</h3>

<ul>
  <li>
    <p>Device Plugins<br />
GPU, FPGA와 같은 특수 하드웨어 리소스를 할당하기 위해 별도의 플러그인을 설치할 수 있습니다.</p>
  </li>
  <li>Custom Metrics Adapter<br />
Application의 메트릭을 Pod레벨로 노출시켜 사용할 수 있게 합니다.
    <blockquote class="prompt-info">
      <p>‘Prometheus Adapter(커스텀 메트릭 어댑터)’를 통해, App에서 제공하는 메트릭을 HPA(Horizontal Pod Autoscaler)가 참조하도록 할 수 있습니다.</p>
    </blockquote>
  </li>
  <li>Static Pods<br />
Node에서 직접 정의된 YAML로 노드수준의 kubelet이 직접 관리하는 Pod입니다. 즉 API Server없이 실행되는 Pod입니다.</li>
</ul>

<h2 id="kube-proxy">kube-proxy</h2>

<p><img src="/assets/img/for-post/k8s%20data%20plane/image%202.png" alt="kube-proxy와 Service의 machanism" />
<em>kube-proxy와 Service의 machanism</em></p>

<dl>
  <dt>‘kube-proxy’는</dt>
  <dd>각각의 Node에서 작동하는 network proxy 입니다.</dd>
  <dd>Kubernetes의 ‘Service’라는 추상화된 컨셉을 적용하기 위한 컨포턴트 입니다.</dd>
  <dd>Node의 network rule을 관리하며, 이를 통해 Pod이 Cluster내부/외부 모두 통신할 수 있게 해줍니다.</dd>
</dl>

<p>만약 OS에서 ‘Packet filtering layer’가 있다면 해당 기능을 사용하고, 그렇지 않으면 ‘kube-proxy’가 직접 그 역할을 수행합니다(Golang 기반의 App).</p>

<blockquote class="prompt-info">
  <p>리눅스 커널에는 Netfilter(iptables)나 IPVS 같은 ‘Packet filtering / routing’ 기능을 포함하고 있습니다.
이를 이용할 수 있으면, ‘kube-proxy’의 packet 처리 기능을 커널레벨에서 처리하므로, 매우 빠르게(CPU효율적으로) 처리할 수 있습니다.</p>
</blockquote>

<h3 id="주요-역할">주요 역할</h3>

<ul>
  <li>
    <p>Service IP와 Pod IP를 매핑(mapping)합니다.<br />
Kubernetes의 ‘Service’에 할당된 ClusterIP(Cluster수준에서 사용되는 가상 IP)를 통해 traffic을 받으면, 이를 개별 Pod으로 포워딩 해줍니다.</p>
  </li>
  <li>
    <p>Load-balancing(부하 분산)을 수행합니다.<br />
‘kube-proxy’가 작동하는 ‘모드’에 따라, 다른 balancing이 이루어 집니다.</p>
  </li>
  <li>
    <p>노드 간 / 노드 밖에 대한 트래픽을 라우팅 합니다.</p>
  </li>
</ul>

<h3 id="동작-모드">동작 모드</h3>

<p>‘kube-proxy’는 크게 2가지의 모드로 구분됩니다.</p>

<blockquote class="prompt-info">
  <p>kube-proxy는 Host의 OS에 따라, 사용할 수 있는 mode가 제한됩니다.</p>
</blockquote>

<h4 id="iptables-모드default설정">ipTables 모드(default설정)</h4>

<p><img src="/assets/img/for-post/k8s%20data%20plane/image%203.png" alt="ipTables Rule을 통해 보는, LB부터 Pod에 이르는 packet flow - from [cncf.io](https://www.cncf.io/blog/2020/01/30/kubernetes-networking-demystified-a-brief-guide/)" />
<em>ipTables Rule을 통해 보는, LB부터 Pod에 이르는 packet flow - from <a href="https://www.cncf.io/blog/2020/01/30/kubernetes-networking-demystified-a-brief-guide/">cncf.io</a></em></p>

<p>‘Service’생성 시에, ‘서비스에 대한 ClusterIP + 포트’ 조합에 대해 iptables 체인을 설정합니다.<br />
노드의 커널 레벨에서, ‘서비스의 ClusterIP + Port’로 들어오는 패킷을 RR(Round Robin) 방식으로 뒷단의 Pod IP로 리다이렉트 합니다.</p>

<dl>
  <dt>장점은</dt>
  <dd>커널 레벨에서 처리하기 때문에, 높은 성능을 제공하므로, 낮은 지연시간을 보여줍니다.</dd>
  <dt>단점은</dt>
  <dd>규칙 수가 너무 많아지면 iptables 룰 체인이 커져서, 관리 오버헤드가 발생합니다.</dd>
</dl>

<p>아래는, ‘iptables’에서 ‘my-service’라는 ClusterIP서비스(<code class="language-plaintext highlighter-rouge">10.96.0.10:80</code>)가 2개의 백엔드 Pod(<code class="language-plaintext highlighter-rouge">192.168.1.11:8080</code>, <code class="language-plaintext highlighter-rouge">192.168.1.12:8080</code>)로 트래픽을 분산하는 Rule의 모습을 보여줍니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span> KUBE-SERVICES <span class="nt">-n</span> <span class="nt">--line-numbers</span>
Chain KUBE-SERVICES <span class="o">(</span>2 references<span class="o">)</span>
num  target     prot opt <span class="nb">source               </span>destination
1    KUBE-SEP-ABCDEF123456  tcp  <span class="nt">--</span>  0.0.0.0/0            10.96.0.10           /<span class="k">*</span> default/my-service: cluster IP <span class="k">*</span>/ tcp dpt:80
2    KUBE-MARK-MASQ       all  <span class="nt">--</span>  0.0.0.0/0            10.96.0.10           /<span class="k">*</span> default/my-service: cluster IP <span class="k">*</span>/ 
3    RETURN               all  <span class="nt">--</span>  0.0.0.0/0            0.0.0.0/0
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li><strong>1번 룰</strong>: Service IP(10.96.0.10:80, Cluster IP) 로 들어오는 TCP 패킷을 KUBE-SEP-ABCDEF123456 체인으로 점프시킵니다.</li>
  <li><strong>2번 룰</strong>: SNAT(소스 마스커레이드) 표시를 위해 KUBE-MARK-MASQ 체인으로 점프.</li>
  <li><strong>3번 RETURN</strong>: 더 이상 매칭되지 않으면 원래 체인으로 복귀.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span> KUBE-SEP-ABCDEF123456 <span class="nt">-n</span> <span class="nt">--line-numbers</span>
Chain KUBE-SEP-ABCDEF123456 <span class="o">(</span>1 references<span class="o">)</span>
num  target     prot opt <span class="nb">source               </span>destination
1    DNAT       tcp  <span class="nt">--</span>  0.0.0.0/0            192.168.1.11        /<span class="k">*</span> default/my-service <span class="k">*</span>/ tcp dpt:8080
2    DNAT       tcp  <span class="nt">--</span>  0.0.0.0/0            192.168.1.12        /<span class="k">*</span> default/my-service <span class="k">*</span>/ tcp dpt:8080
3    RETURN     all  <span class="nt">--</span>  0.0.0.0/0            0.0.0.0/0
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li><strong>1번 DNAT 룰</strong>: 첫 번째 백엔드 Pod로 DNAT.</li>
  <li><strong>2번 DNAT 룰</strong>: 두 번째 백엔드 Pod로 DNAT.</li>
  <li><strong>3번 RETURN</strong>: 매칭 실패 시 복귀.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span> KUBE-MARK-MASQ <span class="nt">-n</span> <span class="nt">--line-numbers</span>

Chain KUBE-MARK-MASQ <span class="o">(</span>1 references<span class="o">)</span>
num  target     prot opt <span class="nb">source               </span>destination
1    MARK       all  <span class="nt">--</span>  0.0.0.0/0            10.96.0.0/12       MARK <span class="nb">set </span>0x4000
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li><strong>MASQ 표시</strong>: Service 외부(외부IP 또는 NodePort)로 나가는 패킷에 마스커레이드를 적용해야 할 때, 이 마크를 보고 SNAT을 수행합니다.</li>
</ul>

<h4 id="ipvsip-virtual-server-모드">IPVS(IP Virtual Server) 모드</h4>

<p>kube-proxy가 IPVS 모드로 동작할 때는, 리눅스 커널의 IP Virtual Server 기능을 이용해 가상 서버(Virtual Server) 를 띄우고, 여기에 실제 백엔드 Pod(Real Servers) 를 등록하는 방식으로 로드밸런싱을 수행합니다.</p>

<h5 id="작동-과정">작동 과정</h5>

<ol>
  <li>리눅스의 커널 레벨의 IPVS 프레임워크에 Service Virtual Server(VS)를 생성합니다.</li>
  <li>각 Endpoint(Backend) 서버를 Virtual Server에 등록합니다.</li>
  <li>IPVS 스케줄러(rr, lc, wlc 등)로 트래픽 분배합니다.</li>
</ol>

<dl>
  <dt>장점은</dt>
  <dd>대규모 Service 처리 시 iptables 대비 더 빠르고, 룰 관리가 간단합니다.</dd>
  <dd>RR(Round Robin)뿐만 아니라, 다양한 스케줄러 모드를 지원합니다.</dd>
  <dt>단점은</dt>
  <dd>커널 모듈 의존성(CentOS/RHEL 등 커널 패치 필요)이 있어서, 초기 세팅이 어렵습니다.</dd>
</dl>

<h3 id="내부-구성요소">내부 구성요소</h3>

<ul>
  <li>
    <p>Watcher<br />
API Server의 /services 및 /endpoints 리소스를 계속해서 감시합니다.</p>
  </li>
  <li>
    <p>Proxier<br />
모드(iptables/IPVS)별로 룰을 생성하고 갱신합니다.</p>
  </li>
  <li>
    <p>Local Manager<br />
Node 로컬 네트워크에 바인딩 혹은 해제된 포트를 관리합니다.</p>
  </li>
  <li>
    <p>Metrics Server<br />
<code class="language-plaintext highlighter-rouge">kube_proxy_metrics</code> 을 통해, 연결 / 종료 건수나 오류율 등을 노출시킵니다.</p>
  </li>
</ul>

<h3 id="service-처리-flow">Service 처리 Flow</h3>

<p><img src="/assets/img/for-post/k8s%20data%20plane/image%204.png" alt="클라이언트 Pod에서 다른 노드의 서버 Pod로의 트래픽 흐름" />
<em>클라이언트 Pod에서 다른 노드의 서버 Pod로의 트래픽 흐름</em></p>

<ol>
  <li>
    <p>kube-proxy에서 iptables를 최신상태로 갱신합니다.<br />
‘kube-proxy’는 API Server를 통해, Pod 목록(라우팅 대상)을 갱신하며, 이를 각 Node의 ‘iptables’에 Rule로 반영합니다.</p>
  </li>
  <li>
    <p>클라이언트 Pod에서는 Cluster내부의 Service를 호출합니다.</p>
  </li>
  <li>
    <p>Service 호출 traffic은 Client Pod이 있는 Node의 iptables에 의해, 목적지 IP와 Port번호가 갱신되어, Backend Pod에 전달됩니다.<br />
iptables의 DNAT(Destination NAT) Rule을 이용해, ‘목적지 IP(Cluster 수준의 IP)’ → ‘Backend Pod의 IP’로 변경됩니다.</p>
  </li>
</ol>

<h3 id="kube-proxy를-다른것으로-대체할-수-있습니다">kube-proxy를 다른것으로 대체할 수 있습니다.</h3>

<p><a href="https://kubernetes.io/docs/concepts/architecture/#kube-proxy">Kubernetes의 공식문서</a>에 보면, ‘kube-proxy’는 Optional로 표현되어 있는데, 이는 서비스 트래픽에 대한 proxy역할을 ‘kube-proxy’가 아니어도 대체할 수 있기 때문입니다.</p>

<h4 id="service-mesh-솔루션">Service Mesh 솔루션</h4>

<p>Istio, Linkerd, Kuma 같은 서비스 메시를 쓰면, Envoy, dataplane 에이전트가 Pod 간·외부 트래픽을 가로채어 처리하며, kube-proxy를 건너뛰고도 충분한 로드밸런싱/리트라이정책 적용이 가능합니다.</p>

<h4 id="headless-서비스">Headless 서비스</h4>

<p>ClusterIP를 쓰지 않고 DNS 기반으로 Endpoint IP 리스트를 Pod가 직접 조회해 접속하는 패턴(headless service)을 쓰면, kube-proxy가 아예 개입할 여지가 없습니다.</p>

<h2 id="container-runtime">Container runtime</h2>

<blockquote>
  <p>A fundamental component that empowers Kubernetes to run containers effectively.
- kubernetes.io</p>
</blockquote>

<p>‘Container runtime’은 Kubernetes의 Pod에 있는 Container를 돌리는 runtime환경입니다.<br />
Container에 대한 실행과 Lifecycle관리에 대한 책임을 갖고 있습니다.</p>

<h3 id="container-runtime-interfacecri">Container Runtime Interface(CRI)</h3>

<p>Kubernetes에선, kubelet과 ‘Container Runtime’이 gRPC로 통신할 수 있는 Interface를 지원합니다. 이를 ‘Container Runtime Interface(CRI)’라고 합니다.</p>

<p><br /></p>

<p>Kubernetes에서는 CRI를 이용하여, 여러 Container Runtime을 지원합니다.</p>

<h4 id="containerd"><a href="https://containerd.io/docs/">container.d</a></h4>

<p>CNCF 프로젝트였으며, Docker의 핵심만 분리하여 경량화한 runtime입니다.<br />
Kubernetes v1.24 이후부터는 ‘container.d’가 <strong>default runtime</strong>입니다.</p>

<blockquote class="prompt-info">
  <p>Q: Docker Engine에서 container.d로 default runtime이 바뀌게된 이유?<br />
A: Docker Engine은 OCI(Open Container Initiative) 규격보다 훨씬 많은 기능(빌드, 네트워크 관리, 로그 드라이버 등)을 포함한 “풀 스택” 플랫폼이었고, 이를 CRI로 감싸기 위해 dockershim이라는 중간 계층(shim)을 유지해야 했습니다.<br />
containerd는 애초에 OCI 런타임에만 집중한 경량 서비스로, CRI 플러그인을 붙이면 kubelet과 직접 통신할 수 있어(shim계층 불필요) default runtime으로 자리잡게 되었습니다.</p>
</blockquote>

<h4 id="cri-o"><a href="https://cri-o.io/#what-is-cri-o">CRI-O</a></h4>

<p>‘Red Hat’의 주도로 만들어진, OpenShift(Red hat의 k8s기반 오케스트레이션 플랫폼)에 최적화된 runtime입니다.</p>

<h3 id="cgroupcontrol-groups-drivers"><a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers">cgroup(control groups) Drivers</a></h3>

<p>Linux의 ‘cgroup(Control Groups)’은 컨테이너 런타임이 ‘리소스 격리 및 할당’을 위해 사용하는 핵심 커널 기능입니다.<br />
cgroup은 프로세스(또는 프로세스 그룹)에 대해 CPU, 메모리, 블록 I/O, PID 수 등을 강제로 제한 / 계측하고, 우선순위를 지정할 수 있게 해 줍니다.</p>

<h4 id="kubernetes에서의-활용">Kubernetes에서의 활용</h4>

<p>kubelet에서는 <code class="language-plaintext highlighter-rouge">--cgroup-driver</code> 를 통해, cgroup으로 사용할 드라이버를 설정할 수 있습니다.<br />
아래와 같은 2가지 드라이버가 가능한 옵션입니다.</p>
<ul>
  <li>cgroupfs</li>
  <li>systemd</li>
</ul>

<h2 id="실습">실습</h2>
<h3 id="data-plane의-node-조회">Data Plane의 Node 조회</h3>

<h4 id="cluster의-노드-목록-조회">Cluster의 노드 목록 조회</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>~ kubectl get nodes
NAME                                              STATUS   ROLES    AGE     VERSION
ip-10-20-99-219.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   8m58s   v1.32.1-eks-5d632ec
</pre></td></tr></tbody></table></code></pre></div></div>
<h4 id="개별-node의-세부-사항-조회">개별 Node의 세부 사항 조회</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>kubectl describe nodes ip-10-20-99-219.ap-northeast-2.compute.internal
Name:               ip-10-20-99-219.ap-northeast-2.compute.internal
Roles:              &lt;none&gt;
Labels:             beta.kubernetes.io/arch<span class="o">=</span>amd64
                    beta.kubernetes.io/instance-type<span class="o">=</span>t3.large
                    beta.kubernetes.io/os<span class="o">=</span>linux
                    ...
Annotations:        alpha.kubernetes.io/provided-node-ip: 10.20.99.219
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: <span class="nb">true
</span>CreationTimestamp:  Thu, 10 Jul 2025 17:27:37 +0900
Taints:             &lt;none&gt;
Unschedulable:      <span class="nb">false
</span>Lease:
  HolderIdentity:  ip-10-20-99-219.ap-northeast-2.compute.internal
  AcquireTime:     &lt;<span class="nb">unset</span><span class="o">&gt;</span>
  RenewTime:       Thu, 10 Jul 2025 17:38:31 +0900
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  <span class="nt">----</span>             <span class="nt">------</span>  <span class="nt">-----------------</span>                 <span class="nt">------------------</span>                <span class="nt">------</span>                       <span class="nt">-------</span>
  MemoryPressure   False   Thu, 10 Jul 2025 17:34:34 +0900   Thu, 10 Jul 2025 17:27:33 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 10 Jul 2025 17:34:34 +0900   Thu, 10 Jul 2025 17:27:33 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 10 Jul 2025 17:34:34 +0900   Thu, 10 Jul 2025 17:27:33 +0900   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 10 Jul 2025 17:34:34 +0900   Thu, 10 Jul 2025 17:27:56 +0900   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:   10.20.99.219
  InternalDNS:  ip-10-20-99-219.ap-northeast-2.compute.internal
  Hostname:     ip-10-20-99-219.ap-northeast-2.compute.internal
Capacity:
  cpu:                2
  ephemeral-storage:  31379436Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7999148Ki
  pods:               35
Allocatable:
  cpu:                1930m
  ephemeral-storage:  27845546346
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7241388Ki
  pods:               35
System Info:
  Machine ID:                 ...
  System UUID:                ...
  Boot ID:                    ...
  Kernel Version:             6.1.127-135.201.amzn2023.x86_64
  OS Image:                   Amazon Linux 2023.6.20250203
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.25
  Kubelet Version:            v1.32.1-eks-5d632ec
  Kube-Proxy Version:         v1.32.1-eks-5d632ec
ProviderID:                   aws:///ap-northeast-2c/i-0288f536d015d5e34
Non-terminated Pods:          <span class="o">(</span>4 <span class="k">in </span>total<span class="o">)</span>
  Namespace                   Name                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  <span class="nt">---------</span>                   <span class="nt">----</span>                        <span class="nt">------------</span>  <span class="nt">----------</span>  <span class="nt">---------------</span>  <span class="nt">-------------</span>  <span class="nt">---</span>
  kube-system                 aws-node-496nj              50m <span class="o">(</span>2%<span class="o">)</span>      0 <span class="o">(</span>0%<span class="o">)</span>      0 <span class="o">(</span>0%<span class="o">)</span>           0 <span class="o">(</span>0%<span class="o">)</span>         9m23s
  kube-system                 coredns-586c6dd46b-5s5cc    100m <span class="o">(</span>5%<span class="o">)</span>     0 <span class="o">(</span>0%<span class="o">)</span>      70Mi <span class="o">(</span>0%<span class="o">)</span>        170Mi <span class="o">(</span>2%<span class="o">)</span>     9m36s
  kube-system                 coredns-586c6dd46b-vsdfd    100m <span class="o">(</span>5%<span class="o">)</span>     0 <span class="o">(</span>0%<span class="o">)</span>      70Mi <span class="o">(</span>0%<span class="o">)</span>        170Mi <span class="o">(</span>2%<span class="o">)</span>     9m36s
  kube-system                 kube-proxy-nc5tm            100m <span class="o">(</span>5%<span class="o">)</span>     0 <span class="o">(</span>0%<span class="o">)</span>      0 <span class="o">(</span>0%<span class="o">)</span>           0 <span class="o">(</span>0%<span class="o">)</span>         10m
Allocated resources:
  <span class="o">(</span>Total limits may be over 100 percent, i.e., overcommitted.<span class="o">)</span>
  Resource           Requests    Limits
  <span class="nt">--------</span>           <span class="nt">--------</span>    <span class="nt">------</span>
  cpu                350m <span class="o">(</span>18%<span class="o">)</span>  0 <span class="o">(</span>0%<span class="o">)</span>
  memory             140Mi <span class="o">(</span>1%<span class="o">)</span>  340Mi <span class="o">(</span>4%<span class="o">)</span>
  ephemeral-storage  0 <span class="o">(</span>0%<span class="o">)</span>      0 <span class="o">(</span>0%<span class="o">)</span>
  hugepages-1Gi      0 <span class="o">(</span>0%<span class="o">)</span>      0 <span class="o">(</span>0%<span class="o">)</span>
  hugepages-2Mi      0 <span class="o">(</span>0%<span class="o">)</span>      0 <span class="o">(</span>0%<span class="o">)</span>
Events:
  Type     Reason                   Age                From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                   <span class="nt">----</span>               <span class="nt">----</span>                   <span class="nt">-------</span>
  Normal   Starting                 10m                kube-proxy
  Normal   Starting                 10m                kubelet                Starting kubelet.
  Warning  InvalidDiskCapacity      10m                kubelet                invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  10m <span class="o">(</span>x2 over 10m<span class="o">)</span>  kubelet                Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    10m <span class="o">(</span>x2 over 10m<span class="o">)</span>  kubelet                Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     10m <span class="o">(</span>x2 over 10m<span class="o">)</span>  kubelet                Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  10m                kubelet                Updated Node Allocatable limit across pods
  Normal   Synced                   10m                cloud-node-controller  Node synced successfully
  Normal   RegisteredNode           10m                node-controller        Node ip-10-20-99-219.ap-northeast-2.compute.internal event: Registered Node ip-10-20-99-219.ap-northeast-2.compute.internal <span class="k">in </span>Controller
  Normal   NodeReady                10m                kubelet                Node ip-10-20-99-219.ap-northeast-2.compute.internal status is now: NodeReady
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="node의-system-component-조회">Node의 System Component 조회</h3>

<h4 id="node의-system-component-조회-1">Node의 System Component 조회</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>kubectl get pods <span class="nt">--namespace</span><span class="o">=</span>kube-system <span class="nt">--field-selector</span> spec.nodeName<span class="o">=</span>ip-10-20-99-219.ap-northeast-2.compute.internal
NAME                       READY   STATUS    RESTARTS   AGE
aws-node-496nj             2/2     Running   0          17m <span class="c">## Amazon VPC CNI 플러그인</span>
coredns-586c6dd46b-5s5cc   1/1     Running   0          17m
coredns-586c6dd46b-vsdfd   1/1     Running   0          17m
kube-proxy-nc5tm           1/1     Running   0          18m
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="kube-proxy-조회">kube-proxy 조회</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>kubectl get daemonSets <span class="nt">--namespace</span><span class="o">=</span>kube-system kube-proxy
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
kube-proxy   1         1         1       1            1           &lt;none&gt;          18m
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="cluster의-dns-deployments-조회">Cluster의 DNS Deployments 조회</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>kubectl get deployments <span class="nt">--namespace</span><span class="o">=</span>kube-system coredns
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
coredns   2/2     2            2           25m
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="dns에-대한-service조회">DNS에 대한 Service조회</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>kubectl get services <span class="nt">--namespace</span><span class="o">=</span>kube-system kube-dns
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>                  AGE
kube-dns   ClusterIP   172.20.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   28m
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="references">References</h2>

<dl>
  <dt>Nodes | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/architecture/nodes/">Nodes</a></dd>
  <dt>Node Components | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/overview/components/#node-components">Kubernetes Components</a></dd>
  <dt>Pod Lifecycle | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle</a></dd>
  <dt>Container Storage Interface(CSI) for Kubernetes GA | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI) for Kubernetes GA</a></dd>
  <dt>Container Storage Interface(CSI) spec | github.com/container-storage-interface/spec</dt>
  <dd><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">https://github.com/container-storage-interface/spec/blob/master/spec.md</a></dd>
  <dt>kube-proxy | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a></dd>
  <dt>Virtual IPs and Service Proxies | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a></dd>
  <dt>Iptables proxy mode | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-iptables">Virtual IPs and Service Proxies</a></dd>
  <dt>IPVS proxy mode | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs">Virtual IPs and Service Proxies</a></dd>
  <dt>Service ClusterIP allocation | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/concepts/services-networking/cluster-ip-allocation/">Service ClusterIP allocation</a></dd>
  <dt>Kubernetes’s IPTables Chains Are Not API | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/">Kubernetes’s IPTables Chains Are Not API</a></dd>
  <dt>Kubernetes networking demystified: a brief guide | cncf.io</dt>
  <dd><a href="https://www.cncf.io/blog/2020/01/30/kubernetes-networking-demystified-a-brief-guide/">Kubernetes networking demystified: a brief guide</a></dd>
  <dt>kube-proxy | GKE networking overview | Google Cloud</dt>
  <dd><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview?hl=ko#kube-proxy">네트워크 개요 | Google Kubernetes Engine (GKE) | Google Cloud</a></dd>
  <dt>Container Runtime Interface(CRI) | github.com/kubernetes/community</dt>
  <dd><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md</a></dd>
  <dt>Container Runtimes | kubernetes.io</dt>
  <dd><a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Container Runtimes</a></dd>
</dl>]]></content><author><name>KanghoonYi(pour)</name></author><category term="DevOps" /><category term="kubernetes" /><category term="aws" /><category term="kubernetes" /><category term="cncf" /><category term="k8s" /><summary type="html"><![CDATA[이번에는 Kubernetes에서 사용자의 Application이 돌아가는 ‘Data Plane(Node)’에서, Kubernetes 시스템을 위해 돌아가는 컴포넌트(Components)들을 알아보고자 합니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://blog.devpour.net/assets/img/for-post/k8s%20data%20plane/k8s-data-plane-cover.jpg" /><media:content medium="image" url="https://blog.devpour.net/assets/img/for-post/k8s%20data%20plane/k8s-data-plane-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>